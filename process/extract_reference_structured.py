# Copyright (c) 2025 Bytedance Ltd. and/or its affiliates
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import json
import re
from bs4 import BeautifulSoup
from urllib.parse import urlparse
from datetime import datetime

def extract_reference_structured(json_file_path=None, reference_html=None, log=None):
    """
    ä»JSONæ–‡ä»¶æˆ–ç›´æ¥ä»HTMLå­—ç¬¦ä¸²ä¸­æå–å¹¶ç»“æ„åŒ–referenceå­—æ®µçš„ä¿¡æ¯
    
    Args:
        json_file_path (str, optional): JSONæ–‡ä»¶è·¯å¾„
        reference_html (str, optional): ç›´æ¥ä¼ å…¥çš„HTMLå­—ç¬¦ä¸²
    """
    
    if reference_html is not None:
        # ç›´æ¥ä½¿ç”¨ä¼ å…¥çš„HTMLå­—ç¬¦ä¸²
        pass
    elif json_file_path is not None:
        # ä»JSONæ–‡ä»¶ä¸­è¯»å–
        with open(json_file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        reference_html = data.get('reference', '')
    else:
        raise ValueError("å¿…é¡»æä¾› json_file_path æˆ– reference_html å‚æ•°ä¹‹ä¸€")
    
    if not reference_html:
        if log:
            log("âŒ æœªæ‰¾åˆ°referenceå­—æ®µ")
        return None
    
    # ä½¿ç”¨BeautifulSoupè§£æHTML
    soup = BeautifulSoup(reference_html, 'html.parser')
    
    # æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
    all_links = soup.find_all('a', href=True)
    if log:
        log(f"ğŸ” æ€»å…±æ‰¾åˆ° {len(all_links)} ä¸ªé“¾æ¥")
    
    # æŸ¥æ‰¾"å…¨éƒ¨æ¥æº"æˆ–"All Sources"æ–‡æœ¬æ¥å®šä½åˆ†ç•Œç‚¹
    source_element = soup.find(string=re.compile(r'å…¨éƒ¨æ¥æº|All Sources'))
    if not source_element:
        if log:
            log("âŒ æœªæ‰¾åˆ°'å…¨éƒ¨æ¥æº'æˆ–'All Sources'æ ‡è¯†ï¼Œæ— æ³•åˆ†å‰²è¯¦ç»†å¼•ç”¨å’ŒåŸŸåæ±‡æ€»")
        return None
    
    # ç¡®å®šæ‰¾åˆ°çš„æ˜¯å“ªç§åˆ†ç•Œç¬¦
    source_text = source_element.strip()
    if log:
        log(f"âœ… æ‰¾åˆ°åˆ†ç•Œç‚¹: '{source_text}'")
    
    # æ‰¾åˆ°åˆ†ç•Œç¬¦åœ¨HTMLä¸­çš„ä½ç½®ï¼Œä»¥æ­¤ä¸ºåˆ†ç•Œç‚¹
    html_str = str(soup)
    source_position = html_str.find('å…¨éƒ¨æ¥æº')
    if source_position == -1:
        source_position = html_str.find('All Sources')
    
    # å°†æ‰€æœ‰é“¾æ¥æŒ‰ä½ç½®åˆ†ç±»
    detailed_references = []
    domain_summary = []
    
    for link in all_links:
        link_position = str(soup).find(str(link))
        if link_position < source_position:
            # åœ¨"å…¨éƒ¨æ¥æº"ä¹‹å‰çš„ä¸ºè¯¦ç»†å¼•ç”¨
            ref_info = extract_detailed_reference_simple(link, len(detailed_references) + 1)
            if ref_info:
                detailed_references.append(ref_info)
        else:
            # åœ¨"å…¨éƒ¨æ¥æº"ä¹‹åçš„ä¸ºåŸŸåæ±‡æ€»
            domain_info = extract_domain_info_simple(link, len(domain_summary) + 1)
            if domain_info:
                domain_summary.append(domain_info)
    
    if log:
        log(f"âœ… è¯¦ç»†å¼•ç”¨è§£æå®Œæˆ: {len(detailed_references)}ä¸ª")
        log(f"âœ… åŸŸåæ±‡æ€»è§£æå®Œæˆ: {len(domain_summary)}ä¸ª")
    
    return {
        'detailed_references': detailed_references,
        'domain_summary': domain_summary
    }

def extract_detailed_reference_simple(link, index):
    """
    ç®€åŒ–ç‰ˆè¯¦ç»†å¼•ç”¨æå–ï¼Œä½¿ç”¨divç»“æ„å‡†ç¡®åˆ†ç¦»titleå’Œdescription
    """
    href = link.get('href')
    if not href:
        return None
    
    # è§£æURLè·å–domain
    parsed_url = urlparse(href)
    domain = parsed_url.netloc
    
    # æŸ¥æ‰¾å†…éƒ¨divç»“æ„
    divs = link.find_all('div')
    
    title = ""
    description = ""
    
    if len(divs) >= 3:
        # æ ‡å‡†çš„3ä¸ªdivç»“æ„
        # div 1: åŸŸå (å¿½ç•¥)
        # div 2: æ ‡é¢˜
        # div 3: æè¿°
        title = divs[1].get_text(strip=True)
        description = divs[2].get_text(strip=True)
    elif len(divs) == 2:
        # 2ä¸ªdivçš„æƒ…å†µ
        title = divs[0].get_text(strip=True)
        description = divs[1].get_text(strip=True)
    else:
        # å›é€€æ–¹æ¡ˆï¼šä½¿ç”¨å…¨éƒ¨æ–‡æœ¬
        display_text = link.get_text(strip=True)
        title = display_text[:100] + "..." if len(display_text) > 100 else display_text
        description = display_text
    
    return {
        'index': index,
        'domain': domain,
        'url': href,
        'title': title,
        'description': description
    }

def extract_domain_info_simple(link, index):
    """
    ç®€åŒ–ç‰ˆåŸŸåä¿¡æ¯æå–
    """
    href = link.get('href')
    if not href:
        return None
    
    display_text = link.get_text(strip=True)
    
    # ä»display_textä¸­æå–åŸŸåå’Œè®¡æ•°
    domain = display_text
    count = 1
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ•°å­—åç¼€ï¼ˆå¦‚"sohu2"è¡¨ç¤º2æ¬¡ï¼‰
    count_match = re.search(r'(\d+)$', display_text)
    if count_match:
        count = int(count_match.group(1))
        domain = re.sub(r'\d+$', '', display_text)
    
    return {
        'index': index,
        'domain': domain,
        'href': href,
        'count': count
    }

def classify_url_type(url):
    """
    æ ¹æ®URLåˆ†ç±»ç½‘ç«™ç±»å‹ï¼ˆä¿ç•™æ­¤å‡½æ•°ä»¥é˜²éœ€è¦ï¼‰
    """
    domain = urlparse(url).netloc.lower()
    
    if any(keyword in domain for keyword in ['law', 'æ³•å¾‹', 'legal']):
        return 'æ³•å¾‹å’¨è¯¢'
    elif any(keyword in domain for keyword in ['news', 'daily', 'æ–°é—»']):
        return 'æ–°é—»åª’ä½“'
    elif any(keyword in domain for keyword in ['zhihu', 'weibo', 'sina', 'sohu', 'ifeng']):
        return 'ç¤¾äº¤å¹³å°'
    elif any(keyword in domain for keyword in ['fang', 'house', 'rent', 'ç§Ÿæˆ¿', 'hexun', 'ziroom']):
        return 'ç§Ÿæˆ¿å¹³å°'
    elif any(keyword in domain for keyword in ['gov', 'æ”¿åºœ']):
        return 'æ”¿åºœå®˜æ–¹'
    elif any(keyword in domain for keyword in ['edu', 'æ•™è‚²', 'libaedu']):
        return 'å­¦æœ¯æ•™è‚²'
    else:
        return 'å…¶ä»–'

def main():
    import sys
    
    # æ£€æŸ¥æ˜¯å¦æä¾›äº†æ–‡ä»¶è·¯å¾„å‚æ•°
    if len(sys.argv) > 1:
        json_file_path = sys.argv[1]
    else:
        json_file_path = 'temp/openai-01.json'  # é»˜è®¤æ–‡ä»¶
    
    print(f"ğŸ“ å¤„ç†æ–‡ä»¶: {json_file_path}")
    
    try:
        result = extract_reference_structured(json_file_path)
        
        if not result:
            print("âŒ è§£æå¤±è´¥")
            return
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        import os
        base_name = os.path.splitext(os.path.basename(json_file_path))[0]
        output_file = f'reference_structured_from_{base_name}.json'
        
        # ä¿å­˜ç»“æ„åŒ–æ•°æ®åˆ°æ–‡ä»¶
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        print("âœ… Referenceå­—æ®µè§£æå®Œæˆï¼")
        print(f"ğŸ“Š è§£ææ‘˜è¦:")
        print(f"ğŸ”— è¯¦ç»†å¼•ç”¨é“¾æ¥: {len(result['detailed_references'])}ä¸ª")
        print(f"ğŸ·ï¸ åŸŸåæ±‡æ€»: {len(result['domain_summary'])}ä¸ª")
        print(f"ğŸ“ ç»“æ„åŒ–æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
        
    except Exception as e:
        print(f"âŒ å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 