{"bib_id":"BanerjeeL05","title":"METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments","author":"Banerjee, Satanjeev and Lavie, Alon","meta_info":{"publisher":"Association for Computational Linguistics","pages":"65--72","year":"2005","booktitle":"Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and\/or Summarization"}}
{"bib_id":"Cohen60","title":"A Coefficient of Agreement for Nominal Scales - Jacob Cohen, 1960","author":"Cohen, Jacob","meta_info":{"urldate":"2024-03-27","year":"1960"}}
{"bib_id":"DeutschBR21","title":"Towards Question-Answering as an Automatic Metric for Evaluating the Content Quality of a Summary","author":"Deutsch, Daniel and Bedrax-Weiss, Tania and Roth, Dan","meta_info":{"doi":"10.1162\/tacl_a_00397","publisher":"MIT Press","pages":"774--789","volume":"9","journal":"Transactions of the Association for Computational Linguistics","year":"2021"}}
{"bib_id":"DurmusHD20a","title":"FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization","author":"Durmus, Esin and He, He and Diab, Mona","meta_info":{"doi":"10.18653\/v1\/2020.acl-main.454","publisher":"Association for Computational Linguistics","pages":"5055--5070","year":"2020","booktitle":"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"}}
{"bib_id":"ElderGOL18","title":"E2E NLG Challenge Submission: Towards Controllable Generation of Diverse Natural Language","author":"Elder, Henry and Gehrmann, Sebastian and O'Connor, Alexander and Liu, Qun","meta_info":{"doi":"10.18653\/v1\/W18-6556","publisher":"Association for Computational Linguistics","pages":"457--462","year":"2018","booktitle":"Proceedings of the 11th International Conference on Natural Language Generation"}}
{"bib_id":"FinnL92","title":"Determining the Appropriate Response to Evidence of Public Concern: The Case of Food Safety","author":"Finn, Adam and Louviere, Jordan J.","meta_info":{"jstor":"30000270","abstract":"With social issues again close to the top of the research agenda for marketers, policy makers are increasingly likely to be presented public opinion research findings that suggest the public is very concerned about issues within their jurisdiction. Such findings have some shortcomings as a basis for policy decision making however. It is often difficult to interpret the practical importance of consumers' expressed levels of concern, and concern alone tells nothing about what actions will be seen as ameliorating the problem. Alternative methods, borrowed from more mainstream marketing research, can readily overcome these shortcomings. To illustrate, the authors report the findings of a low cost study designed to identify what citizens want a government department to do in response to polling suggesting high levels of concern about food safety.","urldate":"2024-03-04","doi":"10.1177\/074391569201100202","issn":"0743-9156","publisher":"American Marketing Association","pages":"12--25","number":"2","volume":"11","journal":"Journal of Public Policy & Marketing","year":"1992","shorttitle":"Determining the Appropriate Response to Evidence of Public Concern"}}
{"bib_id":"Fleiss71a","title":"Measuring Nominal Scale Agreement among Many Raters","author":"Fleiss, Joseph L.","meta_info":{"abstract":"Introduced the statistic kappa to measure nominal scale agreement between a fixed pair of raters. Kappa was generalized to the case where each of a sample of 30 patients was rated on a nominal scale by the same number of psychiatrist raters (n = 6), but where the raters rating 1 s were not necessarily the same as those rating another. Large sample standard errors were derived. (PsycINFO Database Record (c) 2016 APA, all rights reserved)","doi":"10.1037\/h0031619","issn":"1939-1455","publisher":"American Psychological Association","pages":"378--382","number":"5","volume":"76","journal":"Psychological Bulletin","year":"1971"}}
{"bib_id":"JelinekMBB77","title":"Perplexity---a Measure of the Difficulty of Speech Recognition Tasks","author":"Jelinek, F. and Mercer, R. L. and Bahl, L. R. and Baker, J. K.","meta_info":{"language":"en","langid":"english","abstract":"Using counterexamples, we show that vocabulary size and static and dynamic branching factors are all inadequate as measures of speech recognition complexity of finite state grammars. Information theoretic arguments show that perplexity (the logarithm of which is the familiar entropy) is a more appropriate measure of equivalent choice. It too has certain weaknesses which we discuss. We show that perplexity can also be applied to languages having no obvious statistical description, since an entropy-maximizing probability assignment can be found for any finite-state grammar. Table I shows perplexity values for some well-known speech recognition tasks. Perplexity Vocabulary Dynamic Phone Word size branching factor IBM-Lasers 2.14 21.11 1000 1000 IBM-Raleigh 1.69 7.74 250 7.32 CMU-AIX05 1.52 6.41 1011 35","urldate":"2024-03-27","doi":"10.1121\/1.2016299","issn":"0001-4966, 1520-8524","pages":"S63-S63","number":"S1","volume":"62","journal":"The Journal of the Acoustical Society of America","year":"1977"}}
{"bib_id":"KhalifaBM21","title":"A Bag of Tricks for Dialogue Summarization","author":"Khalifa, Muhammad and Ballesteros, Miguel and McKeown, Kathleen","meta_info":{"doi":"10.18653\/v1\/2021.emnlp-main.631","publisher":"Association for Computational Linguistics","pages":"8014--8022","year":"2021","booktitle":"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing"}}
{"bib_id":"Krippendorff70","title":"Bivariate Agreement Coefficients for Reliability of Data","author":"Krippendorff, Klaus","meta_info":{"jstor":"270787","urldate":"2024-03-04","doi":"10.2307\/270787","issn":"0081-1750","publisher":"[American Sociological Association, Wiley, Sage Publications, Inc.]","pages":"139--150","volume":"2","journal":"Sociological Methodology","year":"1970"}}
{"bib_id":"KryscinskiMXS20a","title":"Evaluating the Factual Consistency of Abstractive Text Summarization","author":"Kryscinski, Wojciech and McCann, Bryan and Xiong, Caiming and Socher, Richard","meta_info":{"doi":"10.18653\/v1\/2020.emnlp-main.750","publisher":"Association for Computational Linguistics","pages":"9332--9346","year":"2020","booktitle":"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)"}}
{"bib_id":"Likert32a","title":"A Technique for the Measurement of Attitudes","author":"Likert, R.","meta_info":{"abstract":"The project conceived in 1929 by Gardner Murphy and the writer aimed first to present a wide array of problems having to do with five major \"attitude areas\"---international relations, race relations, economic conflict, political conflict, and religion. The kind of questionnaire material falls into four classes: yes-no, multiple choice, propositions to be responded to by degrees of approval, and a series of brief newspaper narratives to be approved or disapproved in various degrees. The monograph aims to describe a technique rather than to give results. The appendix, covering ten pages, shows the method of constructing an attitude scale. A bibliography is also given. (PsycINFO Database Record (c) 2016 APA, all rights reserved)","pages":"55--55","volume":"22 140","journal":"Archives of Psychology","year":"1932","ids":"Likert32"}}
{"bib_id":"Lin04","title":"ROUGE: A Package for Automatic Evaluation of Summaries","author":"Lin, Chin-Yew","meta_info":{"publisher":"Association for Computational Linguistics","pages":"74--81","year":"2004","booktitle":"Text Summarization Branches Out"}}
{"bib_id":"PapineniRWZ02","title":"Bleu: A Method for Automatic Evaluation of Machine Translation","author":"Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing","meta_info":{"doi":"10.3115\/1073083.1073135","publisher":"Association for Computational Linguistics","pages":"311--318","year":"2002","booktitle":"Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics"}}
{"bib_id":"PearsonG95","title":"VII. Note on Regression and Inheritance in the Case of Two Parents","author":"Pearson, Karl and Galton, Francis","meta_info":{"abstract":"Consider a population in which sexual selection and natural selection may or may not be taking place. Assume only that the deviations from the mean in the case of any organ of any generation follow exactly or closely the normal law of frequency, then the following expressions may be shown to give the law of inheritance of the population.","urldate":"2024-03-27","doi":"10.1098\/rspl.1895.0041","publisher":"Royal Society","pages":"240--242","number":"347-352","volume":"58","journal":"Proceedings of the Royal Society of London","year":"1895"}}
{"bib_id":"Popovic17","title":"chrF++: Words Helping Character n-Grams","author":"PopoviÄ‡, Maja","meta_info":{"doi":"10.18653\/v1\/W17-4770","publisher":"Association for Computational Linguistics","pages":"612--618","year":"2017","booktitle":"Proceedings of the Second Conference on Machine Translation"}}
{"bib_id":"ScialomLPS19","title":"Answers Unite! Unsupervised Metrics for Reinforced Summarization Models","author":"Scialom, Thomas and Lamprier, Sylvain and Piwowarski, Benjamin and Staiano, Jacopo","meta_info":{"doi":"10.18653\/v1\/D19-1320","publisher":"Association for Computational Linguistics","pages":"3246--3256","year":"2019","booktitle":"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)"}}
{"bib_id":"SellamDP20","title":"BLEURT: Learning Robust Metrics for Text Generation","author":"Sellam, Thibault and Das, Dipanjan and Parikh, Ankur","meta_info":{"doi":"10.18653\/v1\/2020.acl-main.704","publisher":"Association for Computational Linguistics","pages":"7881--7892","year":"2020","booktitle":"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"}}
{"bib_id":"Spearman04a","title":"The Proof and Measurement of Association between Two Things","author":"Spearman, C.","meta_info":{"jstor":"1412159","urldate":"2024-03-27","doi":"10.2307\/1412159","issn":"0002-9556","publisher":"University of Illinois Press","pages":"72--101","number":"1","volume":"15","journal":"The American Journal of Psychology","year":"1904"}}
{"bib_id":"VasilyevDB20a","title":"Fill in the BLANC: Human-free Quality Estimation of Document Summaries","author":"Vasilyev, Oleg and Dharnidharka, Vedant and Bohannon, John","meta_info":{"doi":"10.18653\/v1\/2020.eval4nlp-1.2","publisher":"Association for Computational Linguistics","pages":"11--20","year":"2020","booktitle":"Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems"}}
{"bib_id":"YuanNL21","title":"BARTScore: Evaluating Generated Text as Text Generation","author":"Yuan, Weizhe and Neubig, Graham and Liu, Pengfei","meta_info":{"timestamp":"Tue, 03 May 2022 01:00:00 +0200","pages":"27263--27277","year":"2021","editor":"Ranzato, Marc'Aurelio and Beygelzimer, Alina and Dauphin, Yann N. and Liang, Percy and Vaughan, Jennifer Wortman","booktitle":"Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, Virtual"}}
{"bib_id":"ZhangKWW20a","title":"BERTScore: Evaluating Text Generation with BERT","author":"Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q. and Artzi, Yoav","meta_info":{"timestamp":"Wed, 03 Jun 2020 01:00:00 +0200","publisher":"OpenReview.net","year":"2020","booktitle":"8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020"}}
{"bib_id":"ZhaoPLG19","title":"MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance","author":"Zhao, Wei and Peyrard, Maxime and Liu, Fei and Gao, Yang and Meyer, Christian M. and Eger, Steffen","meta_info":{"doi":"10.18653\/v1\/D19-1053","publisher":"Association for Computational Linguistics","pages":"563--578","year":"2019","booktitle":"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)"}}
{"bib_id":"KirsteinRG24b","title":"Is My Meeting Summary Good? Estimating Quality with a Multi-LLM Evaluator","author":"Kirstein, Frederic and Ruas, Terry and Gipp, Bela","meta_info":{"file":"\/Users\/freddy\/Zotero\/storage\/9D295GLN\/KirsteinRG24b--FK--is_my_meeting_summary_good_estimating_quality_with_a_multi-llm_evaluator.pdf;\/Users\/freddy\/Zotero\/storage\/3AZAMTR4\/2411.html","archiveprefix":"arXiv","abstract":"The quality of meeting summaries generated by natural language generation (NLG) systems is hard to measure automatically. Established metrics such as ROUGE and BERTScore have a relatively low correlation with human judgments and fail to capture nuanced errors. Recent studies suggest using large language models (LLMs), which have the benefit of better context understanding and adaption of error definitions without training on a large number of human preference judgments. However, current LLM-based evaluators risk masking errors and can only serve as a weak proxy, leaving human evaluation the gold standard despite being costly and hard to compare across studies. In this work, we present MESA, an LLM-based framework employing a three-step assessment of individual error types, multi-agent discussion for decision refinement, and feedback-based self-training to refine error definition understanding and alignment with human judgment. We show that MESA's components enable thorough error detection, consistent rating, and adaptability to custom error guidelines. Using GPT-4o as its backbone, MESA achieves mid to high Point-Biserial correlation with human judgment in error detection and mid Spearman and Kendall correlation in reflecting error impact on summary quality, on average 0.25 higher than previous methods. The framework's flexibility in adapting to custom error guidelines makes it suitable for various tasks with limited human-labeled data.","urldate":"2024-12-12","doi":"10.48550\/arXiv.2411.18444","publisher":"arXiv","primaryclass":"cs","eprint":"2411.18444","number":"arXiv:2411.18444","month":"November","year":"2024","shorttitle":"Is My Meeting Summary Good?"}}
{"bib_id":"KirsteinRG24c","title":"What's Wrong? Refining Meeting Summaries with LLM Feedback","author":"Kirstein, Frederic and Ruas, Terry and Gipp, Bela","meta_info":{"file":"\/Users\/freddy\/Zotero\/storage\/MB32HB2K\/KirsteinRG24c--FK--whats_wrong_refining_meeting_summaries_with_llm_feedback.pdf;\/Users\/freddy\/Zotero\/storage\/VQN922E2\/2407.html","archiveprefix":"arXiv","abstract":"Meeting summarization has become a critical task since digital encounters have become a common practice. Large language models (LLMs) show great potential in summarization, offering enhanced coherence and context understanding compared to traditional methods. However, they still struggle to maintain relevance and avoid hallucination. We introduce a multi-LLM correction approach for meeting summarization using a two-phase process that mimics the human review process: mistake identification and summary refinement. We release QMSum Mistake, a dataset of 200 automatically generated meeting summaries annotated by humans on nine error types, including structural, omission, and irrelevance errors. Our experiments show that these errors can be identified with high accuracy by an LLM. We transform identified mistakes into actionable feedback to improve the quality of a given summary measured by relevance, informativeness, conciseness, and coherence. This post-hoc refinement effectively improves summary quality by leveraging multiple LLMs to validate output quality. Our multi-LLM approach for meeting summarization shows potential for similar complex text generation tasks requiring robustness, action planning, and discussion towards a goal.","urldate":"2024-12-12","doi":"10.48550\/arXiv.2407.11919","publisher":"arXiv","primaryclass":"cs","eprint":"2407.11919","number":"arXiv:2407.11919","month":"July","year":"2024","shorttitle":"What's Wrong?"}}
