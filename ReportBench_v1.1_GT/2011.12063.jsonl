{"bib_id":"1810.12656","title":"Generative Adversarial Networks for Unpaired Voice Transformation on Impaired Speech","author":"Li-Wei Chen and Hung-Yi Lee and Yu Tsao","meta_info":{"eprint":"arXiv:1810.12656","year":"2018"}}
{"bib_id":"inproceedings_2","title":"Unsupervised End-to-End Learning of Discrete Linguistic Units for Voice Conversion","author":"Liu, Andy and Hsu, Po-chun and Lee, Hung-yi","meta_info":{"doi":"10.21437\/Interspeech.2019-2048","pages":"1108-1112","month":"09","year":"2019"}}
{"bib_id":"inproceedings","title":"One-Shot Voice Conversion with Global Speaker Embeddings","author":"Lu, Hui and Wu, Zhiyong and Dai, Dongyang and Li, Runnan and Kang, Shiyin and Jia, Jia and Meng, Helen","meta_info":{"doi":"10.21437\/Interspeech.2019-2365","pages":"669-673","month":"09","year":"2019"}}
{"bib_id":"2004.11284","title":"Unsupervised Speech Decomposition via Triple Information Bottleneck","author":"Kaizhi Qian and Yang Zhang and Shiyu Chang and David Cox and Mark Hasegawa-Johnson","meta_info":{"eprint":"arXiv:2004.11284","year":"2020"}}
{"bib_id":"2005.08781","title":"Defending Your Voice: Adversarial Attack on Voice Conversion","author":"Chien-yu Huang and Yist Y. Lin and Hung-yi Lee and Lin-shan Lee","meta_info":{"eprint":"arXiv:2005.08781","year":"2020"}}
{"bib_id":"griffin1984signal","title":"Signal estimation from modified short-time Fourier transform","author":"Griffin, Daniel and Lim, Jae","meta_info":{"publisher":"IEEE","year":"1984","pages":"236--243","number":"2","volume":"32","journal":"IEEE Transactions on Acoustics, Speech, and Signal Processing"}}
{"bib_id":"huang2017arbitrary","title":"Arbitrary style transfer in real-time with adaptive instance normalization","author":"Huang, Xun and Belongie, Serge","meta_info":{"year":"2017","pages":"1501--1510","booktitle":"Proceedings of the IEEE International Conference on Computer Vision"}}
{"bib_id":"8553236","title":"CycleGAN-VC: Non-parallel Voice Conversion Using Cycle-Consistent Adversarial Networks","author":"T. Kaneko and H. Kameoka","meta_info":{"pages":"2100-2104","number":"","volume":"","year":"2018","booktitle":"2018 26th European Signal Processing Conference (EUSIPCO)"}}
{"bib_id":"kameoka2018stargan","title":"Stargan-vc: Non-parallel many-to-many voice conversion using star generative adversarial networks","author":"Kameoka, Hirokazu and Kaneko, Takuhiro and Tanaka, Kou and Hojo, Nobukatsu","meta_info":{"organization":"IEEE","year":"2018","pages":"266--273","booktitle":"2018 IEEE Spoken Language Technology Workshop (SLT)"}}
{"bib_id":"chou2018multi","title":"Multi-target Voice Conversion without Parallel Data by Adversarially Learning Disentangled Audio Representations","author":"Chou, Ju-chieh and Yeh, Cheng-chieh and Lee, Hung-yi and Lee, Lin-shan","meta_info":{"year":"2018","pages":"501--505","journal":"Proc. Interspeech 2018"}}
{"bib_id":"goodfellow2014generative","title":"Generative adversarial nets","author":"Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua","meta_info":{"year":"2014","pages":"2672--2680","booktitle":"Advances in neural information processing systems"}}
{"bib_id":"chou2019one","title":"One-Shot Voice Conversion by Separating Speaker and Content Representations with Instance Normalization","author":"Chou, Ju-chieh and Lee, Hung-Yi","meta_info":{"year":"2019","pages":"664--668","journal":"Proc. Interspeech 2019"}}
{"bib_id":"kingma2013auto","title":"Auto-encoding variational bayes","author":"Kingma, Diederik P and Welling, Max","meta_info":{"year":"2013","journal":"arXiv preprint arXiv:1312.6114"}}
{"bib_id":"qian2019autovc","title":"AutoVC: Zero-Shot Voice Style Transfer with Only Autoencoder Loss","author":"Qian, Kaizhi and Zhang, Yang and Chang, Shiyu and Yang, Xuesong and Hasegawa-Johnson, Mark","meta_info":{"year":"2019","pages":"5210--5219","booktitle":"International Conference on Machine Learning"}}
{"bib_id":"kumar2019melgan","title":"Melgan: Generative adversarial networks for conditional waveform synthesis","author":"Kumar, Kundan and Kumar, Rithesh and de Boissiere, Thibault and Gestin, Lucas and Teoh, Wei Zhen and Sotelo, Jose and de Brébisson, Alexandre and Bengio, Yoshua and Courville, Aaron C","meta_info":{"year":"2019","pages":"14910--14921","booktitle":"Advances in Neural Information Processing Systems"}}
{"bib_id":"wan2018generalized","title":"Generalized end-to-end loss for speaker verification","author":"Wan, Li and Wang, Quan and Papir, Alan and Moreno, Ignacio Lopez","meta_info":{"organization":"IEEE","year":"2018","pages":"4879--4883","booktitle":"2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"}}
{"bib_id":"van_den_Oord+2016","title":"WaveNet: A Generative Model for Raw Audio","author":"Aäron van den Oord and Sander Dieleman and Heiga Zen and Karen Simonyan and Oriol Vinyals and Alex Graves and Nal Kalchbrenner and Andrew Senior and Koray Kavukcuoglu","meta_info":{"pages":"125--125","booktitle":"9th ISCA Speech Synthesis Workshop","year":"2016"}}
{"bib_id":"veaux2016superseded","title":"Superseded-cstr vctk corpus: English multi-speaker corpus for cstr voice cloning toolkit","author":"Veaux, Christophe and Yamagishi, Junichi and MacDonald, Kirsten and others","meta_info":{"publisher":"University of Edinburgh. The Centre for Speech Technology Research (CSTR)","year":"2016"}}
{"bib_id":"zen2019libritts","title":"LibriTTS: A Corpus Derived from LibriSpeech for Text-to-Speech","author":"Zen, Heiga and Dang, Viet and Clark, Rob and Zhang, Yu and Weiss, Ron J and Jia, Ye and Chen, Zhifeng and Wu, Yonghui","meta_info":{"year":"2019","pages":"1526--1530","journal":"Proc. Interspeech 2019"}}
{"bib_id":"panayotov2015librispeech","title":"Librispeech: an asr corpus based on public domain audio books","author":"Panayotov, Vassil and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev","meta_info":{"organization":"IEEE","year":"2015","pages":"5206--5210","booktitle":"2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"}}
{"bib_id":"kominek2004cmu","title":"The CMU Arctic speech databases","author":"Kominek, John and Black, Alan W","meta_info":{"year":"2004","booktitle":"Fifth ISCA workshop on speech synthesis"}}
{"bib_id":"wang2015thchs","title":"Thchs-30: A free chinese speech corpus","author":"Wang, Dong and Zhang, Xuewei","meta_info":{"year":"2015","journal":"arXiv preprint arXiv:1512.01882"}}
{"bib_id":"1420369","title":"Eigenvoice modeling with sparse training data","author":"P. Kenny and G. Boulianne and P. Dumouchel","meta_info":{"pages":"345-354","number":"3","volume":"13","year":"2005","journal":"IEEE Transactions on Speech and Audio Processing"}}
{"bib_id":"6854363","title":"Deep neural networks for small footprint text-dependent speaker verification","author":"E. Variani and X. Lei and E. McDermott and I. L. Moreno and J. Gonzalez-Dominguez","meta_info":{"pages":"4052-4056","number":"","volume":"","year":"2014","booktitle":"2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"}}
{"bib_id":"8461375","title":"X-Vectors: Robust DNN Embeddings for Speaker Recognition","author":"D. Snyder and D. Garcia-Romero and G. Sell and D. Povey and S. Khudanpur","meta_info":{"pages":"5329-5333","number":"","volume":"","year":"2018","booktitle":"2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"}}
{"bib_id":"Nagrani17","title":"VoxCeleb: a large-scale speaker identification dataset","author":"Nagrani, A. and Chung, J.~S. and Zisserman, A.","meta_info":{"year":"2017","booktitle":"INTERSPEECH"}}
{"bib_id":"Chung18b","title":"VoxCeleb2: Deep Speaker Recognition","author":"Chung, J.~S. and Nagrani, A. and Zisserman, A.","meta_info":{"year":"2018","booktitle":"INTERSPEECH"}}
{"bib_id":"Povey:192584","title":"The Kaldi Speech Recognition Toolkit","author":"Povey, Daniel and Ghoshal, Arnab and Boulianne, Gilles and  Burget, Lukas and Glembek, Ondrej and Goel, Nagendra and  Hannemann, Mirko and Motlicek, Petr and Qian, Yanmin and  Schwarz, Petr and Silovsky, Jan and Stemmer, Georg and  Vesely, Karel","meta_info":{"url":"http:\/\/infoscience.epfl.ch\/record\/192584","abstract":"We describe the design of Kaldi, a free, open-source  toolkit for speech recognition research. Kaldi provides a  speech recognition system based on finite-state transducers  (using the freely available OpenFst), together with  detailed documentation and scripts for building complete  recognition systems. Kaldi is written is C++, and the core  library supports modeling of arbitrary phonetic-context  sizes, acoustic modeling with subspace Gaussian mixture  models (SGMM) as well as standard Gaussian mixture models,  together with all commonly used linear and affine  transforms. Kaldi is released under the Apache License  v2.0, which is highly nonrestrictive, making it suitable  for a wide community of users.","note":"IEEE Catalog No.: CFP11SRW-USB","year":"2011","publisher":"IEEE Signal Processing Society"}}
{"bib_id":"doi:10.1111\/j.1749-6632.1987.tb48734.x","title":"Fractal Time and 1\/f Noise in Complex Systems","author":"SHLESINGER, MICHAEL F.","meta_info":{"year":"1987","eprint":"https:\/\/nyaspubs.onlinelibrary.wiley.com\/doi\/pdf\/10.1111\/j.1749-6632.1987.tb48734.x","url":"https:\/\/nyaspubs.onlinelibrary.wiley.com\/doi\/abs\/10.1111\/j.1749-6632.1987.tb48734.x","doi":"10.1111\/j.1749-6632.1987.tb48734.x","pages":"214-228","number":"1","volume":"504","journal":"Annals of the New York Academy of Sciences"}}
{"bib_id":"2006.04154","title":"VQVC+: One-Shot Voice Conversion by Vector Quantization and U-Net architecture","author":"Da-Yi Wu and Yen-Hao Chen and Hung-Yi Lee","meta_info":{"eprint":"arXiv:2006.04154","year":"2020"}}
{"bib_id":"serra2019blow","title":"Blow: a single-scale hyperconditioned flow for non-parallel raw-audio voice conversion","author":"Serrà, Joan and Pascual, Santiago and Perales, Carlos Segura","meta_info":{"year":"2019","pages":"6793--6803","booktitle":"Advances in Neural Information Processing Systems"}}
{"bib_id":"7820786","title":"Voice conversion from non-parallel corpora using variational auto-encoder","author":"C. Hsu and H. Hwang and Y. Wu and Y. Tsao and H. Wang","meta_info":{"pages":"1-6","number":"","volume":"","year":"2016","booktitle":"2016 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)"}}
{"bib_id":"Liu2018","title":"Voice Conversion Across Arbitrary Speakers Based on a Single Target-Speaker Utterance","author":"Songxiang Liu and Jinghua Zhong and Lifa Sun and Xixin Wu and Xunying Liu and Helen Meng","meta_info":{"url":"http:\/\/dx.doi.org\/10.21437\/Interspeech.2018-1504","doi":"10.21437\/Interspeech.2018-1504","pages":"496--500","booktitle":"Proc. Interspeech 2018","year":"2018"}}
