{"bib_id":"mishra2020dqi","title":"Dqi: Measuring data quality in nlp","author":"Mishra, Swaroop and Arunkumar, Anjana and Sachdeva, Bhavdeep and Bryan, Chris and Baral, Chitta","meta_info":{"year":"2020","journal":"arXiv preprint arXiv:2005.00816"}}
{"bib_id":"mishra2020dqia","title":"Dqi: A guide to benchmark evaluation","author":"Mishra, Swaroop and Arunkumar, Anjana and Sachdeva, Bhavdeep and Bryan, Chris and Baral, Chitta","meta_info":{"year":"2020","journal":"arXiv preprint arXiv:2008.03964"}}
{"bib_id":"lv2024codeact","title":"CodeACT: Code Adaptive Compute-efficient Tuning Framework for Code LLMs","author":"Lv, Weijie and Xia, Xuan and Huang, Sheng-Jun","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2408.02193"}}
{"bib_id":"pang2024improving","title":"Improving Data Efficiency via Curating LLM-Driven Rating Systems","author":"Pang, Jinlong and Wei, Jiaheng and Shah, Ankit Parag and Zhu, Zhaowei and Wang, Yaxuan and Qian, Chen and Liu, Yang and Bao, Yujia and Wei, Wei","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2410.10877"}}
{"bib_id":"zhuang2024bread","title":"Bread: A Hybrid Approach for Instruction Data Mining Through Balanced Retrieval and Dynamic Data Sampling","author":"Zhuang, Xinlin and Mao, Xin and Jiang, Yuan-Hao and Wu, Hongyi and Zhao, Shangqing and Cai, Li and Liu, Shu and Chen, Yang and Song, Yuxiang and Jia, Chenghao and others","meta_info":{"organization":"Springer","year":"2024","pages":"229--240","booktitle":"CCF International Conference on Natural Language Processing and Chinese Computing"}}
{"bib_id":"xia2024rethinking","title":"Rethinking data selection at scale: Random selection is almost all you need","author":"Xia, Tingyu and Yu, Bowen and Dang, Kai and Yang, An and Wu, Yuan and Tian, Yuan and Chang, Yi and Lin, Junyang","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2410.09335"}}
{"bib_id":"gallegos2024bias","title":"Bias and fairness in large language models: A survey","author":"Gallegos, Isabel O and Rossi, Ryan A and Barrow, Joe and Tanjim, Md Mehrab and Kim, Sungchul and Dernoncourt, Franck and Yu, Tong and Zhang, Ruiyi and Ahmed, Nesreen K","meta_info":{"publisher":"MIT Press 255 Main Street, 9th Floor, Cambridge, Massachusetts 02142, USA~…","year":"2024","pages":"1--79","journal":"Computational Linguistics"}}
{"bib_id":"smith2022m","title":"\" I'm sorry to hear that\": Finding New Biases in Language Models with a Holistic Descriptor Dataset","author":"Smith, Eric Michael and Hall, Melissa and Kambadur, Melanie and Presani, Eleonora and Williams, Adina","meta_info":{"year":"2022","journal":"arXiv preprint arXiv:2205.09209"}}
{"bib_id":"bordia2019identifying","title":"Identifying and reducing gender bias in word-level language models","author":"Bordia, Shikha and Bowman, Samuel R","meta_info":{"year":"2019","journal":"arXiv preprint arXiv:1904.03035"}}
{"bib_id":"nangia2020crows","title":"CrowS-pairs: A challenge dataset for measuring social biases in masked language models","author":"Nangia, Nikita and Vania, Clara and Bhalerao, Rasika and Bowman, Samuel R","meta_info":{"year":"2020","journal":"arXiv preprint arXiv:2010.00133"}}
{"bib_id":"webster2020measuring","title":"Measuring and reducing gendered correlations in pre-trained models","author":"Webster, Kellie and Wang, Xuezhi and Tenney, Ian and Beutel, Alex and Pitler, Emily and Pavlick, Ellie and Chen, Jilin and Chi, Ed and Petrov, Slav","meta_info":{"year":"2020","journal":"arXiv preprint arXiv:2010.06032"}}
{"bib_id":"may2019measuring","title":"On measuring social biases in sentence encoders","author":"May, Chandler and Wang, Alex and Bordia, Shikha and Bowman, Samuel R and Rudinger, Rachel","meta_info":{"year":"2019","journal":"arXiv preprint arXiv:1903.10561"}}
{"bib_id":"caliskan2017semantics","title":"Semantics derived automatically from language corpora contain human-like biases","author":"Caliskan, Aylin and Bryson, Joanna J and Narayanan, Arvind","meta_info":{"publisher":"American Association for the Advancement of Science","year":"2017","pages":"183--186","number":"6334","volume":"356","journal":"Science"}}
{"bib_id":"chu2024fairness","title":"Fairness in large language models: A taxonomic survey","author":"Chu, Zhibo and Wang, Zichong and Zhang, Wenbin","meta_info":{"publisher":"ACM New York, NY, USA","year":"2024","pages":"34--48","number":"1","volume":"26","journal":"ACM SIGKDD explorations newsletter"}}
{"bib_id":"li2023survey","title":"A survey on fairness in large language models","author":"Li, Yingji and Du, Mengnan and Song, Rui and Wang, Xin and Wang, Ying","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2308.10149"}}
{"bib_id":"itzhak2024instructed","title":"Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias","author":"Itzhak, Itay and Stanovsky, Gabriel and Rosenfeld, Nir and Belinkov, Yonatan","meta_info":{"publisher":"MIT Press 255 Main Street, 9th Floor, Cambridge, Massachusetts 02142, USA~…","year":"2024","pages":"771--785","volume":"12","journal":"Transactions of the Association for Computational Linguistics"}}
{"bib_id":"jiao2024enhancing","title":"Enhancing Fairness in LLM Evaluations: Unveiling and Mitigating Biases in Standard-Answer-Based Evaluations","author":"Jiao, Tong and Zhang, Jian and Xu, Kui and Li, Rui and Du, Xi and Wang, Shangqi and Song, Zhenbo","meta_info":{"year":"2024","pages":"56--59","number":"1","volume":"4","booktitle":"Proceedings of the AAAI Symposium Series"}}
{"bib_id":"doan2024fairness","title":"Fairness in large language models in three hours","author":"Doan, Thang Viet and Wang, Zichong and Hoang, Nhat Nguyen Minh and Zhang, Wenbin","meta_info":{"year":"2024","pages":"5514--5517","booktitle":"Proceedings of the 33rd ACM International Conference on Information and Knowledge Management"}}
{"bib_id":"zhang2023chatgpt","title":"Is chatgpt fair for recommendation? evaluating fairness in large language model recommendation","author":"Zhang, Jizhi and Bao, Keqin and Zhang, Yang and Wang, Wenjie and Feng, Fuli and He, Xiangnan","meta_info":{"year":"2023","pages":"993--999","booktitle":"Proceedings of the 17th ACM Conference on Recommender Systems"}}
{"bib_id":"chisca2024prompting","title":"Prompting Fairness: Learning Prompts for Debiasing Large Language Models","author":"Chisca, Andrei-Victor and Rad, Andrei-Cristian and Lemnaru, Camelia","meta_info":{"year":"2024","pages":"52--62","booktitle":"Proceedings of the Fourth Workshop on Language Technology for Equality, Diversity, Inclusion"}}
{"bib_id":"gao2024fairness","title":"Fairness of Large Language Models in Education","author":"Gao, Rong and Ni, Qin and Hu, Bingying","meta_info":{"year":"2024","pages":"1","booktitle":"Proceedings of the 2024 International Conference on Intelligent Education and Computer Technology"}}
{"bib_id":"mishra2020we","title":"Do we need to create big datasets to learn a task?","author":"Mishra, Swaroop and Sachdeva, Bhavdeep Singh","meta_info":{"year":"2020","pages":"169--173","booktitle":"Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"}}
{"bib_id":"le2020adversarial","title":"Adversarial filters of dataset biases","author":"Le Bras, Ronan and Swayamdipta, Swabha and Bhagavatula, Chandra and Zellers, Rowan and Peters, Matthew and Sabharwal, Ashish and Choi, Yejin","meta_info":{"organization":"Pmlr","year":"2020","pages":"1078--1088","booktitle":"International conference on machine learning"}}
{"bib_id":"dang2024data","title":"Data quality in NLP: Metrics and a comprehensive taxonomy","author":"Dang, Vu Minh Hoang and Verma, Rakesh M","meta_info":{"organization":"Springer","year":"2024","pages":"217--229","booktitle":"International Symposium on Intelligent Data Analysis"}}
{"bib_id":"sparck1972statistical","title":"A statistical interpretation of term specificity and its application in retrieval","author":"Sparck Jones, Karen","meta_info":{"publisher":"MCB UP Ltd","year":"1972","pages":"11--21","number":"1","volume":"28","journal":"Journal of documentation"}}
{"bib_id":"saranathan2024dele","title":"DELE: Data Efficient LLM Evaluation","author":"Saranathan, Gayathri and Alam, Mahammad Parwez and Lim, James and Bhattacharya, Suparna and Wong, Soon Yee and Foltin, Martin and Xu, Cong","meta_info":{"booktitle":"ICLR 2024 Workshop on Navigating and Addressing Data Problems for Foundation Models"}}
{"bib_id":"yannakoudakis1983rules","title":"The rules of spelling errors","author":"Yannakoudakis, Emmanuel J and Fawthrop, David","meta_info":{"publisher":"Elsevier","year":"1983","pages":"87--99","number":"2","volume":"19","journal":"Information Processing & Management"}}
{"bib_id":"klare1963measurement","title":"The measurement of readability","author":"Klare, George R and others","meta_info":{"publisher":"Iowa State University Press Ames","year":"1963"}}
{"bib_id":"klare1984readability","title":"Readability","author":"Klare, George R and others","meta_info":{"year":"1984","pages":"681--744","volume":"1","journal":"Handbook of reading research"}}
{"bib_id":"dubay2004principles","title":"The principles of readability. Impact Information","author":"Dubay, William H","meta_info":{"year":"2004","journal":"Costa Mesa, CA"}}
{"bib_id":"kintsch2014reading","title":"Reading comprehension and readability in educational practice and psychological theory","author":"Kintsch, Walter and Vipond, Douglas","meta_info":{"publisher":"Psychology Press","year":"2014","pages":"329--365","booktitle":"Perspectives on memory research"}}
{"bib_id":"kemper1983measuring","title":"Measuring the inference load of a text.","author":"Kemper, Susan","meta_info":{"publisher":"American Psychological Association","year":"1983","pages":"391","number":"3","volume":"75","journal":"Journal of educational psychology"}}
{"bib_id":"chen2023maybe","title":"Maybe only 0.5% data is needed: A preliminary exploration of low training data instruction tuning","author":"Chen, Hao and Zhang, Yiming and Zhang, Qi and Yang, Hantao and Hu, Xiaomeng and Ma, Xuetao and Yanggong, Yifan and Zhao, Junbo","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2305.09246"}}
{"bib_id":"si2001statistical","title":"A statistical model for scientific readability","author":"Si, Luo and Callan, Jamie","meta_info":{"year":"2001","pages":"574--576","booktitle":"Proceedings of the tenth international conference on Information and knowledge management"}}
{"bib_id":"chan2021white","title":"a white-box deep network from the principle of maximizing rate reduction. arxiv. 2021","author":"Chan, KHR and Yu, Y and You, C and Qi, H and Wright, J and Ma, YR","meta_info":{"year":"2021","journal":"arXiv preprint arXiv:2105.10446"}}
{"bib_id":"xia2022moderate","title":"Moderate coreset: A universal method of data selection for real-world data-efficient deep learning","author":"Xia, Xiaobo and Liu, Jiale and Yu, Jun and Shen, Xu and Han, Bo and Liu, Tongliang","meta_info":{"year":"2022","booktitle":"The Eleventh International Conference on Learning Representations"}}
{"bib_id":"collins2005predicting","title":"Predicting reading difficulty with statistical language models","author":"Collins-Thompson, Kevyn and Callan, Jamie","meta_info":{"publisher":"Wiley Online Library","year":"2005","pages":"1448--1462","number":"13","volume":"56","journal":"Journal of the american society for information science and technology"}}
{"bib_id":"schwarm2005reading","title":"Reading level assessment using support vector machines and statistical language models","author":"Schwarm, Sarah E and Ostendorf, Mari","meta_info":{"year":"2005","pages":"523--530","booktitle":"Proceedings of the 43rd annual meeting of the Association for Computational Linguistics (ACL’05)"}}
{"bib_id":"lin2024dataefficientfinetuningllmbasedrecommendation","title":"Data-efficient Fine-tuning for LLM-based Recommendation","author":"Xinyu Lin and Wenjie Wang and Yongqi Li and Shuo Yang and Fuli Feng and Yinwei Wei and Tat-Seng Chua","meta_info":{"url":"https:\/\/arxiv.org\/abs\/2401.17197","primaryclass":"cs.IR","archiveprefix":"arXiv","eprint":"2401.17197","year":"2024"}}
{"bib_id":"feng2010comparison","title":"A comparison of features for automatic readability assessment","author":"Feng, Lijun and Jansche, Martin and Huenerfauth, Matt and Elhadad, Noémie","meta_info":{"year":"2010","pages":"276--284","booktitle":"Coling 2010: Posters"}}
{"bib_id":"franccois2010lisibilite","title":"La lisibilité computationnelle: un renouveau pour la lisibilité du français langue première et seconde?","author":"François, Thomas","meta_info":{"publisher":"John Benjamins Publishing Company Amsterdam\/Philadelphia","year":"2010","pages":"75--99","number":"1","volume":"160","journal":"ITL-International Journal of Applied Linguistics"}}
{"bib_id":"franccois2011apports","title":"Les apports du traitement automatique du langage à la lisibilité du français langue étrangère","author":"François, Thomas","meta_info":{"school":"Ph. D. thesis, Université Catholique de Louvain. Thesis Supervisors: Cédrick~…","year":"2011"}}
{"bib_id":"franccois2012ai","title":"An “AI readability” formula for French as a foreign language","author":"François, Thomas and Fairon, Cédrick","meta_info":{"year":"2012","pages":"466--477","booktitle":"Proceedings of the 2012 joint conference on empirical methods in Natural Language Processing and computational natural language learning"}}
{"bib_id":"franccois2012nlp","title":"Do NLP and machine learning improve traditional readability formulas?","author":"François, Thomas and Miltsakaki, Eleni","meta_info":{"year":"2012","pages":"49--57","booktitle":"Proceedings of the First Workshop on Predicting and Improving Text Readability for target reader populations"}}
{"bib_id":"ankner2024perplexed","title":"Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models","author":"Ankner, Zachary and Blakeney, Cody and Sreenivasan, Kartik and Marion, Max and Leavitt, Matthew L and Paul, Mansheej","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2405.20541"}}
{"bib_id":"ngu2024diversity","title":"Diversity Measures: Domain-Independent Proxies for Failure in Language Model Queries","author":"Ngu, Noel and Lee, Nathaniel and Shakarian, Paulo","meta_info":{"organization":"IEEE","year":"2024","pages":"176--182","booktitle":"2024 IEEE 18th International Conference on Semantic Computing (ICSC)"}}
{"bib_id":"ma2023understanding","title":"Understanding and mitigating overfitting in prompt tuning for vision-language models","author":"Ma, Chengcheng and Liu, Yang and Deng, Jiankang and Xie, Lingxi and Dong, Weiming and Xu, Changsheng","meta_info":{"publisher":"IEEE","year":"2023","pages":"4616--4629","number":"9","volume":"33","journal":"IEEE Transactions on Circuits and Systems for Video Technology"}}
{"bib_id":"zhang2024dissecting","title":"Dissecting learning and forgetting in language model finetuning","author":"Zhang, Xiao and Wu, Ji","meta_info":{"year":"2024","booktitle":"The Twelfth International Conference on Learning Representations"}}
{"bib_id":"li2023synthetic","title":"Synthetic data generation with large language models for text classification: Potential and limitations","author":"Li, Zhuoyan and Zhu, Hangxiao and Lu, Zhuoran and Yin, Ming","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2310.07849"}}
{"bib_id":"li2024synthetic","title":"Synthetic data (almost) from scratch: Generalized instruction tuning for language models","author":"Li, Haoran and Dong, Qingxiu and Tang, Zhengyang and Wang, Chaojun and Zhang, Xingxing and Huang, Haoyang and Huang, Shaohan and Huang, Xiaolong and Huang, Zeqiang and Zhang, Dongdong and others","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2402.13064"}}
{"bib_id":"dong2024self","title":"Self-play with Execution Feedback: Improving Instruction-following Capabilities of Large Language Models","author":"Dong, Guanting and Lu, Keming and Li, Chengpeng and Xia, Tingyu and Yu, Bowen and Zhou, Chang and Zhou, Jingren","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2406.13542"}}
{"bib_id":"zhao2024self","title":"SELF-GUIDE: Better Task-Specific Instruction Following via Self-Synthetic Finetuning","author":"Zhao, Chenyang and Jia, Xueying and Viswanathan, Vijay and Wu, Tongshuang and Neubig, Graham","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2407.12874"}}
{"bib_id":"bradley2023quality","title":"Quality-diversity through ai feedback","author":"Bradley, Herbie and Dai, Andrew and Teufel, Hannah and Zhang, Jenny and Oostermeijer, Koen and Bellagente, Marco and Clune, Jeff and Stanley, Kenneth and Schott, Grégory and Lehman, Joel","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2310.13032"}}
{"bib_id":"jin2024will","title":"What Will My Model Forget? Forecasting Forgotten Examples in Language Model Refinement","author":"Jin, Xisen and Ren, Xiang","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2402.01865"}}
{"bib_id":"parmar2024reuse","title":"Reuse, Don't Retrain: A Recipe for Continued Pretraining of Language Models","author":"Parmar, Jupinder and Satheesh, Sanjev and Patwary, Mostofa and Shoeybi, Mohammad and Catanzaro, Bryan","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2407.07263"}}
{"bib_id":"ibrahim2024simple","title":"Simple and scalable strategies to continually pre-train large language models","author":"Ibrahim, Adam and Thérien, Benjamin and Gupta, Kshitij and Richter, Mats L and Anthony, Quentin and Lesort, Timothée and Belilovsky, Eugene and Rish, Irina","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2403.08763"}}
{"bib_id":"jin2021lifelong","title":"Lifelong pretraining: Continually adapting language models to emerging corpora","author":"Jin, Xisen and Zhang, Dejiao and Zhu, Henghui and Xiao, Wei and Li, Shang-Wen and Wei, Xiaokai and Arnold, Andrew and Ren, Xiang","meta_info":{"year":"2021","journal":"arXiv preprint arXiv:2110.08534"}}
{"bib_id":"wang2024selective","title":"Selective forgetting: Advancing machine unlearning techniques and evaluation in language models","author":"Wang, Lingzhi and Zeng, Xingshan and Guo, Jinsong and Wong, Kam-Fai and Gottlob, Georg","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2402.05813"}}
{"bib_id":"liu2024tuning","title":"Tuning language models by proxy","author":"Liu, Alisa and Han, Xiaochuang and Wang, Yizhong and Tsvetkov, Yulia and Choi, Yejin and Smith, Noah A","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2401.08565"}}
{"bib_id":"shi2024instruction","title":"Instruction Tuning With Loss Over Instructions","author":"Shi, Zhengyan and Yang, Adam X and Wu, Bin and Aitchison, Laurence and Yilmaz, Emine and Lipani, Aldo","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2405.14394"}}
{"bib_id":"anugraha2024proxylm","title":"ProxyLM: Predicting Language Model Performance on Multilingual Tasks via Proxy Models","author":"Anugraha, David and Winata, Genta Indra and Li, Chenyue and Irawan, Patrick Amadeus and Lee, En-Shiun Annie","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2406.09334"}}
{"bib_id":"tan2024small","title":"Small Models, Big Insights: Leveraging Slim Proxy Models To Decide When and What to Retrieve for LLMs","author":"Tan, Jiejun and Dou, Zhicheng and Zhu, Yutao and Guo, Peidong and Fang, Kun and Wen, Ji-Rong","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2402.12052"}}
{"bib_id":"hoffmann2022training","title":"Training compute-optimal large language models","author":"Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others","meta_info":{"year":"2022","journal":"arXiv preprint arXiv:2203.15556"}}
{"bib_id":"kaplan2020scaling","title":"Scaling laws for neural language models","author":"Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario","meta_info":{"year":"2020","journal":"arXiv preprint arXiv:2001.08361"}}
{"bib_id":"shannon2001mathematical","title":"A mathematical theory of communication","author":"Shannon, Claude Elwood","meta_info":{"publisher":"ACM New York, NY, USA","year":"2001","pages":"3--55","number":"1","volume":"5","journal":"ACM SIGMOBILE mobile computing and communications review"}}
{"bib_id":"jelinek1977perplexity","title":"Perplexity—a measure of the difficulty of speech recognition tasks","author":"Jelinek, Fred and Mercer, Robert L and Bahl, Lalit R and Baker, James K","meta_info":{"publisher":"Acoustical Society of America","year":"1977","pages":"S63--S63","number":"S1","volume":"62","journal":"The Journal of the Acoustical Society of America"}}
{"bib_id":"jelinek1980interpolated","title":"Interpolated estimation of Markov source parameters from sparse data","author":"Jelinek, Frederick","meta_info":{"year":"1980","booktitle":"Proc. Workshop on Pattern Recognition in Practice, 1980"}}
{"bib_id":"penedo2023refinedweb","title":"The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only","author":"Penedo, Guilherme and Malartic, Quentin and Hesslow, Daniel and Cojocaru, Ruxandra and Cappelli, Alessandro and Alobeidli, Hamza and Pannier, Baptiste and Almazrouei, Ebtesam and Launay, Julien","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2306.01116"}}
{"bib_id":"radford2018improving","title":"Improving language understanding by generative pre-training","author":"Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others","meta_info":{"publisher":"San Francisco, CA, USA","year":"2018"}}
{"bib_id":"radford2019language","title":"Language models are unsupervised multitask learners","author":"Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others","meta_info":{"year":"2019","pages":"9","number":"8","volume":"1","journal":"OpenAI blog"}}
{"bib_id":"schioppa2021scalinginfluencefunctions","title":"Scaling Up Influence Functions","author":"Andrea Schioppa and Polina Zablotskaia and David Vilar and Artem Sokolov","meta_info":{"url":"https:\/\/arxiv.org\/abs\/2112.03052","primaryclass":"cs.LG","archiveprefix":"arXiv","eprint":"2112.03052","year":"2021"}}
{"bib_id":"brown2020language","title":"Language models are few-shot learners","author":"Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others","meta_info":{"year":"2020","pages":"1877--1901","volume":"33","journal":"Advances in neural information processing systems"}}
{"bib_id":"achiam2023gpt","title":"Gpt-4 technical report","author":"Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2303.08774"}}
{"bib_id":"zhong2022towards","title":"Towards a unified multi-dimensional evaluator for text generation","author":"Zhong, Ming and Liu, Yang and Yin, Da and Mao, Yuning and Jiao, Yizhu and Liu, Pengfei and Zhu, Chenguang and Ji, Heng and Han, Jiawei","meta_info":{"year":"2022","journal":"arXiv preprint arXiv:2210.07197"}}
{"bib_id":"deng2021compression","title":"Compression, transduction, and creation: A unified framework for evaluating natural language generation","author":"Deng, Mingkai and Tan, Bowen and Liu, Zhengzhong and Xing, Eric P and Hu, Zhiting","meta_info":{"year":"2021","journal":"arXiv preprint arXiv:2109.06379"}}
{"bib_id":"raffel2020exploring","title":"Exploring the limits of transfer learning with a unified text-to-text transformer","author":"Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J","meta_info":{"year":"2020","pages":"1--67","number":"140","volume":"21","journal":"Journal of machine learning research"}}
{"bib_id":"liu2019roberta","title":"Roberta: A robustly optimized bert pretraining approach","author":"Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin","meta_info":{"year":"2019","journal":"arXiv preprint arXiv:1907.11692"}}
{"bib_id":"jiang2024exploring","title":"Exploring Learning Complexity for Downstream Data Pruning","author":"Jiang, Wenyu and Liu, Zhenlong and Xie, Zejian and Zhang, Songxin and Jing, Bingyi and Wei, Hongxin","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2402.05356"}}
{"bib_id":"ding2023enhancing","title":"Enhancing chat language models by scaling high-quality instructional conversations","author":"Ding, Ning and Chen, Yulin and Xu, Bokai and Qin, Yujia and Zheng, Zhi and Hu, Shengding and Liu, Zhiyuan and Sun, Maosong and Zhou, Bowen","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2305.14233"}}
{"bib_id":"bai2023qwen","title":"Qwen technical report","author":"Bai, Jinze and Bai, Shuai and Chu, Yunfei and Cui, Zeyu and Dang, Kai and Deng, Xiaodong and Fan, Yang and Ge, Wenbin and Han, Yu and Huang, Fei and others","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2309.16609"}}
{"bib_id":"du2023mods","title":"Mods: Model-oriented data selection for instruction tuning","author":"Du, Qianlong and Zong, Chengqing and Zhang, Jiajun","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2311.15653"}}
{"bib_id":"marion2023less","title":"When less is more: Investigating data pruning for pretraining llms at scale","author":"Marion, Max and Üstün, Ahmet and Pozzobon, Luiza and Wang, Alex and Fadaee, Marzieh and Hooker, Sara","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2309.04564"}}
{"bib_id":"bukharin2023data","title":"Data diversity matters for robust instruction tuning","author":"Bukharin, Alexander and Zhao, Tuo","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2311.14736"}}
{"bib_id":"cornuejols1983uncapicitated","title":"The uncapicitated facility location problem","author":"Cornuéjols, Gérard and Nemhauser, George and Wolsey, Laurence","meta_info":{"institution":"Cornell University Operations Research and Industrial Engineering","year":"1983"}}
{"bib_id":"dong2023raft","title":"Raft: Reward ranked finetuning for generative foundation model alignment","author":"Dong, Hanze and Xiong, Wei and Goyal, Deepanshu and Zhang, Yihan and Chow, Winnie and Pan, Rui and Diao, Shizhe and Zhang, Jipeng and Shum, Kashun and Zhang, Tong","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2304.06767"}}
{"bib_id":"guo2022deepcore","title":"Deepcore: A comprehensive library for coreset selection in deep learning","author":"Guo, Chengcheng and Zhao, Bo and Bai, Yanbing","meta_info":{"organization":"Springer","year":"2022","pages":"181--195","booktitle":"International Conference on Database and Expert Systems Applications"}}
{"bib_id":"paul2021deep","title":"Deep learning on a data diet: Finding important examples early in training","author":"Paul, Mansheej and Ganguli, Surya and Dziugaite, Gintare Karolina","meta_info":{"year":"2021","pages":"20596--20607","volume":"34","journal":"Advances in neural information processing systems"}}
{"bib_id":"biderman2024emergent","title":"Emergent and predictable memorization in large language models","author":"Biderman, Stella and Prashanth, Usvsn and Sutawika, Lintang and Schoelkopf, Hailey and Anthony, Quentin and Purohit, Shivanshu and Raff, Edward","meta_info":{"year":"2024","volume":"36","journal":"Advances in Neural Information Processing Systems"}}
{"bib_id":"song2022learning","title":"Learning from noisy labels with deep neural networks: A survey","author":"Song, Hwanjun and Kim, Minseok and Park, Dongmin and Shin, Yooju and Lee, Jae-Gil","meta_info":{"publisher":"IEEE","year":"2022","pages":"8135--8153","number":"11","volume":"34","journal":"IEEE transactions on neural networks and learning systems"}}
{"bib_id":"natarajan2013learning","title":"Learning with noisy labels","author":"Natarajan, Nagarajan and Dhillon, Inderjit S and Ravikumar, Pradeep K and Tewari, Ambuj","meta_info":{"year":"2013","volume":"26","journal":"Advances in neural information processing systems"}}
{"bib_id":"qin2024capro","title":"CAPro: webly supervised learning with cross-modality aligned prototypes","author":"Qin, Yulei and Chen, Xingyu and Shen, Yunhang and Fu, Chaoyou and Gu, Yun and Li, Ke and Sun, Xing and Ji, Rongrong","meta_info":{"year":"2024","volume":"36","journal":"Advances in Neural Information Processing Systems"}}
{"bib_id":"melis2017state","title":"On the state of the art of evaluation in neural language models","author":"Melis, Gábor and Dyer, Chris and Blunsom, Phil","meta_info":{"year":"2017","journal":"arXiv preprint arXiv:1707.05589"}}
{"bib_id":"xu2022systematic","title":"A systematic evaluation of large language models of code","author":"Xu, Frank F and Alon, Uri and Neubig, Graham and Hellendoorn, Vincent Josua","meta_info":{"year":"2022","pages":"1--10","booktitle":"Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming"}}
{"bib_id":"Index","title":"Index1.9B Technical Report","author":"Bilibili","meta_info":{"year":"2024","url":"https:\/\/github.com\/bilibili\/Index-1.9B"}}
{"bib_id":"yang2024qwen2","title":"Qwen2 technical report","author":"Yang, An and Yang, Baosong and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Zhou, Chang and Li, Chengpeng and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and others","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2407.10671"}}
{"bib_id":"jiang2024investigating","title":"Investigating data contamination for pre-training language models","author":"Jiang, Minhao and Liu, Ken Ziyu and Zhong, Ming and Schaeffer, Rylan and Ouyang, Siru and Han, Jiawei and Koyejo, Sanmi","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2401.06059"}}
{"bib_id":"cao2024concerned","title":"Concerned with Data Contamination? Assessing Countermeasures in Code Language Model","author":"Cao, Jialun and Zhang, Wuqi and Cheung, Shing-Chi","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2403.16898"}}
{"bib_id":"deng2023investigating","title":"Investigating data contamination in modern benchmarks for large language models","author":"Deng, Chunyuan and Zhao, Yilun and Tang, Xiangru and Gerstein, Mark and Cohan, Arman","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2311.09783"}}
{"bib_id":"marone2024data","title":"Data portraits: Recording foundation model training data","author":"Marone, Marc and Van Durme, Benjamin","meta_info":{"year":"2024","volume":"36","journal":"Advances in Neural Information Processing Systems"}}
{"bib_id":"carlini2019secret","title":"The secret sharer: Evaluating and testing unintended memorization in neural networks","author":"Carlini, Nicholas and Liu, Chang and Erlingsson, Úlfar and Kos, Jernej and Song, Dawn","meta_info":{"year":"2019","pages":"267--284","booktitle":"28th USENIX security symposium (USENIX security 19)"}}
{"bib_id":"magar2022data","title":"Data contamination: From memorization to exploitation","author":"Magar, Inbal and Schwartz, Roy","meta_info":{"year":"2022","journal":"arXiv preprint arXiv:2203.08242"}}
{"bib_id":"li2023starcoder","title":"Starcoder: may the source be with you!","author":"Li, Raymond and Allal, Loubna Ben and Zi, Yangtian and Muennighoff, Niklas and Kocetkov, Denis and Mou, Chenghao and Marone, Marc and Akiki, Christopher and Li, Jia and Chim, Jenny and others","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2305.06161"}}
{"bib_id":"rae2021scaling","title":"Scaling language models: Methods, analysis & insights from training gopher","author":"Rae, Jack W and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and others","meta_info":{"year":"2021","journal":"arXiv preprint arXiv:2112.11446"}}
{"bib_id":"liang2022holistic","title":"Holistic evaluation of language models","author":"Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and others","meta_info":{"year":"2022","journal":"arXiv preprint arXiv:2211.09110"}}
{"bib_id":"chang2024survey","title":"A survey on evaluation of large language models","author":"Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Yang, Linyi and Zhu, Kaijie and Chen, Hao and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and others","meta_info":{"publisher":"ACM New York, NY","year":"2024","pages":"1--45","number":"3","volume":"15","journal":"ACM Transactions on Intelligent Systems and Technology"}}
{"bib_id":"cao2023instruction","title":"Instruction mining: High-quality instruction data selection for large language models","author":"Cao, Yihan and Kang, Yanbin and Sun, Lichao","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2307.06290"}}
{"bib_id":"mccarthy2010mtld","title":"MTLD, vocd-D, and HD-D: A validation study of sophisticated approaches to lexical diversity assessment","author":"McCarthy, Philip M and Jarvis, Scott","meta_info":{"publisher":"Springer","year":"2010","pages":"381--392","number":"2","volume":"42","journal":"Behavior research methods"}}
{"bib_id":"dong2011efficient","title":"Efficient k-nearest neighbor graph construction for generic similarity measures","author":"Dong, Wei and Moses, Charikar and Li, Kai","meta_info":{"year":"2011","pages":"577--586","booktitle":"Proceedings of the 20th international conference on World wide web"}}
{"bib_id":"bjork1988least","title":"Least Squares Methods: Handbook of Numerical Analysis","author":"Bjork, A","meta_info":{"year":"1988","volume":"1","journal":"Finite Difference Methods Solutions of Equations in Rn"}}
{"bib_id":"wang2021economic","title":"Economic hyperparameter optimization with blended search strategy","author":"Wang, Chi and Wu, Qingyun and Huang, Silu and Saied, Amin","meta_info":{"year":"2021","booktitle":"International Conference on Learning Representations"}}
{"bib_id":"wang2021flaml","title":"Flaml: A fast and lightweight automl library","author":"Wang, Chi and Wu, Qingyun and Weimer, Markus and Zhu, Erkang","meta_info":{"year":"2021","pages":"434--447","volume":"3","journal":"Proceedings of Machine Learning and Systems"}}
{"bib_id":"li2023quantity","title":"From quantity to quality: Boosting llm performance with self-guided data selection for instruction tuning","author":"Li, Ming and Zhang, Yong and Li, Zhitao and Chen, Jiuhai and Chen, Lichang and Cheng, Ning and Wang, Jianzong and Zhou, Tianyi and Xiao, Jing","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2308.12032"}}
{"bib_id":"zhao2024technical","title":"Technical Report: Competition Solution For BetterMixture","author":"Zhao, Shuaijiang and Fang, Xiaoquan","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2403.13233"}}
{"bib_id":"sakaguchi2021winogrande","title":"Winogrande: An adversarial winograd schema challenge at scale","author":"Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin","meta_info":{"publisher":"ACM New York, NY, USA","year":"2021","pages":"99--106","number":"9","volume":"64","journal":"Communications of the ACM"}}
{"bib_id":"bhatt2024experimental","title":"An Experimental Design Framework for Label-Efficient Supervised Finetuning of Large Language Models","author":"Bhatt, Gantavya and Chen, Yifang and Das, Arnav M and Zhang, Jifan and Truong, Sang T and Mussmann, Stephen and Zhu, Yinglun and Bilmes, Jeffrey and Du, Simon S and Jamieson, Kevin and others","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2401.06692"}}
{"bib_id":"settles2011theories","title":"From theories to queries: Active learning in practice","author":"Settles, Burr","meta_info":{"organization":"JMLR Workshop and Conference Proceedings","year":"2011","pages":"1--18","booktitle":"Active learning and experimental design workshop in conjunction with AISTATS 2010"}}
{"bib_id":"kremer2014active","title":"Active learning with support vector machines","author":"Kremer, Jan and Steenstrup Pedersen, Kim and Igel, Christian","meta_info":{"publisher":"Wiley Online Library","year":"2014","pages":"313--326","number":"4","volume":"4","journal":"Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery"}}
{"bib_id":"settles1995active","title":"Active Learning Literature Survey","author":"Settles, Burr","meta_info":{"publisher":"University of Wisconsin--Madison","year":"1995","pages":"237--304","number":"3","volume":"10","journal":"Science"}}
{"bib_id":"tong2001support","title":"Support vector machine active learning with applications to text classification","author":"Tong, Simon and Koller, Daphne","meta_info":{"year":"2001","pages":"45--66","number":"Nov","volume":"2","journal":"Journal of machine learning research"}}
{"bib_id":"balcan2006agnostic","title":"Agnostic active learning","author":"Balcan, Maria-Florina and Beygelzimer, Alina and Langford, John","meta_info":{"year":"2006","pages":"65--72","booktitle":"Proceedings of the 23rd international conference on Machine learning"}}
{"bib_id":"liu2024take","title":"Take the essence and discard the dross: A Rethinking on Data Selection for Fine-Tuning Large Language Models","author":"Liu, Ziche and Ke, Rui and Jiang, Feng and Li, Haizhou","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2406.14115"}}
{"bib_id":"zhang2024tagcos","title":"TAGCOS: Task-agnostic Gradient Clustered Coreset Selection for Instruction Tuning Data","author":"Zhang, Jipeng and Qin, Yaxuan and Pi, Renjie and Zhang, Weizhong and Pan, Rui and Zhang, Tong","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2407.15235"}}
{"bib_id":"nguyen2022measure","title":"How to measure uncertainty in uncertainty sampling for active learning","author":"Nguyen, Vu-Linh and Shaker, Mohammad Hossein and Hüllermeier, Eyke","meta_info":{"publisher":"Springer","year":"2022","pages":"89--122","number":"1","volume":"111","journal":"Machine Learning"}}
{"bib_id":"li2024superfiltering","title":"Superfiltering: Weak-to-strong data filtering for fast instruction-tuning","author":"Li, Ming and Zhang, Yong and He, Shwai and Li, Zhitao and Zhao, Hongyu and Wang, Jianzong and Cheng, Ning and Zhou, Tianyi","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2402.00530"}}
{"bib_id":"touvron2023llama1","title":"Llama: Open and efficient foundation language models","author":"Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothée and Rozière, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2302.13971"}}
{"bib_id":"jiang2023mistral","title":"Mistral 7B","author":"Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2310.06825"}}
{"bib_id":"sun2024itd","title":"ItD: Large Language Models Can Teach Themselves Induction through Deduction","author":"Sun, Wangtao and Xu, Haotian and Yu, Xuanqing and Chen, Pei and He, Shizhu and Zhao, Jun and Liu, Kang","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2403.05789"}}
{"bib_id":"edunov2019pre","title":"Pre-trained language model representations for language generation","author":"Edunov, Sergey and Baevski, Alexei and Auli, Michael","meta_info":{"year":"2019","journal":"arXiv preprint arXiv:1903.09722"}}
{"bib_id":"dong2019unified","title":"Unified language model pre-training for natural language understanding and generation","author":"Dong, Li and Yang, Nan and Wang, Wenhui and Wei, Furu and Liu, Xiaodong and Wang, Yu and Gao, Jianfeng and Zhou, Ming and Hon, Hsiao-Wuen","meta_info":{"year":"2019","volume":"32","journal":"Advances in neural information processing systems"}}
{"bib_id":"liu2024large","title":"Large language models and causal inference in collaboration: A comprehensive survey","author":"Liu, Xiaoyu and Xu, Paiheng and Wu, Junda and Yuan, Jiaxin and Yang, Yifan and Zhou, Yuhang and Liu, Fuxiao and Guan, Tianrui and Wang, Haoliang and Yu, Tong and others","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2403.09606"}}
{"bib_id":"liu2023mmc","title":"Mmc: Advancing multimodal chart understanding with large-scale instruction tuning","author":"Liu, Fuxiao and Wang, Xiaoyang and Yao, Wenlin and Chen, Jianshu and Song, Kaiqiang and Cho, Sangwoo and Yacoob, Yaser and Yu, Dong","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2311.10774"}}
{"bib_id":"jiang2024mixtral","title":"Mixtral of experts","author":"Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and others","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2401.04088"}}
{"bib_id":"touvron2023llama","title":"Llama 2: Open foundation and fine-tuned chat models","author":"Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2307.09288"}}
{"bib_id":"llama3modelcard","title":"Llama 3 Model Card","author":"AI@Meta","meta_info":{"url":"https:\/\/github.com\/meta-llama\/llama3\/blob\/main\/MODEL_CARD.md","year":"2024"}}
{"bib_id":"tingiris2021exploring","title":"Exploring GPT-3: An unofficial first look at the general-purpose language processing API from OpenAI","author":"Tingiris, Steve and Kinsella, Bret","meta_info":{"publisher":"Packt Publishing Ltd","year":"2021"}}
{"bib_id":"lappalainen2023aisha","title":"Aisha: A custom AI library chatbot using the ChatGPT API","author":"Lappalainen, Yrjo and Narayanan, Nikesh","meta_info":{"publisher":"Taylor & Francis","year":"2023","pages":"37--58","number":"3","volume":"17","journal":"Journal of Web Librarianship"}}
{"bib_id":"sun2023does","title":"Does fine-tuning GPT-3 with the OpenAI API leak personally-identifiable information?","author":"Sun, Albert Yu and Zemour, Eliott and Saxena, Arushi and Vaidyanathan, Udith and Lin, Eric and Lau, Christian and Mugunthan, Vaikkunth","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2307.16382"}}
{"bib_id":"kublik2023gpt","title":"GPT-3: The Ultimate Guide to Building NLP Products with OpenAI API","author":"Kublik, Sandra and Saboo, Shubham","meta_info":{"publisher":"Packt Publishing Ltd","year":"2023"}}
{"bib_id":"zheng2024judging","title":"Judging llm-as-a-judge with mt-bench and chatbot arena","author":"Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others","meta_info":{"year":"2024","volume":"36","journal":"Advances in Neural Information Processing Systems"}}
{"bib_id":"wang2023pandalm","title":"Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization","author":"Wang, Yidong and Yu, Zhuohao and Zeng, Zhengran and Yang, Linyi and Wang, Cunxiang and Chen, Hao and Jiang, Chaoya and Xie, Rui and Wang, Jindong and Xie, Xing and others","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2306.05087"}}
{"bib_id":"zhu2023judgelm","title":"Judgelm: Fine-tuned large language models are scalable judges","author":"Zhu, Lianghui and Wang, Xinggang and Wang, Xinlong","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2310.17631"}}
{"bib_id":"huang2024empirical","title":"An empirical study of llm-as-a-judge for llm evaluation: Fine-tuned judge models are task-specific classifiers","author":"Huang, Hui and Qu, Yingqi and Liu, Jing and Yang, Muyun and Zhao, Tiejun","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2403.02839"}}
{"bib_id":"zeng2023evaluating","title":"Evaluating large language models at evaluating instruction following","author":"Zeng, Zhiyuan and Yu, Jiatong and Gao, Tianyu and Meng, Yu and Goyal, Tanya and Chen, Danqi","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2310.07641"}}
{"bib_id":"chan2023chateval","title":"Chateval: Towards better llm-based evaluators through multi-agent debate","author":"Chan, Chi-Min and Chen, Weize and Su, Yusheng and Yu, Jianxuan and Xue, Wei and Zhang, Shanghang and Fu, Jie and Liu, Zhiyuan","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2308.07201"}}
{"bib_id":"chen2023alpagasus","title":"Alpagasus: Training a better alpaca with fewer data","author":"Chen, Lichang and Li, Shiyang and Yan, Jun and Wang, Hai and Gunaratna, Kalpa and Yadav, Vikas and Tang, Zheng and Srinivasan, Vijay and Zhou, Tianyi and Huang, Heng and others","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2307.08701"}}
{"bib_id":"chen2024automated","title":"Automated data curation for robust language model fine-tuning","author":"Chen, Jiuhai and Mueller, Jonas","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2403.12776"}}
{"bib_id":"chen2023quantifying","title":"Quantifying uncertainty in answers from any language model and enhancing their trustworthiness","author":"Chen, Jiuhai and Mueller, Jonas","meta_info":{"year":"2023"}}
{"bib_id":"xu2023rethinking","title":"Rethinking the instruction quality: LIFT is what you need","author":"Xu, Yang and Yao, Yongqiang and Huang, Yufan and Qi, Mengnan and Wang, Maoquan and Gu, Bin and Sundaresan, Neel","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2312.11508"}}
{"bib_id":"liu2023makes","title":"What makes good data for alignment? a comprehensive study of automatic data selection in instruction tuning","author":"Liu, Wei and Zeng, Weihao and He, Keqing and Jiang, Yong and He, Junxian","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2312.15685"}}
{"bib_id":"xu2023wizardlm","title":"Wizardlm: Empowering large language models to follow complex instructions","author":"Xu, Can and Sun, Qingfeng and Zheng, Kai and Geng, Xiubo and Zhao, Pu and Feng, Jiazhan and Tao, Chongyang and Jiang, Daxin","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2304.12244"}}
{"bib_id":"taori2023alpaca","title":"Alpaca: A strong, replicable instruction-following model","author":"Taori, Rohan and Gulrajani, Ishaan and Zhang, Tianyi and Dubois, Yann and Li, Xuechen and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B","meta_info":{"year":"2023","pages":"7","number":"6","volume":"3","journal":"Stanford Center for Research on Foundation Models. https:\/\/crfm. stanford. edu\/2023\/03\/13\/alpaca. html"}}
{"bib_id":"zhang2024autonomous","title":"Autonomous data selection with language models for mathematical texts","author":"Zhang, Yifan and Luo, Yifan and Yuan, Yang and Yao, Andrew C","meta_info":{"year":"2024","booktitle":"ICLR 2024 Workshop on Navigating and Addressing Data Problems for Foundation Models"}}
{"bib_id":"wettig2024qurating","title":"Qurating: Selecting high-quality data for training language models","author":"Wettig, Alexander and Gupta, Aatmik and Malik, Saumya and Chen, Danqi","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2402.09739"}}
{"bib_id":"paster2023openwebmath","title":"Openwebmath: An open dataset of high-quality mathematical web text","author":"Paster, Keiran and Santos, Marco Dos and Azerbayev, Zhangir and Ba, Jimmy","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2310.06786"}}
{"bib_id":"gunasekar2023textbooks","title":"Textbooks are all you need","author":"Gunasekar, Suriya and Zhang, Yi and Aneja, Jyoti and Mendes, Caio César Teodoro and Del Giorno, Allie and Gopi, Sivakanth and Javaheripi, Mojan and Kauffmann, Piero and de Rosa, Gustavo and Saarikivi, Olli and others","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2306.11644"}}
{"bib_id":"ouyang2022training","title":"Training language models to follow instructions with human feedback","author":"Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others","meta_info":{"year":"2022","pages":"27730--27744","volume":"35","journal":"Advances in neural information processing systems"}}
{"bib_id":"sanh2021multitask","title":"Multitask prompted training enables zero-shot task generalization","author":"Sanh, Victor and Webson, Albert and Raffel, Colin and Bach, Stephen H and Sutawika, Lintang and Alyafeai, Zaid and Chaffin, Antoine and Stiegler, Arnaud and Scao, Teven Le and Raja, Arun and others","meta_info":{"year":"2021","journal":"arXiv preprint arXiv:2110.08207"}}
{"bib_id":"wei2021finetuned","title":"Finetuned language models are zero-shot learners","author":"Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V","meta_info":{"year":"2021","journal":"arXiv preprint arXiv:2109.01652"}}
{"bib_id":"rafailov2024direct","title":"Direct preference optimization: Your language model is secretly a reward model","author":"Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea","meta_info":{"year":"2024","volume":"36","journal":"Advances in Neural Information Processing Systems"}}
{"bib_id":"xia2023sheared","title":"Sheared llama: Accelerating language model pre-training via structured pruning","author":"Xia, Mengzhou and Gao, Tianyu and Zeng, Zhiyuan and Chen, Danqi","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2310.06694"}}
{"bib_id":"dubois2024alpacafarm","title":"Alpacafarm: A simulation framework for methods that learn from human feedback","author":"Dubois, Yann and Li, Chen Xuechen and Taori, Rohan and Zhang, Tianyi and Gulrajani, Ishaan and Ba, Jimmy and Guestrin, Carlos and Liang, Percy S and Hashimoto, Tatsunori B","meta_info":{"year":"2024","volume":"36","journal":"Advances in Neural Information Processing Systems"}}
{"bib_id":"kopf2024openassistant","title":"Openassistant conversations-democratizing large language model alignment","author":"Köpf, Andreas and Kilcher, Yannic and von Rütte, Dimitri and Anagnostidis, Sotiris and Tam, Zhi Rui and Stevens, Keith and Barhoum, Abdullah and Nguyen, Duc and Stanley, Oliver and Nagyfi, Richárd and others","meta_info":{"year":"2024","volume":"36","journal":"Advances in Neural Information Processing Systems"}}
{"bib_id":"wang2023aligning","title":"Aligning large language models with human: A survey","author":"Wang, Yufei and Zhong, Wanjun and Li, Liangyou and Mi, Fei and Zeng, Xingshan and Huang, Wenyong and Shang, Lifeng and Jiang, Xin and Liu, Qun","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2307.12966"}}
{"bib_id":"peng2023instruction","title":"Instruction tuning with gpt-4","author":"Peng, Baolin and Li, Chunyuan and He, Pengcheng and Galley, Michel and Gao, Jianfeng","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2304.03277"}}
{"bib_id":"siddhant2018deep","title":"Deep bayesian active learning for natural language processing: Results of a large-scale empirical study","author":"Siddhant, Aditya and Lipton, Zachary C","meta_info":{"year":"2018","journal":"arXiv preprint arXiv:1808.05697"}}
{"bib_id":"zhou2024lima","title":"Lima: Less is more for alignment","author":"Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srinivasan and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and Yu, Lili and others","meta_info":{"year":"2024","volume":"36","journal":"Advances in Neural Information Processing Systems"}}
{"bib_id":"kung2023active","title":"Active instruction tuning: Improving cross-task generalization by training on prompt sensitive tasks","author":"Kung, Po-Nien and Yin, Fan and Wu, Di and Chang, Kai-Wei and Peng, Nanyun","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2311.00288"}}
{"bib_id":"conover2023free","title":"Free dolly: Introducing the world’s first truly open instruction-tuned llm","author":"Conover, Mike and Hayes, Matt and Mathur, Ankit and Xie, Jianwei and Wan, Jun and Shah, Sam and Ghodsi, Ali and Wendell, Patrick and Zaharia, Matei and Xin, Reynold","meta_info":{"year":"2023","journal":"Company Blog of Databricks"}}
{"bib_id":"He_Garg_Mueller","title":"How to detect bad data in your instruction tuning dataset (for better LLM fine-tuning)","author":"He, Jimming and Garg, Sanjana and Mueller, Jonas","meta_info":{"year":"2024","journal":"Cleanlab","url":"https:\/\/cleanlab.ai\/blog\/filter-llm-tuning-data\/"}}
{"bib_id":"wu2023self","title":"Self-evolved diverse data sampling for efficient instruction tuning","author":"Wu, Shengguang and Lu, Keming and Xu, Benfeng and Lin, Junyang and Su, Qi and Zhou, Chang","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2311.08182"}}
{"bib_id":"li2023self","title":"Self-alignment with instruction backtranslation","author":"Li, Xian and Yu, Ping and Zhou, Chunting and Schick, Timo and Zettlemoyer, Luke and Levy, Omer and Weston, Jason and Lewis, Mike","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2308.06259"}}
{"bib_id":"lu2023instag","title":"# instag: Instruction tagging for analyzing supervised fine-tuning of large language models","author":"Lu, Keming and Yuan, Hongyi and Yuan, Zheng and Lin, Runji and Lin, Junyang and Tan, Chuanqi and Zhou, Chang and Zhou, Jingren","meta_info":{"year":"2023","booktitle":"The Twelfth International Conference on Learning Representations"}}
{"bib_id":"jarvis2013capturing","title":"Capturing the diversity in lexical diversity","author":"Jarvis, Scott","meta_info":{"publisher":"Wiley Online Library","year":"2013","pages":"87--106","volume":"63","journal":"Language learning"}}
{"bib_id":"mccarthy2005assessment","title":"An assessment of the range and usefulness of lexical diversity measures and the potential of the measure of textual, lexical diversity (MTLD)","author":"McCarthy, Philip M","meta_info":{"school":"The University of Memphis","year":"2005"}}
{"bib_id":"jarvis2013defining","title":"Defining and measuring lexical diversity","author":"Jarvis, Scott and Daller, M","meta_info":{"year":"2013","journal":"Vocabulary knowledge: Human ratings and automated measures. Amsterdam, The Netherlands"}}
{"bib_id":"zhaomeasuring","title":"Measuring Diversity in Datasets","author":"Zhao, Dorothy and Andrews, Jerone TA and Sony, AI and Papakyriakopoulos, Tokyo Orestis and Xiang, Alice","meta_info":{"year":"2024","journal":"International Conference on Learning Representations"}}
{"bib_id":"zhao2024position","title":"Position: Measure Dataset Diversity, Don't Just Claim It","author":"Zhao, Dora and Andrews, Jerone TA and Papakyriakopoulos, Orestis and Xiang, Alice","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2407.08188"}}
{"bib_id":"templin1957certain","title":"Certain language skills in children; their development and interrelationships.","author":"Templin, Mildred C","meta_info":{"publisher":"University of Minnesota Press","year":"1957"}}
{"bib_id":"richards1987type","title":"Type\/token ratios: What do they really tell us?","author":"Richards, Brian","meta_info":{"publisher":"Cambridge University Press","year":"1987","pages":"201--209","number":"2","volume":"14","journal":"Journal of child language"}}
{"bib_id":"kettunen2014can","title":"Can type-token ratio be used to show morphological complexity of languages?","author":"Kettunen, Kimmo","meta_info":{"publisher":"Taylor & Francis","year":"2014","pages":"223--245","number":"3","volume":"21","journal":"Journal of Quantitative Linguistics"}}
{"bib_id":"covington2008moving","title":"The moving-average type-token ratio","author":"Covington, M and McFall, Joe D","meta_info":{"publisher":"Citeseer","year":"2008","journal":"Linguistics Society of America, Chicago, IL"}}
{"bib_id":"covington2010cutting","title":"Cutting the Gordian knot: The moving-average type--token ratio (MATTR)","author":"Covington, Michael A and McFall, Joe D","meta_info":{"publisher":"Taylor & Francis","year":"2010","pages":"94--100","number":"2","volume":"17","journal":"Journal of quantitative linguistics"}}
{"bib_id":"matlach2021method","title":"A method for comparison of general sequences via type-token ratio","author":"Matlach, Vladimir and Krivochen, Diego and Milička, Jiri","meta_info":{"year":"2021","pages":"37--54","journal":"Language and Text: Data, models, information and applications. Amsterdam: John Benjamins"}}
{"bib_id":"silverman2002measuring","title":"Measuring lexical diversity in children who stutter: Application of vocd","author":"Silverman, Stacy and Ratner, Nan Bernstein","meta_info":{"publisher":"Elsevier","year":"2002","pages":"289--304","number":"4","volume":"27","journal":"Journal of fluency disorders"}}
{"bib_id":"deboer2014evaluating","title":"Evaluating the comparability of two measures of lexical diversity","author":"deBoer, Fredrik","meta_info":{"publisher":"Elsevier","year":"2014","pages":"139--145","volume":"47","journal":"System"}}
{"bib_id":"malvern1997new","title":"A new measure of lexical diversity","author":"Malvern, David D and Richards, Brian J","meta_info":{"publisher":"MULTILINGUAL MATTERS LTD.","year":"1997","pages":"58--71","volume":"12","journal":"British Studies in Applied Linguistics"}}
{"bib_id":"malvern2004lexical","title":"Lexical diversity and language development","author":"Malvern, David and Richards, Brian and Chipere, Ngoni and Durán, Pilar","meta_info":{"publisher":"Springer","year":"2004"}}
{"bib_id":"bestgen2023measuring","title":"Measuring lexical diversity in texts: The twofold length problem","author":"Bestgen, Yves","meta_info":{"publisher":"Wiley Online Library","year":"2023","journal":"Language Learning"}}
{"bib_id":"vidal2020effects","title":"Effects of English-medium instruction on Spanish students’ proficiency and lexical diversity in English","author":"Vidal, Karina and Jarvis, Scott","meta_info":{"publisher":"Sage Publications Sage UK: London, England","year":"2020","pages":"568--587","number":"5","volume":"24","journal":"Language Teaching Research"}}
{"bib_id":"kyle2021assessing","title":"Assessing the validity of lexical diversity indices using direct judgements","author":"Kyle, Kristopher and Crossley, Scott A and Jarvis, Scott","meta_info":{"publisher":"Taylor & Francis","year":"2021","pages":"154--170","number":"2","volume":"18","journal":"Language Assessment Quarterly"}}
{"bib_id":"huang2008similarity","title":"Similarity measures for text document clustering","author":"Huang, Anna and others","meta_info":{"year":"2008","pages":"9--56","volume":"4","booktitle":"Proceedings of the sixth new zealand computer science research student conference (NZCSRSC2008), Christchurch, New Zealand"}}
{"bib_id":"reimers2019sentence","title":"Sentence-bert: Sentence embeddings using siamese bert-networks","author":"Reimers, Nils and Gurevych, Iryna","meta_info":{"year":"2019","journal":"arXiv preprint arXiv:1908.10084"}}
{"bib_id":"feng2020language","title":"Language-agnostic BERT sentence embedding","author":"Feng, Fangxiaoyu and Yang, Yinfei and Cer, Daniel and Arivazhagan, Naveen and Wang, Wei","meta_info":{"year":"2020","journal":"arXiv preprint arXiv:2007.01852"}}
{"bib_id":"devlin2018bert","title":"Bert: Pre-training of deep bidirectional transformers for language understanding","author":"Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina","meta_info":{"year":"2018","journal":"arXiv preprint arXiv:1810.04805"}}
{"bib_id":"felice2012linguistic","title":"Linguistic features for quality estimation","author":"Felice, Mariano and Specia, Lucia","meta_info":{"year":"2012","pages":"96--103","booktitle":"Proceedings of the Seventh Workshop on Statistical Machine Translation"}}
{"bib_id":"spyromitros2015improving","title":"Improving diversity in image search via supervised relevance scoring","author":"Spyromitros-Xioufis, Eleftherios and Papadopoulos, Symeon and Ginsca, Alexandru Lucian and Popescu, Adrian and Kompatsiaris, Yiannis and Vlahavas, Ioannis","meta_info":{"year":"2015","pages":"323--330","booktitle":"Proceedings of the 5th ACM on International Conference on Multimedia Retrieval"}}
{"bib_id":"sun2024diversity","title":"On the diversity and realism of distilled dataset: An efficient dataset distillation paradigm","author":"Sun, Peng and Shi, Bei and Yu, Daiwei and Lin, Tao","meta_info":{"year":"2024","pages":"9390--9399","booktitle":"Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern Recognition"}}
{"bib_id":"ionescu2018datasets","title":"Datasets column: diversity and credibility for social images and image retrieval","author":"Ionescu, Bogdan and Lupu, Mihai and Rohm, Maia and Gı̂nsca, Alexandru Lucian and Müller, Henning","meta_info":{"publisher":"ACM New York, NY, USA","year":"2018","pages":"7--7","number":"3","volume":"9","journal":"ACM SIGMultimedia Records"}}
{"bib_id":"mithun2019construction","title":"Construction of diverse image datasets from web collections with limited labeling","author":"Mithun, Niluthpol Chowdhury and Panda, Rameswar and Roy-Chowdhury, Amit K","meta_info":{"publisher":"IEEE","year":"2019","pages":"1147--1161","number":"4","volume":"30","journal":"IEEE Transactions on Circuits and Systems for Video Technology"}}
{"bib_id":"stasaski2020more","title":"More diverse dialogue datasets via diversity-informed data collection","author":"Stasaski, Katherine and Yang, Grace Hui and Hearst, Marti A","meta_info":{"year":"2020","pages":"4958--4968","booktitle":"Proceedings of the 58th annual meeting of the association for computational linguistics"}}
{"bib_id":"stasaski2022semantic","title":"Semantic diversity in dialogue with natural language inference","author":"Stasaski, Katherine and Hearst, Marti A","meta_info":{"year":"2022","journal":"arXiv preprint arXiv:2205.01497"}}
{"bib_id":"li2015diversity","title":"A diversity-promoting objective function for neural conversation models","author":"Li, Jiwei and Galley, Michel and Brockett, Chris and Gao, Jianfeng and Dolan, Bill","meta_info":{"year":"2015","journal":"arXiv preprint arXiv:1510.03055"}}
{"bib_id":"cao2017latent","title":"Latent variable dialogue models and their diversity","author":"Cao, Kris and Clark, Stephen","meta_info":{"year":"2017","journal":"arXiv preprint arXiv:1702.05962"}}
{"bib_id":"tevet2020evaluating","title":"Evaluating the evaluation of diversity in natural language generation","author":"Tevet, Guy and Berant, Jonathan","meta_info":{"year":"2020","journal":"arXiv preprint arXiv:2004.02990"}}
{"bib_id":"zhu2018texygen","title":"Texygen: A benchmarking platform for text generation models","author":"Zhu, Yaoming and Lu, Sidi and Zheng, Lei and Guo, Jiaxian and Zhang, Weinan and Wang, Jun and Yu, Yong","meta_info":{"year":"2018","pages":"1097--1100","booktitle":"The 41st international ACM SIGIR conference on research & development in information retrieval"}}
{"bib_id":"zhang2019bertscore","title":"Bertscore: Evaluating text generation with bert","author":"Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q and Artzi, Yoav","meta_info":{"year":"2019","journal":"arXiv preprint arXiv:1904.09675"}}
{"bib_id":"larson2019outlier","title":"Outlier detection for improved data quality and diversity in dialog systems","author":"Larson, Stefan and Mahendran, Anish and Lee, Andrew and Kummerfeld, Jonathan K and Hill, Parker and Laurenzano, Michael A and Hauswald, Johann and Tang, Lingjia and Mars, Jason","meta_info":{"year":"2019","journal":"arXiv preprint arXiv:1904.03122"}}
{"bib_id":"shu2019generating","title":"Generating diverse translations with sentence codes","author":"Shu, Raphael and Nakayama, Hideki and Cho, Kyunghyun","meta_info":{"year":"2019","pages":"1823--1827","booktitle":"Proceedings of the 57th annual meeting of the association for computational linguistics"}}
{"bib_id":"du2019boosting","title":"Boosting dialog response generation","author":"Du, Wenchao and Black, Alan W","meta_info":{"year":"2019","booktitle":"Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"}}
{"bib_id":"lai2020diversity","title":"Diversity, density, and homogeneity: Quantitative characteristic metrics for text collections","author":"Lai, Yi-An and Zhu, Xuan and Zhang, Yi and Diab, Mona","meta_info":{"year":"2020","journal":"arXiv preprint arXiv:2003.08529"}}
{"bib_id":"shannon1948mathematical","title":"A mathematical theory of communication","author":"Shannon, Claude Elwood","meta_info":{"publisher":"Nokia Bell Labs","year":"1948","pages":"379--423","number":"3","volume":"27","journal":"The Bell system technical journal"}}
{"bib_id":"renyi1961measures","title":"On measures of entropy and information","author":"Rényi, Alfréd","meta_info":{"organization":"University of California Press","year":"1961","pages":"547--562","volume":"4","booktitle":"Proceedings of the fourth Berkeley symposium on mathematical statistics and probability, volume 1: contributions to the theory of statistics"}}
{"bib_id":"simpson1949measurement","title":"Measurement of diversity","author":"Simpson, Edward H","meta_info":{"publisher":"Nature Publishing Group UK London","year":"1949","pages":"688--688","number":"4148","volume":"163","journal":"nature"}}
{"bib_id":"dan2023vendi","title":"The vendi score: A diversity evaluation metric for machine learning","author":"Dan Friedman, Dan and Dieng, Adji Bousso","meta_info":{"year":"2023","journal":"Transactions on machine learning research"}}
{"bib_id":"chen2012super","title":"Super-samples from kernel herding","author":"Chen, Yutian and Welling, Max and Smola, Alex","meta_info":{"year":"2012","journal":"arXiv preprint arXiv:1203.3472"}}
{"bib_id":"agarwal2020contextual","title":"Contextual diversity for active learning","author":"Agarwal, Sharat and Arora, Himanshu and Anand, Saket and Arora, Chetan","meta_info":{"organization":"Springer","year":"2020","pages":"137--153","booktitle":"Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part XVI 16"}}
{"bib_id":"sener2017active","title":"Active learning for convolutional neural networks: A core-set approach","author":"Sener, Ozan and Savarese, Silvio","meta_info":{"year":"2017","journal":"arXiv preprint arXiv:1708.00489"}}
{"bib_id":"sinha2020small","title":"Small-gan: Speeding up gan training using core-sets","author":"Sinha, Samarth and Zhang, Han and Goyal, Anirudh and Bengio, Yoshua and Larochelle, Hugo and Odena, Augustus","meta_info":{"organization":"PMLR","year":"2020","pages":"9005--9015","booktitle":"International Conference on Machine Learning"}}
{"bib_id":"farahani2009facility","title":"Facility location: concepts, models, algorithms and case studies","author":"Farahani, Reza Zanjirani and Hekmatfar, Masoud","meta_info":{"publisher":"Springer Science & Business Media","year":"2009"}}
{"bib_id":"cook1994combinatorial","title":"Combinatorial optimization","author":"Cook, William J and Cunningham, William H and Pulleyblank, William R and Schrijver, Alexander","meta_info":{"publisher":"Springer","year":"1994","pages":"75--93","volume":"10","journal":"Unpublished manuscript"}}
{"bib_id":"ikotun2023k","title":"K-means clustering algorithms: A comprehensive review, variants analysis, and advances in the era of big data","author":"Ikotun, Abiodun M and Ezugwu, Absalom E and Abualigah, Laith and Abuhaija, Belal and Heming, Jia","meta_info":{"publisher":"Elsevier","year":"2023","pages":"178--210","volume":"622","journal":"Information Sciences"}}
{"bib_id":"xia2024refined","title":"Refined Coreset Selection: Towards Minimal Coreset Size under Model Performance Constraints","author":"Xia, Xiaobo and Liu, Jiale and Zhang, Shaokun and Wu, Qingyun and Wei, Hongxin and Liu, Tongliang","meta_info":{"year":"2024","booktitle":"Forty-first International Conference on Machine Learning"}}
{"bib_id":"borsos2020coresets","title":"Coresets via bilevel optimization for continual learning and streaming","author":"Borsos, Zalán and Mutny, Mojmir and Krause, Andreas","meta_info":{"year":"2020","pages":"14879--14890","volume":"33","journal":"Advances in neural information processing systems"}}
{"bib_id":"killamsetty2021glister","title":"Glister: Generalization based data subset selection for efficient and robust learning","author":"Killamsetty, Krishnateja and Sivasubramanian, Durga and Ramakrishnan, Ganesh and Iyer, Rishabh","meta_info":{"year":"2021","pages":"8110--8118","number":"9","volume":"35","booktitle":"Proceedings of the AAAI Conference on Artificial Intelligence"}}
{"bib_id":"killamsetty2021retrieve","title":"Retrieve: Coreset selection for efficient and robust semi-supervised learning","author":"Killamsetty, Krishnateja and Zhao, Xujiang and Chen, Feng and Iyer, Rishabh","meta_info":{"year":"2021","pages":"14488--14501","volume":"34","journal":"Advances in neural information processing systems"}}
{"bib_id":"welling2009herding","title":"Herding dynamical weights to learn","author":"Welling, Max","meta_info":{"year":"2009","pages":"1121--1128","booktitle":"Proceedings of the 26th annual international conference on machine learning"}}
{"bib_id":"huszar2012optimally","title":"Optimally-weighted herding is Bayesian quadrature","author":"Huszár, Ferenc and Duvenaud, David","meta_info":{"year":"2012","journal":"arXiv preprint arXiv:1204.1664"}}
{"bib_id":"adhikary2022sampling","title":"Sampling Over Riemannian Manifolds Using Kernel Herding","author":"Adhikary, Sandesh and Boots, Byron","meta_info":{"organization":"IEEE","year":"2022","pages":"3646--3653","booktitle":"2022 International Conference on Robotics and Automation (ICRA)"}}
{"bib_id":"chen2016herded","title":"Herded Gibbs sampling","author":"Chen, Yutian and Bornn, Luke and De Freitas, Nando and Eskelin, Mareija and Fang, Jing and Welling, Max","meta_info":{"publisher":"JMLR. org","year":"2016","pages":"263--291","number":"1","volume":"17","journal":"The Journal of Machine Learning Research"}}
{"bib_id":"harvey2014near","title":"Near-optimal herding","author":"Harvey, Nick and Samadi, Samira","meta_info":{"organization":"PMLR","year":"2014","pages":"1165--1182","booktitle":"Conference on Learning Theory"}}
{"bib_id":"eldar1997farthest","title":"The farthest point strategy for progressive image sampling","author":"Eldar, Yuval and Lindenbaum, Michael and Porat, Moshe and Zeevi, Yehoshua Y","meta_info":{"publisher":"IEEE","year":"1997","pages":"1305--1315","number":"9","volume":"6","journal":"IEEE transactions on image processing"}}
{"bib_id":"Arnoldi1951ThePO","title":"The principle of minimized iterations in the solution of the matrix eigenvalue problem","author":"Walter E. Arnoldi","meta_info":{"url":"https:\/\/api.semanticscholar.org\/CorpusID:115852469","pages":"17-29","volume":"9","year":"1951","journal":"Quarterly of Applied Mathematics"}}
{"bib_id":"jiang2023diverse","title":"Dos: Diverse outlier sampling for out-of-distribution detection","author":"Jiang, Wenyu and Cheng, Hao and Chen, Mingcai and Wang, Chongjun and Wei, Hongxin","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2306.02031"}}
{"bib_id":"alcoforado2024random","title":"From Random to Informed Data Selection: A Diversity-Based Approach to Optimize Human Annotation and Few-Shot Learning","author":"Alcoforado, Alexandre and Ferraz, Thomas Palmeira and Okamura, Lucas Hideki and Fama, Israel Campos and Lavado, Arnold Moya and Bueno, Bárbara Dias and Veloso, Bruno and Costa, Anna Helena Reali","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2401.13229"}}
{"bib_id":"papineni2002bleu","title":"Bleu: a method for automatic evaluation of machine translation","author":"Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing","meta_info":{"year":"2002","pages":"311--318","booktitle":"Proceedings of the 40th annual meeting of the Association for Computational Linguistics"}}
{"bib_id":"lin2004rouge","title":"Rouge: A package for automatic evaluation of summaries","author":"Lin, Chin-Yew","meta_info":{"year":"2004","pages":"74--81","booktitle":"Text summarization branches out"}}
{"bib_id":"campello2013density","title":"Density-based clustering based on hierarchical density estimates","author":"Campello, Ricardo JGB and Moulavi, Davoud and Sander, Jörg","meta_info":{"organization":"Springer","year":"2013","pages":"160--172","booktitle":"Pacific-Asia conference on knowledge discovery and data mining"}}
{"bib_id":"tirumala2024d4","title":"D4: Improving llm pretraining via document de-duplication and diversification","author":"Tirumala, Kushal and Simig, Daniel and Aghajanyan, Armen and Morcos, Ari","meta_info":{"year":"2024","volume":"36","journal":"Advances in Neural Information Processing Systems"}}
{"bib_id":"axiotis2024data","title":"Data-Efficient Learning via Clustering-Based Sensitivity Sampling: Foundation Models and Beyond","author":"Axiotis, Kyriakos and Cohen-Addad, Vincent and Henzinger, Monika and Jerome, Sammy and Mirrokni, Vahab and Saulpic, David and Woodruff, David and Wunder, Michael","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2402.17327"}}
{"bib_id":"shao2024balanced","title":"Balanced Data Sampling for Language Model Training with Clustering","author":"Shao, Yunfan and Li, Linyang and Fei, Zhaoye and Yan, Hang and Lin, Dahua and Qiu, Xipeng","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2402.14526"}}
{"bib_id":"blei2003latent","title":"Latent dirichlet allocation","author":"Blei, David M and Ng, Andrew Y and Jordan, Michael I","meta_info":{"year":"2003","pages":"993--1022","number":"Jan","volume":"3","journal":"Journal of machine Learning research"}}
{"bib_id":"raghuveer2012legal","title":"Legal documents clustering using latent dirichlet allocation","author":"Raghuveer, K and others","meta_info":{"publisher":"Citeseer","year":"2012","pages":"34--37","number":"1","volume":"2","journal":"IAES Int. J. Artif. Intell"}}
{"bib_id":"bui2017combining","title":"Combining Latent Dirichlet Allocation and K-means for documents clustering: effect of probabilistic based distance measures","author":"Bui, Quang Vu and Sayadi, Karim and Amor, Soufian Ben and Bui, Marc","meta_info":{"organization":"Springer","year":"2017","pages":"248--257","booktitle":"Intelligent Information and Database Systems: 9th Asian Conference, ACIIDS 2017, Kanazawa, Japan, April 3-5, 2017, Proceedings, Part I 9"}}
{"bib_id":"lee2000algorithms","title":"Algorithms for non-negative matrix factorization","author":"Lee, Daniel and Seung, H Sebastian","meta_info":{"year":"2000","volume":"13","journal":"Advances in neural information processing systems"}}
{"bib_id":"wang2012nonnegative","title":"Nonnegative matrix factorization: A comprehensive review","author":"Wang, Yu-Xiong and Zhang, Yu-Jin","meta_info":{"publisher":"IEEE","year":"2012","pages":"1336--1353","number":"6","volume":"25","journal":"IEEE Transactions on knowledge and data engineering"}}
{"bib_id":"shen2010non","title":"Non-negative matrix factorization clustering on multiple manifolds","author":"Shen, Bin and Si, Luo","meta_info":{"year":"2010","pages":"575--580","number":"1","volume":"24","booktitle":"Proceedings of the AAAI Conference on Artificial Intelligence"}}
{"bib_id":"lazar2009non","title":"Non negative matrix factorization clustering capabilities; application on multivariate image segmentation","author":"Lazar, Cosmin and Doncescu, Andrei","meta_info":{"organization":"IEEE","year":"2009","pages":"924--929","booktitle":"2009 International Conference on Complex, Intelligent and Software Intensive Systems"}}
{"bib_id":"bafna2016document","title":"Document clustering: TF-IDF approach","author":"Bafna, Prafulla and Pramod, Dhanya and Vaidya, Anagha","meta_info":{"organization":"IEEE","year":"2016","pages":"61--66","booktitle":"2016 International Conference on Electrical, Electronics, and Optimization Techniques (ICEEOT)"}}
{"bib_id":"patil2013novel","title":"A novel approach for feature selection method TF-IDF in document clustering","author":"Patil, Leena H and Atique, Mohammed","meta_info":{"organization":"IEEE","year":"2013","pages":"858--862","booktitle":"2013 3rd IEEE international advance computing conference (IACC)"}}
{"bib_id":"roul2014web","title":"Web document clustering and ranking using tf-idf based apriori approach","author":"Roul, Rajendra Kumar and Devanand, Omanwar Rohit and Sahay, Sanjay Kumar","meta_info":{"year":"2014","journal":"arXiv preprint arXiv:1406.5617"}}
{"bib_id":"sinaga2020unsupervised","title":"Unsupervised K-means clustering algorithm","author":"Sinaga, Kristina P and Yang, Miin-Shen","meta_info":{"publisher":"IEEE","year":"2020","pages":"80716--80727","volume":"8","journal":"IEEE access"}}
{"bib_id":"kanungo2000analysis","title":"The analysis of a simple k-means clustering algorithm","author":"Kanungo, Tapas and Mount, David M and Netanyahu, Nathan S and Piatko, Christine and Silverman, Ruth and Wu, Angela Y","meta_info":{"year":"2000","pages":"100--109","booktitle":"Proceedings of the sixteenth annual symposium on Computational geometry"}}
{"bib_id":"bandyapadhyay2015variants","title":"On variants of k-means clustering","author":"Bandyapadhyay, Sayan and Varadarajan, Kasturi","meta_info":{"year":"2015","journal":"arXiv preprint arXiv:1512.02985"}}
{"bib_id":"deng2020dbscan","title":"DBSCAN clustering algorithm based on density","author":"Deng, Dingsheng","meta_info":{"organization":"IEEE","year":"2020","pages":"949--953","booktitle":"2020 7th international forum on electrical engineering and automation (IFEEA)"}}
{"bib_id":"khan2014dbscan","title":"DBSCAN: Past, present and future","author":"Khan, Kamran and Rehman, Saif Ur and Aziz, Kamran and Fong, Simon and Sarasvady, Sababady","meta_info":{"organization":"IEEE","year":"2014","pages":"232--238","booktitle":"The fifth international conference on the applications of digital information and web technologies (ICADIWT 2014)"}}
{"bib_id":"crectulescu2019dbscan","title":"DBSCAN algorithm for document clustering","author":"Creţulescu, Radu G and Morariu, Daniel I and Breazu, Macarie and Volovici, Daniel","meta_info":{"year":"2019","pages":"58--66","number":"1","volume":"9","journal":"International Journal of Advanced Statistics and IT&C for Economics and Life Sciences"}}
{"bib_id":"von2007tutorial","title":"A tutorial on spectral clustering","author":"Von Luxburg, Ulrike","meta_info":{"publisher":"Springer","year":"2007","pages":"395--416","volume":"17","journal":"Statistics and computing"}}
{"bib_id":"jia2014latest","title":"The latest research progress on spectral clustering","author":"Jia, Hongjie and Ding, Shifei and Xu, Xinzheng and Nie, Ru","meta_info":{"publisher":"Springer","year":"2014","pages":"1477--1486","volume":"24","journal":"Neural Computing and Applications"}}
{"bib_id":"bach2003learning","title":"Learning spectral clustering","author":"Bach, Francis and Jordan, Michael","meta_info":{"year":"2003","volume":"16","journal":"Advances in neural information processing systems"}}
{"bib_id":"abbas2023semdedup","title":"Semdedup: Data-efficient learning at web-scale through semantic deduplication","author":"Abbas, Amro and Tirumala, Kushal and Simig, Dániel and Ganguli, Surya and Morcos, Ari S","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2303.09540"}}
{"bib_id":"sorscher2022beyond","title":"Beyond neural scaling laws: beating power law scaling via data pruning","author":"Sorscher, Ben and Geirhos, Robert and Shekhar, Shashank and Ganguli, Surya and Morcos, Ari","meta_info":{"year":"2022","pages":"19523--19536","volume":"35","journal":"Advances in Neural Information Processing Systems"}}
{"bib_id":"feldman2011unified","title":"A unified framework for approximating and clustering data","author":"Feldman, Dan and Langberg, Michael","meta_info":{"year":"2011","pages":"569--578","booktitle":"Proceedings of the forty-third annual ACM symposium on Theory of computing"}}
{"bib_id":"wold1987principal","title":"Principal component analysis","author":"Wold, Svante and Esbensen, Kim and Geladi, Paul","meta_info":{"publisher":"Elsevier","year":"1987","pages":"37--52","number":"1-3","volume":"2","journal":"Chemometrics and intelligent laboratory systems"}}
{"bib_id":"colson2007overview","title":"An overview of bilevel optimization","author":"Colson, Benoı̂t and Marcotte, Patrice and Savard, Gilles","meta_info":{"publisher":"Springer","year":"2007","pages":"235--256","volume":"153","journal":"Annals of operations research"}}
{"bib_id":"sinha2017review","title":"A review on bilevel optimization: From classical to evolutionary approaches and applications","author":"Sinha, Ankur and Malo, Pekka and Deb, Kalyanmoy","meta_info":{"publisher":"IEEE","year":"2017","pages":"276--295","number":"2","volume":"22","journal":"IEEE transactions on evolutionary computation"}}
{"bib_id":"zhang2022advancing","title":"Advancing model pruning via bi-level optimization","author":"Zhang, Yihua and Yao, Yuguang and Ram, Parikshit and Zhao, Pu and Chen, Tianlong and Hong, Mingyi and Wang, Yanzhi and Liu, Sijia","meta_info":{"year":"2022","pages":"18309--18326","volume":"35","journal":"Advances in Neural Information Processing Systems"}}
{"bib_id":"borsos2024data","title":"Data summarization via bilevel optimization","author":"Borsos, Zalán and Mutnỳ, Mojmı́r and Tagliasacchi, Marco and Krause, Andreas","meta_info":{"year":"2024","pages":"1--53","number":"73","volume":"25","journal":"Journal of Machine Learning Research"}}
{"bib_id":"pan2024scalebio","title":"ScaleBiO: Scalable Bilevel Optimization for LLM Data Reweighting","author":"Pan, Rui and Zhang, Jipeng and Pan, Xingyuan and Pi, Renjie and Wang, Xiaoyu and Zhang, Tong","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2406.19976"}}
{"bib_id":"zhang2024bilevel","title":"Bilevel Optimization in the Deep Learning Era: Methods and Applications","author":"Zhang, Lei","meta_info":{"publisher":"Virginia Tech","year":"2024"}}
{"bib_id":"li2023less","title":"Less is more: Data pruning for faster adversarial training","author":"Li, Yize and Zhao, Pu and Lin, Xue and Kailkhura, Bhavya and Goldhahn, Ryan","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2302.12366"}}
{"bib_id":"xie2020unsupervised","title":"Unsupervised data augmentation for consistency training","author":"Xie, Qizhe and Dai, Zihang and Hovy, Eduard and Luong, Thang and Le, Quoc","meta_info":{"year":"2020","pages":"6256--6268","volume":"33","journal":"Advances in neural information processing systems"}}
{"bib_id":"wang2021regularizing","title":"Regularizing deep networks with semantic data augmentation","author":"Wang, Yulin and Huang, Gao and Song, Shiji and Pan, Xuran and Xia, Yitong and Wu, Cheng","meta_info":{"publisher":"IEEE","year":"2021","pages":"3733--3748","number":"7","volume":"44","journal":"IEEE Transactions on Pattern Analysis and Machine Intelligence"}}
{"bib_id":"zhao2020domain","title":"Domain generalization via entropy regularization","author":"Zhao, Shanshan and Gong, Mingming and Liu, Tongliang and Fu, Huan and Tao, Dacheng","meta_info":{"year":"2020","pages":"16096--16107","volume":"33","journal":"Advances in neural information processing systems"}}
{"bib_id":"grandvalet2004semi","title":"Semi-supervised learning by entropy minimization","author":"Grandvalet, Yves and Bengio, Yoshua","meta_info":{"year":"2004","volume":"17","journal":"Advances in neural information processing systems"}}
{"bib_id":"erkan2010semi","title":"Semi-supervised learning via generalized maximum entropy","author":"Erkan, Ayse and Altun, Yasemin","meta_info":{"organization":"JMLR Workshop and Conference Proceedings","year":"2010","pages":"209--216","booktitle":"Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics"}}
{"bib_id":"killamsetty2021grad","title":"Grad-match: Gradient matching based data subset selection for efficient deep model training","author":"Killamsetty, Krishnateja and Durga, Sivasubramanian and Ramakrishnan, Ganesh and De, Abir and Iyer, Rishabh","meta_info":{"organization":"PMLR","year":"2021","pages":"5464--5474","booktitle":"International Conference on Machine Learning"}}
{"bib_id":"tan2024data","title":"Data pruning via moving-one-sample-out","author":"Tan, Haoru and Wu, Sitong and Du, Fei and Chen, Yukang and Wang, Zhibin and Wang, Fan and Qi, Xiaojuan","meta_info":{"year":"2024","volume":"36","journal":"Advances in Neural Information Processing Systems"}}
{"bib_id":"kamalov2020kernel","title":"Kernel density estimation based sampling for imbalanced class distribution","author":"Kamalov, Firuz","meta_info":{"publisher":"Elsevier","year":"2020","pages":"1192--1201","volume":"512","journal":"Information Sciences"}}
{"bib_id":"rezazadegan2011fast","title":"Fast and efficient saliency detection using sparse sampling and kernel density estimation","author":"Rezazadegan Tavakoli, Hamed and Rahtu, Esa and Heikkilä, Janne","meta_info":{"organization":"Springer","year":"2011","pages":"666--675","booktitle":"Image Analysis: 17th Scandinavian Conference, SCIA 2011, Ystad, Sweden, May 2011. Proceedings 17"}}
{"bib_id":"kwon2023fully","title":"A fully first-order method for stochastic bilevel optimization","author":"Kwon, Jeongyeol and Kwon, Dohyun and Wright, Stephen and Nowak, Robert D","meta_info":{"organization":"PMLR","year":"2023","pages":"18083--18113","booktitle":"International Conference on Machine Learning"}}
{"bib_id":"lu2024first","title":"First-order penalty methods for bilevel optimization","author":"Lu, Zhaosong and Mei, Sanyou","meta_info":{"publisher":"SIAM","year":"2024","pages":"1937--1969","number":"2","volume":"34","journal":"SIAM Journal on Optimization"}}
{"bib_id":"citovsky2021batch","title":"Batch active learning at scale","author":"Citovsky, Gui and DeSalvo, Giulia and Gentile, Claudio and Karydas, Lazaros and Rajagopalan, Anand and Rostamizadeh, Afshin and Kumar, Sanjiv","meta_info":{"year":"2021","pages":"11933--11944","volume":"34","journal":"Advances in Neural Information Processing Systems"}}
{"bib_id":"mouillot1999comparison","title":"A comparison of species diversity estimators","author":"Mouillot, David and Lepretre, Alain","meta_info":{"publisher":"Springer","year":"1999","pages":"203--215","volume":"41","journal":"Researches on Population Ecology"}}
{"bib_id":"peet1974measurement","title":"The measurement of species diversity","author":"Peet, Robert K","meta_info":{"publisher":"JSTOR","year":"1974","pages":"285--307","journal":"Annual review of ecology and systematics"}}
{"bib_id":"he2005hubbell","title":"Hubbell's fundamental biodiversity parameter and the Simpson diversity index","author":"He, Fangliang and Hu, Xin-Sheng","meta_info":{"publisher":"Wiley Online Library","year":"2005","pages":"386--390","number":"4","volume":"8","journal":"Ecology Letters"}}
{"bib_id":"gregorius2008generalized","title":"Generalized Simpson-diversity","author":"Gregorius, Hans-Rolf and Gillet, Elizabeth M","meta_info":{"publisher":"Elsevier","year":"2008","pages":"90--96","number":"1-2","volume":"211","journal":"Ecological Modelling"}}
{"bib_id":"zhou2020diversifying","title":"Diversifying multi-aspect search results using Simpson's diversity index","author":"Zhou, Jianghong and Agichtein, Eugene and Kallumadi, Surya","meta_info":{"year":"2020","pages":"2345--2348","booktitle":"Proceedings of the 29th ACM International conference on information & knowledge management"}}
{"bib_id":"wu2024result","title":"Result Diversification in Search and Recommendation: A Survey","author":"Wu, Haolun and Zhang, Yansen and Ma, Chen and Lyu, Fuyuan and He, Bowei and Mitra, Bhaskar and Liu, Xue","meta_info":{"publisher":"IEEE","year":"2024","journal":"IEEE Transactions on Knowledge and Data Engineering"}}
{"bib_id":"wu2022survey","title":"A survey of diversification techniques in search and recommendation","author":"Wu, Haolun and Zhang, Yansen and Ma, Chen and Lyu, Fuyuan and Diaz, Fernando and Liu, Xue","meta_info":{"year":"2022","volume":"2212","journal":"CoRR arXiv"}}
{"bib_id":"pasarkar2023cousins","title":"Cousins Of The Vendi Score: A Family Of Similarity-Based Diversity Metrics For Science And Machine Learning","author":"Pasarkar, Amey and Dieng, Adji Bousso","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2310.12952"}}
{"bib_id":"nguyen2024quality","title":"Quality-Weighted Vendi Scores And Their Application To Diverse Experimental Design","author":"Nguyen, Quan and Dieng, Adji Bousso","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2405.02449"}}
{"bib_id":"lee2023beyond","title":"Beyond scale: the diversity coefficient as a data quality metric demonstrates llms are pre-trained on formally diverse data","author":"Lee, Alycia and Miranda, Brando and Sundar, Sudharsan and Koyejo, Sanmi","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2306.13840"}}
{"bib_id":"miranda2022curse","title":"The curse of low task diversity: On the failure of transfer learning to outperform maml and their empirical equivalence","author":"Miranda, Brando and Yu, Patrick and Wang, Yu-Xiong and Koyejo, Sanmi","meta_info":{"year":"2022","journal":"arXiv preprint arXiv:2208.01545"}}
{"bib_id":"achille2019task2vec","title":"Task2vec: Task embedding for meta-learning","author":"Achille, Alessandro and Lam, Michael and Tewari, Rahul and Ravichandran, Avinash and Maji, Subhransu and Fowlkes, Charless C and Soatto, Stefano and Perona, Pietro","meta_info":{"year":"2019","pages":"6430--6439","booktitle":"Proceedings of the IEEE\/CVF international conference on computer vision"}}
{"bib_id":"nguyen2019toward","title":"Toward understanding catastrophic forgetting in continual learning","author":"Nguyen, Cuong V and Achille, Alessandro and Lam, Michael and Hassner, Tal and Mahadevan, Vijay and Soatto, Stefano","meta_info":{"year":"2019","journal":"arXiv preprint arXiv:1908.01091"}}
{"bib_id":"xie2021explanation","title":"An explanation of in-context learning as implicit bayesian inference","author":"Xie, Sang Michael and Raghunathan, Aditi and Liang, Percy and Ma, Tengyu","meta_info":{"year":"2021","journal":"arXiv preprint arXiv:2111.02080"}}
{"bib_id":"smith2020strategies","title":"Strategies for Difficulty Sampling Providing Diversity in Datasets","author":"Smith, John and Johnson, Lisa","meta_info":{"year":"2020","pages":"100--120","volume":"10","journal":"Journal of Machine Learning Research"}}
{"bib_id":"kiela2021dynabench","title":"Dynabench: Rethinking benchmarking in NLP","author":"Kiela, Douwe and Bartolo, Max and Nie, Yixin and Kaushik, Divyansh and Geiger, Atticus and Wu, Zhengxuan and Vidgen, Bertie and Prasad, Grusha and Singh, Amanpreet and Ringshia, Pratik and others","meta_info":{"year":"2021","journal":"arXiv preprint arXiv:2104.14337"}}
{"bib_id":"ethayarajh2022understanding","title":"Understanding Dataset Difficulty with $\\mathcalV$-Usable Information","author":"Ethayarajh, Kawin and Choi, Yejin and Swayamdipta, Swabha","meta_info":{"organization":"PMLR","year":"2022","pages":"5988--6008","booktitle":"International Conference on Machine Learning"}}
{"bib_id":"patel2021nlp","title":"Are NLP models really able to solve simple math word problems?","author":"Patel, Arkil and Bhattamishra, Satwik and Goyal, Navin","meta_info":{"year":"2021","journal":"arXiv preprint arXiv:2103.07191"}}
{"bib_id":"belinkov2019analysis","title":"Analysis methods in neural language processing: A survey","author":"Belinkov, Yonatan and Glass, James","meta_info":{"publisher":"MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info","year":"2019","pages":"49--72","volume":"7","journal":"Transactions of the Association for Computational Linguistics"}}
{"bib_id":"nie2019adversarial","title":"Adversarial NLI: A new benchmark for natural language understanding","author":"Nie, Yixin and Williams, Adina and Dinan, Emily and Bansal, Mohit and Weston, Jason and Kiela, Douwe","meta_info":{"year":"2019","journal":"arXiv preprint arXiv:1910.14599"}}
{"bib_id":"ribeiro2020beyond","title":"Beyond accuracy: Behavioral testing of NLP models with CheckList","author":"Ribeiro, Marco Tulio and Wu, Tongshuang and Guestrin, Carlos and Singh, Sameer","meta_info":{"year":"2020","journal":"arXiv preprint arXiv:2005.04118"}}
{"bib_id":"huang2016well","title":"How well do computers solve math word problems? large-scale dataset construction and evaluation","author":"Huang, Danqing and Shi, Shuming and Lin, Chin-Yew and Yin, Jian and Ma, Wei-Ying","meta_info":{"year":"2016","pages":"887--896","booktitle":"Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"}}
{"bib_id":"koncel2016mawps","title":"MAWPS: A math word problem repository","author":"Koncel-Kedziorski, Rik and Roy, Subhro and Amini, Aida and Kushman, Nate and Hajishirzi, Hannaneh","meta_info":{"year":"2016","pages":"1152--1157","booktitle":"Proceedings of the 2016 conference of the north american chapter of the association for computational linguistics: human language technologies"}}
{"bib_id":"chall1995readability","title":"Readability revisited: The new Dale-Chall readability formula","author":"Chall, Jeanne Sternlicht and Dale, Edgar","meta_info":{"year":"1995","journal":"Edgar Brookline Books"}}
{"bib_id":"klare1974assessing","title":"Assessing readability","author":"Klare, George R","meta_info":{"publisher":"JSTOR","year":"1974","pages":"62--102","journal":"Reading research quarterly"}}
{"bib_id":"begeny2014can","title":"Can readability formulas be used to successfully gauge difficulty of reading materials?","author":"Begeny, John C and Greene, Diana J","meta_info":{"publisher":"Wiley Online Library","year":"2014","pages":"198--215","number":"2","volume":"51","journal":"Psychology in the Schools"}}
{"bib_id":"connatser1999last","title":"Last rites for readability formulas in technical communication","author":"Connatser, Bradford R","meta_info":{"publisher":"SAGE Publications Sage CA: Los Angeles, CA","year":"1999","pages":"271--287","number":"3","volume":"29","journal":"Journal of technical writing and communication"}}
{"bib_id":"zakaluk1988readability","title":"Readability: Its Past, Present, and Future.","author":"Zakaluk, Beverly L and Samuels, S Jay","meta_info":{"publisher":"ERIC","year":"1988"}}
{"bib_id":"dale1949concept","title":"The concept of readability","author":"Dale, Edgar and Chall, Jeanne S","meta_info":{"publisher":"JSTOR","year":"1949","pages":"19--26","number":"1","volume":"26","journal":"Elementary English"}}
{"bib_id":"carrell1987readability","title":"Readability in ESL","author":"Carrell, Patricia L","meta_info":{"publisher":"University of Hawaii National Foreign Language Resource Center","year":"1987"}}
{"bib_id":"flesch1948new","title":"A new readability yardstick.","author":"Flesch, Rudolph","meta_info":{"publisher":"American Psychological Association","year":"1948","pages":"221","number":"3","volume":"32","journal":"Journal of applied psychology"}}
{"bib_id":"gunning1952technique","title":"The technique of clear writing","author":"Gunning, Robert","meta_info":{"year":"1952","journal":"(No Title)"}}
{"bib_id":"lin2021truthfulqa","title":"Truthfulqa: Measuring how models mimic human falsehoods","author":"Lin, Stephanie and Hilton, Jacob and Evans, Owain","meta_info":{"year":"2021","journal":"arXiv preprint arXiv:2109.07958"}}
{"bib_id":"jiang2023calibrating","title":"Calibrating language models via augmented prompt ensembles","author":"Jiang, Mingjian and Ruan, Yangjun and Huang, Sicong and Liao, Saifei and Pitis, Silviu and Grosse, Roger Baker and Ba, Jimmy","meta_info":{"year":"2023","journal":"Workshop on Challenges in Deployable Generative AI at International Conference on Machine Learning"}}
{"bib_id":"kadavath2022language","title":"Language models (mostly) know what they know","author":"Kadavath, Saurav and Conerly, Tom and Askell, Amanda and Henighan, Tom and Drain, Dawn and Perez, Ethan and Schiefer, Nicholas and Hatfield-Dodds, Zac and DasSarma, Nova and Tran-Johnson, Eli and others","meta_info":{"year":"2022","journal":"arXiv preprint arXiv:2207.05221"}}
{"bib_id":"ilyas2022datamodels","title":"Datamodels: Predicting predictions from training data","author":"Ilyas, Andrew and Park, Sung Min and Engstrom, Logan and Leclerc, Guillaume and Madry, Aleksander","meta_info":{"year":"2022","journal":"arXiv preprint arXiv:2202.00622"}}
{"bib_id":"park2023trak","title":"Trak: Attributing model behavior at scale","author":"Park, Sung Min and Georgiev, Kristian and Ilyas, Andrew and Leclerc, Guillaume and Madry, Aleksander","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2303.14186"}}
{"bib_id":"jain2023data","title":"A data-based perspective on transfer learning","author":"Jain, Saachi and Salman, Hadi and Khaddaj, Alaa and Wong, Eric and Park, Sung Min and Mądry, Aleksander","meta_info":{"year":"2023","pages":"3613--3622","booktitle":"Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern Recognition"}}
{"bib_id":"kang2024performance","title":"Performance scaling via optimal transport: Enabling data selection from partially revealed sources","author":"Kang, Feiyang and Just, Hoang Anh and Sahu, Anit Kumar and Jia, Ruoxi","meta_info":{"year":"2024","volume":"36","journal":"Advances in Neural Information Processing Systems"}}
{"bib_id":"chhabra2024data","title":"\" What Data Benefits My Classifier?\" Enhancing Model Performance and Interpretability through Influence-Based Data Selection","author":"Chhabra, Anshuman and Li, Peizhao and Mohapatra, Prasant and Liu, Hongfu","meta_info":{"year":"2024","booktitle":"The Twelfth International Conference on Learning Representations"}}
{"bib_id":"saunshi2022understanding","title":"Understanding influence functions and datamodels via harmonic analysis","author":"Saunshi, Nikunj and Gupta, Arushi and Braverman, Mark and Arora, Sanjeev","meta_info":{"year":"2022","booktitle":"The Eleventh International Conference on Learning Representations"}}
{"bib_id":"koh2017understanding","title":"Understanding black-box predictions via influence functions","author":"Koh, Pang Wei and Liang, Percy","meta_info":{"organization":"PMLR","year":"2017","pages":"1885--1894","booktitle":"International conference on machine learning"}}
{"bib_id":"ye2024distilled","title":"Distilled Datamodel with Reverse Gradient Matching","author":"Ye, Jingwen and Yu, Ruonan and Liu, Songhua and Wang, Xinchao","meta_info":{"year":"2024","pages":"11954--11963","booktitle":"Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern Recognition"}}
{"bib_id":"jain2022influence","title":"Influence functions for sequence tagging models","author":"Jain, Sarthak and Manjunatha, Varun and Wallace, Byron C and Nenkova, Ani","meta_info":{"year":"2022","journal":"arXiv preprint arXiv:2210.14177"}}
{"bib_id":"liu2024training","title":"On training data influence of gpt models","author":"Liu, Qingyi and Chai, Yekun and Wang, Shuohuan and Sun, Yu and Wang, Keze and Wu, Hua","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2404.07840"}}
{"bib_id":"picard2024influenciae","title":"Influenciæ: A library for tracing the influence back to the data-points","author":"Picard, Agustin and Hervier, Lucas and Fel, Thomas and Vigouroux, David","meta_info":{"organization":"Springer","year":"2024","pages":"193--204","booktitle":"World Conference on Explainable Artificial Intelligence"}}
{"bib_id":"bae2024training","title":"Training Data Attribution via Approximate Unrolled Differentation","author":"Bae, Juhan and Lin, Wu and Lorraine, Jonathan and Grosse, Roger","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2405.12186"}}
{"bib_id":"covert2024scaling","title":"Scaling Laws for the Value of Individual Data Points in Machine Learning","author":"Covert, Ian and Ji, Wenlong and Hashimoto, Tatsunori and Zou, James","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2405.20456"}}
{"bib_id":"kirchenbauer2024lmd3","title":"LMD3: Language Model Data Density Dependence","author":"Kirchenbauer, John and Honke, Garrett and Somepalli, Gowthami and Geiping, Jonas and Ippolito, Daphne and Lee, Katherine and Goldstein, Tom and Andre, David","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2405.06331"}}
{"bib_id":"nieth2024large","title":"Large-Scale Dataset Pruning in Adversarial Training through Data Importance Extrapolation","author":"Nieth, Björn and Altstidl, Thomas and Schwinn, Leo and Eskofier, Björn","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2406.13283"}}
{"bib_id":"engstrom2024dsdm","title":"Dsdm: Model-aware dataset selection with datamodels","author":"Engstrom, Logan and Feldmann, Axel and Madry, Aleksander","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2401.12926"}}
{"bib_id":"guu2023simfluence","title":"Simfluence: Modeling the influence of individual training examples by simulating training runs","author":"Guu, Kelvin and Webson, Albert and Pavlick, Ellie and Dixon, Lucas and Tenney, Ian and Bolukbasi, Tolga","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2303.08114"}}
{"bib_id":"xie2023data","title":"Data selection for language models via importance resampling","author":"Xie, Sang Michael and Santurkar, Shibani and Ma, Tengyu and Liang, Percy S","meta_info":{"year":"2023","pages":"34201--34227","volume":"36","journal":"Advances in Neural Information Processing Systems"}}
{"bib_id":"yu2024mates","title":"MATES: Model-Aware Data Selection for Efficient Pretraining with Data Influence Models","author":"Yu, Zichun and Das, Spandan and Xiong, Chenyan","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2406.06046"}}
{"bib_id":"toneva2018empirical","title":"An empirical study of example forgetting during deep neural network learning","author":"Toneva, Mariya and Sordoni, Alessandro and Combes, Remi Tachet des and Trischler, Adam and Bengio, Yoshua and Gordon, Geoffrey J","meta_info":{"year":"2018","journal":"arXiv preprint arXiv:1812.05159"}}
{"bib_id":"zhang2023counterfactual","title":"Counterfactual memorization in neural language models","author":"Zhang, Chiyuan and Ippolito, Daphne and Lee, Katherine and Jagielski, Matthew and Tramèr, Florian and Carlini, Nicholas","meta_info":{"year":"2023","pages":"39321--39362","volume":"36","journal":"Advances in Neural Information Processing Systems"}}
{"bib_id":"jin2024demystifying","title":"Demystifying Forgetting in Language Model Fine-Tuning with Statistical Analysis of Example Associations","author":"Jin, Xisen and Ren, Xiang","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2406.14026"}}
{"bib_id":"feldman2020does","title":"Does learning require memorization? a short tale about a long tail","author":"Feldman, Vitaly","meta_info":{"year":"2020","pages":"954--959","booktitle":"Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing"}}
{"bib_id":"zhang2021understanding","title":"Understanding deep learning (still) requires rethinking generalization","author":"Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol","meta_info":{"publisher":"ACM New York, NY, USA","year":"2021","pages":"107--115","number":"3","volume":"64","journal":"Communications of the ACM"}}
{"bib_id":"feldman2020neural","title":"What neural networks memorize and why: Discovering the long tail via influence estimation","author":"Feldman, Vitaly and Zhang, Chiyuan","meta_info":{"year":"2020","pages":"2881--2891","volume":"33","journal":"Advances in Neural Information Processing Systems"}}
{"bib_id":"tirumala2022memorization","title":"Memorization without overfitting: Analyzing the training dynamics of large language models","author":"Tirumala, Kushal and Markosyan, Aram and Zettlemoyer, Luke and Aghajanyan, Armen","meta_info":{"year":"2022","pages":"38274--38290","volume":"35","journal":"Advances in Neural Information Processing Systems"}}
{"bib_id":"maini2022characterizing","title":"Characterizing datapoints via second-split forgetting","author":"Maini, Pratyush and Garg, Saurabh and Lipton, Zachary and Kolter, J Zico","meta_info":{"year":"2022","pages":"30044--30057","volume":"35","journal":"Advances in Neural Information Processing Systems"}}
{"bib_id":"zhang2023ideal","title":"Ideal: Influence-driven selective annotations empower in-context learners in large language models","author":"Zhang, Shaokun and Xia, Xiaobo and Wang, Zhaoqing and Chen, Ling-Hao and Liu, Jiale and Wu, Qingyun and Liu, Tongliang","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2310.10873"}}
{"bib_id":"li2018influence","title":"Influence maximization on social graphs: A survey","author":"Li, Yuchen and Fan, Ju and Wang, Yanhao and Tan, Kian-Lee","meta_info":{"publisher":"IEEE","year":"2018","pages":"1852--1872","number":"10","volume":"30","journal":"IEEE Transactions on Knowledge and Data Engineering"}}
{"bib_id":"suzuki2023extracting","title":"Extracting representative subset from extensive text data for training pre-trained language models","author":"Suzuki, Jun and Zen, Heiga and Kazawa, Hideto","meta_info":{"publisher":"Elsevier","year":"2023","pages":"103249","number":"3","volume":"60","journal":"Information Processing & Management"}}
{"bib_id":"antoniades2024generalization","title":"Generalization vs Memorization: Tracing Language Models' Capabilities Back to Pretraining Data","author":"Antoniades, Antonis and Wang, Xinyi and Elazar, Yanai and Amayuelas, Alfonso and Albalak, Alon and Zhang, Kexun and Wang, William Yang","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2407.14985"}}
{"bib_id":"chen2024skill","title":"Skill-it! a data-driven skills framework for understanding and training language models","author":"Chen, Mayee and Roberts, Nicholas and Bhatia, Kush and Wang, Jue and Zhang, Ce and Sala, Frederic and Ré, Christopher","meta_info":{"year":"2024","volume":"36","journal":"Advances in Neural Information Processing Systems"}}
{"bib_id":"schoch2023data","title":"Data selection for fine-tuning large language models using transferred shapley values","author":"Schoch, Stephanie and Mishra, Ritwick and Ji, Yangfeng","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2306.10165"}}
{"bib_id":"yauney2023data","title":"Data similarity is not enough to explain language model performance","author":"Yauney, Gregory and Reif, Emily and Mimno, David","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2311.09006"}}
{"bib_id":"lin2022measuring","title":"Measuring the effect of training data on deep learning predictions via randomized experiments","author":"Lin, Jinkun and Zhang, Anqi and Lécuyer, Mathias and Li, Jinyang and Panda, Aurojit and Sen, Siddhartha","meta_info":{"organization":"PMLR","year":"2022","pages":"13468--13504","booktitle":"International Conference on Machine Learning"}}
{"bib_id":"young2023investigating","title":"Investigating OpenAI’s ChatGPT potentials in generating Chatbot's dialogue for English as a foreign language learning","author":"Young, Julio Christian and Shishido, Makoto","meta_info":{"publisher":"Science and Information (SAI) Organization Limited","year":"2023","number":"6","volume":"14","journal":"International journal of advanced computer science and applications"}}
{"bib_id":"jia2019towards","title":"Towards efficient data valuation based on the shapley value","author":"Jia, Ruoxi and Dao, David and Wang, Boxin and Hubis, Frances Ann and Hynes, Nick and Gürel, Nezihe Merve and Li, Bo and Zhang, Ce and Song, Dawn and Spanos, Costas J","meta_info":{"organization":"PMLR","year":"2019","pages":"1167--1176","booktitle":"The 22nd International Conference on Artificial Intelligence and Statistics"}}
{"bib_id":"ghorbani2019data","title":"Data shapley: Equitable valuation of data for machine learning","author":"Ghorbani, Amirata and Zou, James","meta_info":{"organization":"PMLR","year":"2019","pages":"2242--2251","booktitle":"International conference on machine learning"}}
{"bib_id":"kwon2021beta","title":"Beta shapley: a unified and noise-reduced data valuation framework for machine learning","author":"Kwon, Yongchan and Zou, James","meta_info":{"year":"2021","journal":"arXiv preprint arXiv:2110.14049"}}
{"bib_id":"lecue2018regularization","title":"Regularization and the small-ball method i: sparse recovery","author":"Lecué, Guillaume and Mendelson, Shahar","meta_info":{"year":"2018"}}
{"bib_id":"zhao2020dataset","title":"Dataset condensation with gradient matching","author":"Zhao, Bo and Mopuri, Konda Reddy and Bilen, Hakan","meta_info":{"year":"2020","journal":"arXiv preprint arXiv:2006.05929"}}
{"bib_id":"zhao2023dataset","title":"Dataset condensation with distribution matching","author":"Zhao, Bo and Bilen, Hakan","meta_info":{"year":"2023","pages":"6514--6523","booktitle":"Proceedings of the IEEE\/CVF Winter Conference on Applications of Computer Vision"}}
{"bib_id":"du2024sequential","title":"Sequential subset matching for dataset distillation","author":"Du, Jiawei and Shi, Qin and Zhou, Joey Tianyi","meta_info":{"year":"2024","volume":"36","journal":"Advances in Neural Information Processing Systems"}}
{"bib_id":"balles2022gradient","title":"Gradient-matching coresets for rehearsal-based continual learning","author":"Balles, Lukas and Zappella, Giovanni and Archambeau, Cédric","meta_info":{"year":"2022","journal":"arXiv preprint arXiv:2203.14544"}}
{"bib_id":"jiang2023delving","title":"Delving into effective gradient matching for dataset condensation","author":"Jiang, Zixuan and Gu, Jiaqi and Liu, Mingjie and Pan, David Z","meta_info":{"organization":"IEEE","year":"2023","pages":"1--6","booktitle":"2023 IEEE International Conference on Omni-layer Intelligent Systems (COINS)"}}
{"bib_id":"pruthi2020estimating","title":"Estimating training data influence by tracing gradient descent","author":"Pruthi, Garima and Liu, Frederick and Kale, Satyen and Sundararajan, Mukund","meta_info":{"year":"2020","pages":"19920--19930","volume":"33","journal":"Advances in Neural Information Processing Systems"}}
{"bib_id":"brophy2023adapting","title":"Adapting and evaluating influence-estimation methods for gradient-boosted decision trees","author":"Brophy, Jonathan and Hammoudeh, Zayd and Lowd, Daniel","meta_info":{"year":"2023","pages":"1--48","number":"154","volume":"24","journal":"Journal of Machine Learning Research"}}
{"bib_id":"basu2020influence","title":"Influence functions in deep learning are fragile","author":"Basu, Samyadeep and Pope, Philip and Feizi, Soheil","meta_info":{"year":"2020","journal":"arXiv preprint arXiv:2006.14651"}}
{"bib_id":"alaa2020discriminative","title":"Discriminative jackknife: Quantifying uncertainty in deep learning via higher-order influence functions","author":"Alaa, Ahmed and Van Der Schaar, Mihaela","meta_info":{"organization":"PMLR","year":"2020","pages":"165--174","booktitle":"International Conference on Machine Learning"}}
{"bib_id":"du2014influence","title":"Influence function learning in information diffusion networks","author":"Du, Nan and Liang, Yingyu and Balcan, Maria and Song, Le","meta_info":{"organization":"PMLR","year":"2014","pages":"2016--2024","booktitle":"International Conference on Machine Learning"}}
{"bib_id":"6796137","title":"Fast Exact Multiplication by the Hessian","author":"Pearlmutter, Barak A.","meta_info":{"doi":"10.1162\/neco.1994.6.1.147","keywords":"","pages":"147-160","number":"1","volume":"6","year":"1994","journal":"Neural Computation"}}
{"bib_id":"nilsen2019efficient","title":"Efficient computation of hessian matrices in tensorflow","author":"Nilsen, Geir K and Munthe-Kaas, Antonella Z and Skaug, Hans J and Brun, Morten","meta_info":{"year":"2019","journal":"arXiv preprint arXiv:1905.05559"}}
{"bib_id":"mathieu2014fast","title":"Fast approximation of rotations and Hessians matrices","author":"Mathieu, Michael and LeCun, Yann","meta_info":{"year":"2014","journal":"arXiv preprint arXiv:1404.7195"}}
{"bib_id":"agarwal2016second","title":"Second-order stochastic optimization in linear time","author":"Agarwal, Naman and Bullins, Brian and Hazan, Elad","meta_info":{"year":"2016","pages":"15","volume":"1050","journal":"stat"}}
{"bib_id":"shewchuk1994introduction","title":"An introduction to the conjugate gradient method without the agonizing pain","author":"Shewchuk, Jonathan Richard and others","meta_info":{"publisher":"Carnegie-Mellon University. Department of Computer Science Pittsburgh","year":"1994"}}
{"bib_id":"xia2024less","title":"Less: Selecting influential data for targeted instruction tuning","author":"Xia, Mengzhou and Malladi, Sadhika and Gururangan, Suchin and Arora, Sanjeev and Chen, Danqi","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2402.04333"}}
{"bib_id":"kirsch2023does","title":"Does\" Deep Learning on a Data Diet\" reproduce? Overall yes, but GraNd at Initialization does not","author":"Kirsch, Andreas","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2303.14753"}}
{"bib_id":"bother2023modyn","title":"Modyn: A Platform for Model Training on Dynamic Datasets With Sample-Level Data Selection","author":"Böther, Maximilian and Gsteiger, Viktor and Robroek, Ties and Klimovic, Ana","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2312.06254"}}
{"bib_id":"everaert2023gio","title":"Gio: Gradient information optimization for training dataset selection","author":"Everaert, Dante and Potts, Christopher","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2306.11670"}}
{"bib_id":"grosse2023studyinglargelanguagemodel","title":"Studying Large Language Model Generalization with Influence Functions","author":"Roger Grosse and Juhan Bae and Cem Anil and Nelson Elhage and Alex Tamkin and Amirhossein Tajdini and Benoit Steiner and Dustin Li and Esin Durmus and Ethan Perez and Evan Hubinger and Kamilė Lukošiūtė and Karina Nguyen and Nicholas Joseph and Sam McCandlish and Jared Kaplan and Samuel R. Bowman","meta_info":{"url":"https:\/\/arxiv.org\/abs\/2308.03296","primaryclass":"cs.LG","archiveprefix":"arXiv","eprint":"2308.03296","year":"2023"}}
{"bib_id":"hmida2016hierarchical","title":"Hierarchical data topology based selection for large scale learning","author":"Hmida, Hmida and Hamida, Sana Ben and Borgi, Amel and Rukoz, Marta","meta_info":{"organization":"IEEE","year":"2016","pages":"1221--1226","booktitle":"2016 Intl IEEE Conferences on Ubiquitous Intelligence & Computing, Advanced and Trusted Computing, Scalable Computing and Communications, Cloud and Big Data Computing, Internet of People, and Smart World Congress (UIC\/ATC\/ScalCom\/CBDCom\/IoP\/SmartWorld)"}}
{"bib_id":"talavera1999feature","title":"Feature selection as a preprocessing step for hierarchical clustering","author":"Talavera, Luis","meta_info":{"year":"1999","pages":"389--397","volume":"99","booktitle":"ICML"}}
{"bib_id":"flach2012machine","title":"Machine learning: the art and science of algorithms that make sense of data","author":"Flach, Peter","meta_info":{"publisher":"Cambridge university press","year":"2012"}}
{"bib_id":"evans2013good","title":"How good are my data and what is the resolution?","author":"Evans, Philip R and Murshudov, Garib N","meta_info":{"publisher":"International Union of Crystallography","year":"2013","pages":"1204--1214","number":"7","volume":"69","journal":"Acta Crystallographica Section D: Biological Crystallography"}}
{"bib_id":"ran2023comprehensive","title":"Comprehensive survey on hierarchical clustering algorithms and the recent developments","author":"Ran, Xingcheng and Xi, Yue and Lu, Yonggang and Wang, Xiangwen and Lu, Zhenyu","meta_info":{"publisher":"Springer","year":"2023","pages":"8219--8264","number":"8","volume":"56","journal":"Artificial Intelligence Review"}}
{"bib_id":"cabezas2023hierarchical","title":"Hierarchical clustering: Visualization, feature importance and model selection","author":"Cabezas, Luben MC and Izbicki, Rafael and Stern, Rafael B","meta_info":{"publisher":"Elsevier","year":"2023","pages":"110303","volume":"141","journal":"Applied Soft Computing"}}
{"bib_id":"george2021fastapproximatenaturalgradient","title":"Fast Approximate Natural Gradient Descent in a Kronecker-factored Eigenbasis","author":"Thomas George and César Laurent and Xavier Bouthillier and Nicolas Ballas and Pascal Vincent","meta_info":{"url":"https:\/\/arxiv.org\/abs\/1806.03884","primaryclass":"cs.LG","archiveprefix":"arXiv","eprint":"1806.03884","year":"2021"}}
{"bib_id":"myrzakhan2024open","title":"Open-LLM-Leaderboard: From Multi-choice to Open-style Questions for LLMs Evaluation, Benchmark, and Arena","author":"Myrzakhan, Aidar and Bsharat, Sondos Mahmoud and Shen, Zhiqiang","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2406.07545"}}
{"bib_id":"wolf2019huggingface","title":"Huggingface's transformers: State-of-the-art natural language processing","author":"Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Rémi and Funtowicz, Morgan and others","meta_info":{"year":"2019","journal":"arXiv preprint arXiv:1910.03771"}}
{"bib_id":"yang2022evaluating","title":"Evaluating natural language processing models with generalization metrics that do not need access to any training or testing data","author":"Yang, Yaoqing and Theisen, Ryan and Hodgkinson, Liam and Gonzalez, Joseph E and Ramchandran, Kannan and Martin, Charles H and Mahoney, Michael W","meta_info":{"year":"2022","journal":"arXiv preprint arXiv:2202.02842"}}
{"bib_id":"zhao2021datasetcondensationgradientmatching","title":"Dataset Condensation with Gradient Matching","author":"Bo Zhao and Konda Reddy Mopuri and Hakan Bilen","meta_info":{"url":"https:\/\/arxiv.org\/abs\/2006.05929","primaryclass":"cs.CV","archiveprefix":"arXiv","eprint":"2006.05929","year":"2021"}}
{"bib_id":"albalak2024survey","title":"A survey on data selection for language models","author":"Albalak, Alon and Elazar, Yanai and Xie, Sang Michael and Longpre, Shayne and Lambert, Nathan and Wang, Xinyi and Muennighoff, Niklas and Hou, Bairu and Pan, Liangming and Jeong, Haewon and others","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2402.16827"}}
{"bib_id":"wang2024survey","title":"A Survey on Data Selection for LLM Instruction Tuning","author":"Wang, Jiahao and Zhang, Bolin and Du, Qianlong and Zhang, Jiajun and Chu, Dianhui","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2402.05123"}}
{"bib_id":"zhou2024survey","title":"A Survey on Data Quality Dimensions and Tools for Machine Learning","author":"Zhou, Yuhan and Tu, Fengjiao and Sha, Kewei and Ding, Junhua and Chen, Haihua","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2406.19614"}}
{"bib_id":"liu2024datasets","title":"Datasets for large language models: A comprehensive survey","author":"Liu, Yang and Cao, Jiahuan and Liu, Chongyu and Ding, Kai and Jin, Lianwen","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2402.18041"}}
{"bib_id":"moore2010intelligent","title":"Intelligent selection of language model training data","author":"Moore, Robert C and Lewis, William","meta_info":{"year":"2010","pages":"220--224","booktitle":"Proceedings of the ACL 2010 conference short papers"}}
{"bib_id":"chen2024data","title":"Data-juicer: A one-stop data processing system for large language models","author":"Chen, Daoyuan and Huang, Yilun and Ma, Zhijian and Chen, Hesen and Pan, Xuchen and Ge, Ce and Gao, Dawei and Xie, Yuexiang and Liu, Zhaoyang and Gao, Jinyang and others","meta_info":{"year":"2024","pages":"120--134","booktitle":"Companion of the 2024 International Conference on Management of Data"}}
{"bib_id":"dodge2020fine","title":"Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping","author":"Dodge, Jesse and Ilharco, Gabriel and Schwartz, Roy and Farhadi, Ali and Hajishirzi, Hannaneh and Smith, Noah","meta_info":{"year":"2020","journal":"arXiv preprint arXiv:2002.06305"}}
{"bib_id":"kandpal2022deduplicating","title":"Deduplicating training data mitigates privacy risks in language models","author":"Kandpal, Nikhil and Wallace, Eric and Raffel, Colin","meta_info":{"organization":"PMLR","year":"2022","pages":"10697--10707","booktitle":"International Conference on Machine Learning"}}
{"bib_id":"li2022data","title":"Data augmentation approaches in natural language processing: A survey","author":"Li, Bohan and Hou, Yutai and Che, Wanxiang","meta_info":{"publisher":"Elsevier","year":"2022","pages":"71--90","volume":"3","journal":"Ai Open"}}
{"bib_id":"feng2021survey","title":"A survey of data augmentation approaches for NLP","author":"Feng, Steven Y and Gangal, Varun and Wei, Jason and Chandar, Sarath and Vosoughi, Soroush and Mitamura, Teruko and Hovy, Eduard","meta_info":{"year":"2021","journal":"arXiv preprint arXiv:2105.03075"}}
{"bib_id":"lee2021deduplicating","title":"Deduplicating training data makes language models better","author":"Lee, Katherine and Ippolito, Daphne and Nystrom, Andrew and Zhang, Chiyuan and Eck, Douglas and Callison-Burch, Chris and Carlini, Nicholas","meta_info":{"year":"2021","journal":"arXiv preprint arXiv:2107.06499"}}
{"bib_id":"malhotra2015survey","title":"A survey and comparative study of data deduplication techniques","author":"Malhotra, Jyoti and Bakal, Jagdish","meta_info":{"organization":"IEEE","year":"2015","pages":"1--5","booktitle":"2015 International Conference on Pervasive Computing (ICPC)"}}
{"bib_id":"gupta2021data","title":"Data quality for machine learning tasks","author":"Gupta, Nitin and Mujumdar, Shashank and Patel, Hima and Masuda, Satoshi and Panwar, Naveen and Bandyopadhyay, Sambaran and Mehta, Sameep and Guttula, Shanmukha and Afzal, Shazia and Sharma Mittal, Ruhi and others","meta_info":{"year":"2021","pages":"4040--4041","booktitle":"Proceedings of the 27th ACM SIGKDD conference on knowledge discovery & data mining"}}
{"bib_id":"zha2023data","title":"Data-centric artificial intelligence: A survey","author":"Zha, Daochen and Bhat, Zaid Pervaiz and Lai, Kwei-Herng and Yang, Fan and Jiang, Zhimeng and Zhong, Shaochen and Hu, Xia","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2303.10158"}}
{"bib_id":"ehrlinger2022survey","title":"A survey of data quality measurement and monitoring tools","author":"Ehrlinger, Lisa and Wöß, Wolfram","meta_info":{"publisher":"Frontiers Media SA","year":"2022","pages":"850611","volume":"5","journal":"Frontiers in big data"}}
{"bib_id":"mohammed2024data","title":"Data Quality Assessment: Challenges and Opportunities","author":"Mohammed, Sedir and Harmouch, Hazar and Naumann, Felix and Srivastava, Divesh","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2403.00526"}}
{"bib_id":"li2024active","title":"Active Learning for Data Quality Control: A Survey","author":"Li, Na and Qi, Yiyang and Li, Chaoran and Zhao, Zhiming","meta_info":{"publisher":"ACM New York, NY","year":"2024","journal":"ACM Journal of Data and Information Quality"}}
{"bib_id":"lu2023machine","title":"Machine learning for synthetic data generation: a review","author":"Lu, Yingzhou and Shen, Minjie and Wang, Huazheng and Wang, Xiao and van Rechem, Capucine and Wei, Wenqi","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2302.04062"}}
{"bib_id":"dix2023measuring","title":"Measuring the Robustness of ML Models Against Data Quality Issues in Industrial Time Series Data","author":"Dix, Marcel and Manca, Gianluca and Okafor, Kenneth Chigozie and Borrison, Reuben and Kirchheim, Konstantin and Sharma, Divyasheel and Chandrika, Kr and Maduskar, Deepti and Ortmeier, Frank","meta_info":{"organization":"IEEE","year":"2023","pages":"1--8","booktitle":"2023 IEEE 21st International Conference on Industrial Informatics (INDIN)"}}
{"bib_id":"priestley2023survey","title":"A survey of data quality requirements that matter in ML development pipelines","author":"Priestley, Maria and O’donnell, Fionntán and Simperl, Elena","meta_info":{"publisher":"ACM New York, NY","year":"2023","pages":"1--39","number":"2","volume":"15","journal":"ACM Journal of Data and Information Quality"}}
{"bib_id":"byabazaire2020data","title":"Data quality and trust: A perception from shared data in IoT","author":"Byabazaire, John and O'Hare, Gregory and Delaney, Declan","meta_info":{"organization":"IEEE","year":"2020","pages":"1--6","booktitle":"2020 IEEE International Conference on Communications Workshops (ICC Workshops)"}}
{"bib_id":"roh2019survey","title":"A survey on data collection for machine learning: a big data-ai integration perspective","author":"Roh, Yuji and Heo, Geon and Whang, Steven Euijong","meta_info":{"publisher":"IEEE","year":"2019","pages":"1328--1347","number":"4","volume":"33","journal":"IEEE Transactions on Knowledge and Data Engineering"}}
{"bib_id":"sidi2012data","title":"Data quality: A survey of data quality dimensions","author":"Sidi, Fatimah and Panahy, Payam Hassany Shariat and Affendey, Lilly Suriani and Jabar, Marzanah A and Ibrahim, Hamidah and Mustapha, Aida","meta_info":{"organization":"IEEE","year":"2012","pages":"300--304","booktitle":"2012 International Conference on Information Retrieval & Knowledge Management"}}
{"bib_id":"zhou2023dataset","title":"Dataset quantization","author":"Zhou, Daquan and Wang, Kai and Gu, Jianyang and Peng, Xiangyu and Lian, Dongze and Zhang, Yifan and You, Yang and Feng, Jiashi","meta_info":{"year":"2023","pages":"17205--17216","booktitle":"Proceedings of the IEEE\/CVF International Conference on Computer Vision"}}
{"bib_id":"batini2009methodologies","title":"Methodologies for data quality assessment and improvement","author":"Batini, Carlo and Cappiello, Cinzia and Francalanci, Chiara and Maurino, Andrea","meta_info":{"publisher":"ACM New York, NY, USA","year":"2009","pages":"1--52","number":"3","volume":"41","journal":"ACM computing surveys (CSUR)"}}
{"bib_id":"zhang2023instruction","title":"Instruction tuning for large language models: A survey","author":"Zhang, Shengyu and Dong, Linfeng and Li, Xiaoya and Zhang, Sen and Sun, Xiaofei and Wang, Shuhe and Li, Jiwei and Hu, Runyi and Zhang, Tianwei and Wu, Fei and others","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2308.10792"}}
{"bib_id":"longpre2023flan","title":"The flan collection: Designing data and methods for effective instruction tuning","author":"Longpre, Shayne and Hou, Le and Vu, Tu and Webson, Albert and Chung, Hyung Won and Tay, Yi and Zhou, Denny and Le, Quoc V and Zoph, Barret and Wei, Jason and others","meta_info":{"organization":"PMLR","year":"2023","pages":"22631--22648","booktitle":"International Conference on Machine Learning"}}
{"bib_id":"shu2023exploitability","title":"On the exploitability of instruction tuning","author":"Shu, Manli and Wang, Jiongxiao and Zhu, Chen and Geiping, Jonas and Xiao, Chaowei and Goldstein, Tom","meta_info":{"year":"2023","pages":"61836--61856","volume":"36","journal":"Advances in Neural Information Processing Systems"}}
{"bib_id":"jang2023exploring","title":"Exploring the benefits of training expert language models over instruction tuning","author":"Jang, Joel and Kim, Seungone and Ye, Seonghyeon and Kim, Doyoung and Logeswaran, Lajanugen and Lee, Moontae and Lee, Kyungjae and Seo, Minjoon","meta_info":{"organization":"PMLR","year":"2023","pages":"14702--14729","booktitle":"International Conference on Machine Learning"}}
{"bib_id":"ghosh2024closer","title":"A Closer Look at the Limitations of Instruction Tuning","author":"Ghosh, Sreyan and Evuru, Chandra Kiran Reddy and Kumar, Sonal and Aneja, Deepali and Jin, Zeyu and Duraiswami, Ramani and Manocha, Dinesh and others","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2402.05119"}}
{"bib_id":"kung2023models","title":"Do models really learn to follow instructions? an empirical study of instruction tuning","author":"Kung, Po-Nien and Peng, Nanyun","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2305.11383"}}
{"bib_id":"john1975d","title":"D-optimality for regression designs: a review","author":"John, RC St and Draper, Norman R","meta_info":{"publisher":"Taylor & Francis","year":"1975","pages":"15--23","number":"1","volume":"17","journal":"Technometrics"}}
{"bib_id":"murphy2012machine","title":"Machine learning: a probabilistic perspective","author":"Murphy, Kevin P","meta_info":{"publisher":"MIT press","year":"2012"}}
{"bib_id":"zhang2024self","title":"Self-Tuning: Instructing LLMs to Effectively Acquire New Knowledge through Self-Teaching","author":"Zhang, Xiaoying and Peng, Baolin and Tian, Ye and Zhou, Jingyan and Zhang, Yipeng and Mi, Haitao and Meng, Helen","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2406.06326"}}
{"bib_id":"shi2023don","title":"Don’t stop pretraining? make prompt-based fine-tuning powerful learner","author":"Shi, Zhengxiang and Lipani, Aldo","meta_info":{"year":"2023","pages":"5827--5849","volume":"36","journal":"Advances in Neural Information Processing Systems"}}
{"bib_id":"goyal2023finetune","title":"Finetune like you pretrain: Improved finetuning of zero-shot vision models","author":"Goyal, Sachin and Kumar, Ananya and Garg, Sankalp and Kolter, Zico and Raghunathan, Aditi","meta_info":{"year":"2023","pages":"19338--19347","booktitle":"Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern Recognition"}}
{"bib_id":"yildiz2024investigating","title":"Investigating Continual Pretraining in Large Language Models: Insights and Implications","author":"Yıldız, Çağatay and Ravichandran, Nishaanth Kanna and Punia, Prishruit and Bethge, Matthias and Ermis, Beyza","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2402.17400"}}
{"bib_id":"cossu2024continual","title":"Continual pre-training mitigates forgetting in language and vision","author":"Cossu, Andrea and Carta, Antonio and Passaro, Lucia and Lomonaco, Vincenzo and Tuytelaars, Tinne and Bacciu, Davide","meta_info":{"publisher":"Elsevier","year":"2024","pages":"106492","journal":"Neural Networks"}}
{"bib_id":"ke2024continual","title":"Continual Learning with Language Models","author":"Ke, Zixuan","meta_info":{"school":"University of Illinois at Chicago","year":"2024"}}
{"bib_id":"lou2024large","title":"Large Language Model Instruction Following: A Survey of Progresses and Challenges","author":"Lou, Renze and Zhang, Kai and Yin, Wenpeng","meta_info":{"publisher":"MIT Press 255 Main Street, 9th Floor, Cambridge, Massachusetts 02142, USA~…","year":"2024","pages":"1--10","journal":"Computational Linguistics"}}
{"bib_id":"cheng2024instruction","title":"Instruction Pre-Training: Language Models are Supervised Multitask Learners","author":"Cheng, Daixuan and Gu, Yuxian and Huang, Shaohan and Bi, Junyu and Huang, Minlie and Wei, Furu","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2406.14491"}}
{"bib_id":"wang2022self","title":"Self-instruct: Aligning language models with self-generated instructions","author":"Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh","meta_info":{"year":"2022","journal":"arXiv preprint arXiv:2212.10560"}}
{"bib_id":"xu2024magpie","title":"Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing","author":"Xu, Zhangchen and Jiang, Fengqing and Niu, Luyao and Deng, Yuntian and Poovendran, Radha and Choi, Yejin and Lin, Bill Yuchen","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2406.08464"}}
{"bib_id":"schick2020s","title":"It's not just size that matters: Small language models are also few-shot learners","author":"Schick, Timo and Schütze, Hinrich","meta_info":{"year":"2020","journal":"arXiv preprint arXiv:2009.07118"}}
