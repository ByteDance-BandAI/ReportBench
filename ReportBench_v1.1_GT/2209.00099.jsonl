{"bib_id":"ahmed2020dedemocratization","title":"The de-democratization of AI: Deep learning and the compute divide in artificial intelligence research","author":"Ahmed, Nur and Wahed, Muntasir","meta_info":{"url":"https:\/\/arxiv.org\/abs\/2010.15581v1","year":"2020","journal":"arXiv preprint arXiv:2010.15581v1"}}
{"bib_id":"pareto1896cours","title":"Cours d'Économie Politique professé à l'Université de Lausanne","author":"Pareto, Vilfredo","meta_info":{"publisher":"F. Rouge","year":"1896","volume":"1"}}
{"bib_id":"bell2022modeling","title":"Modeling the Machine Learning Multiverse","author":"Samuel Bell and Onno Kampman and Jesse Dodge and Neil D Lawrence","meta_info":{"url":"https:\/\/openreview.net\/forum?id=8OH6t0YQGPJ","year":"2022","editor":"Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho","booktitle":"Advances in Neural Information Processing Systems"}}
{"bib_id":"valipourdylora","title":"DyLoRA: Parameter Efficient Tuning of Pre-trained Models using Dynamic Search-Free Low Rank Adaptation","author":"Valipour, Mojtaba and Rezagholizadeh, Mehdi and Kobyzev, Ivan and Ghodsi, Ali","meta_info":{"url":"https:\/\/neurips2022-enlsp.github.io\/papers\/paper_37.pdf","pages":"1--6","booktitle":"2nd workshop on Efficiennt Natural Language and Speech Processing, (NeurIPS workshops) ","year":"2022"}}
{"bib_id":"gundersen2022sources","title":"Sources of Irreproducibility in Machine Learning: A Review","author":"Gundersen, Odd Erik and Coakley, Kevin and Kirkpatrick, Christine","meta_info":{"year":"2022","url":"https:\/\/arxiv.org\/abs\/2204.07610v1","journal":"arXiv preprint arXiv:2204.07610v1"}}
{"bib_id":"bouthillier2020survey","title":"Survey of machine-learning experimental methods at NeurIPS2019 and ICLR2020","author":"Bouthillier, Xavier and Varoquaux, Gaël","meta_info":{"hal_version":"v1","hal_id":"hal-02447823","pdf":"https:\/\/hal.science\/hal-02447823\/file\/ml_methods_survey.pdf","month":"January","year":"2020","institution":"Inria Saclay Ile de France","type":"Research Report","url":"https:\/\/hal.science\/hal-02447823"}}
{"bib_id":"NEURIPS2021_f514cec8","title":"Deep Reinforcement Learning at the Edge of the Statistical Precipice","author":"Agarwal, Rishabh and Schwarzer, Max and Castro, Pablo Samuel and Courville, Aaron C and Bellemare, Marc","meta_info":{"year":"2021","volume":"34","url":"https:\/\/proceedings.neurips.cc\/paper\/2021\/file\/f514cec81cb148559cf475e7426eed5e-Paper.pdf","publisher":"Curran Associates, Inc.","pages":"29304--29320","editor":"M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan","booktitle":"Advances in Neural Information Processing Systems"}}
{"bib_id":"wang-etal-2022-adamix","title":"AdaMix: Mixture-of-Adaptations for Parameter-efficient Model Tuning","author":"Wang, Yaqing  and\nAgarwal, Sahaj  and\nMukherjee, Subhabrata  and\nLiu, Xiaodong  and\nGao, Jing  and\nAwadallah, Ahmed Hassan  and\nGao, Jianfeng","meta_info":{"abstract":"Standard fine-tuning of large pre-trained language models (PLMs) for downstream tasks requires updating hundreds of millions to billions of parameters, and storing a large copy of the PLM weights for every task resulting in increased cost for storing, sharing and serving the models. To address this, parameter-efficient fine-tuning (PEFT) techniques were introduced where small trainable components are injected in the PLM and updated during fine-tuning. We propose AdaMix as a general PEFT method that tunes a mixture of adaptation modules -- given the underlying PEFT method of choice -- introduced in each Transformer layer while keeping most of the PLM weights frozen. For instance, AdaMix can leverage a mixture of adapters like Houlsby or a mixture of low rank decomposition matrices like LoRA to improve downstream task performance over the corresponding PEFT methods for fully supervised and few-shot NLU and NLG tasks. Further, we design AdaMix such that it matches the same computational cost and the number of tunable parameters as the underlying PEFT method. By only tuning 0.1-0.2% of PLM parameters, we show that AdaMix outperforms SOTA parameter-efficient fine-tuning and full model fine-tuning for both NLU and NLG tasks.","pages":"5744--5760","url":"https:\/\/aclanthology.org\/2022.emnlp-main.388","publisher":"Association for Computational Linguistics","address":"Abu Dhabi, United Arab Emirates","year":"2022","month":"December","booktitle":"Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"}}
{"bib_id":"reimers-gurevych-2017-reporting","title":"Reporting Score Distributions Makes a Difference: Performance Study of LSTM-networks for Sequence Tagging","author":"Reimers, Nils  and\nGurevych, Iryna","meta_info":{"abstract":"In this paper we show that reporting a single performance score is insufficient to compare non-deterministic approaches. We demonstrate for common sequence tagging tasks that the seed value for the random number generator can result in statistically significant ($p < 10^-4$) differences for state-of-the-art systems. For two recent systems for NER, we observe an absolute difference of one percentage point F₁-score depending on the selected seed value, making these systems perceived either as state-of-the-art or mediocre. Instead of publishing and reporting single performance scores, we propose to compare score distributions based on multiple executions. Based on the evaluation of 50.000 LSTM-networks for five sequence tagging tasks, we present network architectures that produce both superior performance as well as are more stable with respect to the remaining hyperparameters.","pages":"338--348","doi":"10.18653\/v1\/D17-1035","url":"https:\/\/aclanthology.org\/D17-1035","publisher":"Association for Computational Linguistics","address":"Copenhagen, Denmark","year":"2017","month":"September","booktitle":"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing"}}
{"bib_id":"leskovec2020mining","title":"Mining of Massive Data Sets","author":"Leskovec, Jure and Rajaraman, Anand and Ullman, Jeffrey David","meta_info":{"publisher":"Cambridge University Press","year":"2020","url":"https:\/\/books.google.pt\/books?id=S4HCDwAAQBAJ","lccn":"2020012035","isbn":"9781108476348"}}
{"bib_id":"gebru2021datasheets","title":"Datasheets for Datasets","author":"Gebru, Timnit and Morgenstern, Jamie and Vecchione, Briana and Vaughan, Jennifer Wortman and Wallach, Hanna and Iii, Hal Daumé and Crawford, Kate","meta_info":{"url":"https:\/\/cacm.acm.org\/magazines\/2021\/12\/256932-datasheets-for-datasets\/abstract","publisher":"ACM New York, NY, USA","year":"2021","pages":"86--92","number":"12","volume":"64","journal":"Communications of the ACM"}}
{"bib_id":"manes2018ensuring","title":"Ensuring More Sustainable Reporting in Europe Using Non-Financial Disclosure—De Facto and De Jure Evidence","author":"Manes-Rossi, Francesca and Tiron-Tudor, Adriana and Nicolò, Giuseppe and Zanellato, Gianluca","meta_info":{"url":"https:\/\/www.mdpi.com\/2071-1050\/10\/4\/1162","publisher":"MDPI","year":"2018","pages":"1162","number":"4","volume":"10","journal":"Sustainability"}}
{"bib_id":"pmlr-v162-du22c","title":"GLaM: Efficient Scaling of Language Models with Mixture-of-Experts","author":"Du, Nan and Huang, Yanping and Dai, Andrew M and Tong, Simon and Lepikhin, Dmitry and Xu, Yuanzhong and Krikun, Maxim and Zhou, Yanqi and Yu, Adams Wei and Firat, Orhan and Zoph, Barret and Fedus, Liam and Bosma, Maarten P and Zhou, Zongwei and Wang, Tao and Wang, Emma and Webster, Kellie and Pellat, Marie and Robinson, Kevin and Meier-Hellstern, Kathleen and Duke, Toju and Dixon, Lucas and Zhang, Kun and Le, Quoc and Wu, Yonghui and Chen, Zhifeng and Cui, Claire","meta_info":{"url":"https:\/\/proceedings.mlr.press\/v162\/du22c.html","pdf":"https:\/\/proceedings.mlr.press\/v162\/du22c\/du22c.pdf","publisher":"PMLR","month":"17--23 Jul","series":"Proceedings of Machine Learning Research","volume":"162","editor":"Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan","year":"2022","pages":"5547--5569","booktitle":"Proceedings of the 39th International Conference on Machine Learning"}}
{"bib_id":"chen2021evaluating","title":"Evaluating Large Language Models Trained on Code","author":"Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde de Oliveira Pinto and Jared Kaplan and Harri Edwards and Yuri Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and Dave Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William Hebgen Guss and Alex Nichol and Alex Paino and Nikolas Tezak and Jie Tang and Igor Babuschkin and Suchir Balaji and Shantanu Jain and William Saunders and Christopher Hesse and Andrew N. Carr and Jan Leike and Josh Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba","meta_info":{"year":"2021","url":"https:\/\/arxiv.org\/abs\/2107.03374v2","journal":"arXiv preprint arXiv:2107.03374v2"}}
{"bib_id":"thompson2020computational","title":"The Computational Limits of Deep Learning","author":"Thompson, Neil C. and Greenewald, Kristjan and Lee, Keeheon and Manso, Gabriel F.","meta_info":{"year":"2020","url":"https:\/\/arxiv.org\/abs\/2007.05558v1","journal":"arXiv preprint arXiv:2007.05558v1"}}
{"bib_id":"liu2021gpt","title":"GPT understands, too","author":"Liu, Xiao and Zheng, Yanan and Du, Zhengxiao and Ding, Ming and Qian, Yujie and Yang, Zhilin and Tang, Jie","meta_info":{"year":"2021","url":"https:\/\/arxiv.org\/abs\/2103.10385v1","journal":"arXiv preprint arXiv:2103.10385v1"}}
{"bib_id":"moosavi-etal-2022-adaptable","title":"Adaptable Adapters","author":"Moosavi, Nafise  and\nDelfosse, Quentin  and\nKersting, Kristian  and\nGurevych, Iryna","meta_info":{"abstract":"State-of-the-art pretrained NLP models contain a hundred million to trillion parameters. Adapters provide a parameter-efficient alternative for the full finetuning in which we can only finetune lightweight neural network layers on top of pretrained weights. Adapter layers are initialized randomly. However, existing work uses the same adapter architecture---i.e., the same adapter layer on top of each layer of the pretrained model---for every dataset, regardless of the properties of the dataset or the amount of available training data. In this work, we introduce adaptable adapters that contain (1) learning different activation functions for different layers and different input data, and (2) a learnable switch to select and only use the beneficial adapter layers. We show that adaptable adapters achieve on-par performances with the standard adapter architecture while using a considerably smaller number of adapter layers. In addition, we show that the selected adapter architecture by adaptable adapters transfers well across different data settings and similar tasks. We propose to use adaptable adapters for designing efficient and effective adapter architectures. The resulting adapters (a) contain about 50% of the learning parameters of the standard adapter and are therefore more efficient at training and inference, and require less storage space, and (b) achieve considerably higher performances in low-data settings.","pages":"3742--3753","doi":"10.18653\/v1\/2022.naacl-main.274","url":"https:\/\/aclanthology.org\/2022.naacl-main.274","publisher":"Association for Computational Linguistics","address":"Seattle, United States","year":"2022","month":"July","booktitle":"Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"}}
{"bib_id":"pfeiffer-etal-2020-adapterhub","title":"AdapterHub: A Framework for Adapting Transformers","author":"Pfeiffer, Jonas  and\nRücklé, Andreas  and\nPoth, Clifton  and\nKamath, Aishwarya  and\nVulić, Ivan  and\nRuder, Sebastian  and\nCho, Kyunghyun  and\nGurevych, Iryna","meta_info":{"abstract":"The current modus operandi in NLP involves downloading and fine-tuning pre-trained models consisting of millions or billions of parameters. Storing and sharing such large trained models is expensive, slow, and time-consuming, which impedes progress towards more general and versatile NLP methods that learn from and for many tasks. Adapters---small learnt bottleneck layers inserted within each layer of a pre-trained model--- ameliorate this issue by avoiding full fine-tuning of the entire model. However, sharing and integrating adapter layers is not straightforward. We propose AdapterHub, a framework that allows dynamic ``stiching-in″ of pre-trained adapters for different tasks and languages. The framework, built on top of the popular HuggingFace Transformers library, enables extremely easy and quick adaptations of state-of-the-art pre-trained models (e.g., BERT, RoBERTa, XLM-R) across tasks and languages. Downloading, sharing, and training adapters is as seamless as possible using minimal changes to the training scripts and a specialized infrastructure. Our framework enables scalable and easy access to sharing of task-specific models, particularly in low-resource scenarios. AdapterHub includes all recent adapter architectures and can be found at AdapterHub.ml","pages":"46--54","doi":"10.18653\/v1\/2020.emnlp-demos.7","url":"https:\/\/aclanthology.org\/2020.emnlp-demos.7","publisher":"Association for Computational Linguistics","address":"Online","year":"2020","month":"October","booktitle":"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations"}}
{"bib_id":"elman1993learning","title":"Learning and Development in Neural Networks: The Importance of Starting Small","author":"Jeffrey L. Elman","meta_info":{"url":"https:\/\/www.sciencedirect.com\/science\/article\/pii\/0010027793900584","doi":"https:\/\/doi.org\/10.1016\/0010-0277(93)90058-4","issn":"0010-0277","year":"1993","pages":"71-99","number":"1","volume":"48","journal":"Cognition"}}
{"bib_id":"lee-etal-2020-empowering","title":"Empowering Active Learning to Jointly Optimize System and User Demands","author":"Lee, Ji-Ung  and\nMeyer, Christian M.  and\nGurevych, Iryna","meta_info":{"abstract":"Existing approaches to active learning maximize the system performance by sampling unlabeled instances for annotation that yield the most efficient training. However, when active learning is integrated with an end-user application, this can lead to frustration for participating users, as they spend time labeling instances that they would not otherwise be interested in reading. In this paper, we propose a new active learning approach that jointly optimizes the seemingly counteracting objectives of the active learning system (training efficiently) and the user (receiving useful instances). We study our approach in an educational application, which particularly benefits from this technique as the system needs to rapidly learn to predict the appropriateness of an exercise to a particular user, while the users should receive only exercises that match their skills. We evaluate multiple learning strategies and user types with data from real users and find that our joint approach better satisfies both objectives when alternative methods lead to many unsuitable exercises for end users.","pages":"4233--4247","doi":"10.18653\/v1\/2020.acl-main.390","url":"https:\/\/aclanthology.org\/2020.acl-main.390","publisher":"Association for Computational Linguistics","address":"Online","year":"2020","month":"July","booktitle":"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"}}
{"bib_id":"klie-etal-2020-zero","title":"From Zero to Hero: Human-In-The-Loop Entity Linking in Low Resource Domains","author":"Klie, Jan-Christoph  and\nEckart de Castilho, Richard  and\nGurevych, Iryna","meta_info":{"abstract":"Entity linking (EL) is concerned with disambiguating entity mentions in a text against knowledge bases (KB). It is crucial in a considerable number of fields like humanities, technical writing and biomedical sciences to enrich texts with semantics and discover more knowledge. The use of EL in such domains requires handling noisy texts, low resource settings and domain-specific KBs. Existing approaches are mostly inappropriate for this, as they depend on training data. However, in the above scenario, there exists hardly annotated data, and it needs to be created from scratch. We therefore present a novel domain-agnostic Human-In-The-Loop annotation approach: we use recommenders that suggest potential concepts and adaptive candidate ranking, thereby speeding up the overall annotation process and making it less tedious for users. We evaluate our ranking approach in a simulation on difficult texts and show that it greatly outperforms a strong baseline in ranking accuracy. In a user study, the annotation speed improves by 35% compared to annotating without interactive support; users report that they strongly prefer our system. An open-source and ready-to-use implementation based on the text annotation platform INCEpTION (https:\/\/inception-project.github.io) is made available.","pages":"6982--6993","doi":"10.18653\/v1\/2020.acl-main.624","url":"https:\/\/aclanthology.org\/2020.acl-main.624","publisher":"Association for Computational Linguistics","address":"Online","year":"2020","month":"July","booktitle":"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"}}
{"bib_id":"margatina-etal-2021-active","title":"Active Learning by Acquiring Contrastive Examples","author":"Margatina, Katerina  and\nVernikos, Giorgos  and\nBarrault, Loı̈c  and\nAletras, Nikolaos","meta_info":{"abstract":"Common acquisition functions for active learning use either uncertainty or diversity sampling, aiming to select difficult and diverse data points from the pool of unlabeled data, respectively. In this work, leveraging the best of both worlds, we propose an acquisition function that opts for selecting contrastive examples, i.e. data points that are similar in the model feature space and yet the model outputs maximally different predictive likelihoods. We compare our approach, CAL (Contrastive Active Learning), with a diverse set of acquisition functions in four natural language understanding tasks and seven datasets. Our experiments show that CAL performs consistently better or equal than the best performing baseline across all tasks, on both in-domain and out-of-domain data. We also conduct an extensive ablation study of our method and we further analyze all actively acquired datasets showing that CAL achieves a better trade-off between uncertainty and diversity compared to other strategies.","pages":"650--663","doi":"10.18653\/v1\/2021.emnlp-main.51","url":"https:\/\/aclanthology.org\/2021.emnlp-main.51","publisher":"Association for Computational Linguistics","address":"Online and Punta Cana, Dominican Republic","year":"2021","month":"November","booktitle":"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing"}}
{"bib_id":"Ash2020Deep","title":"Deep Batch Active Learning by Diverse, Uncertain Gradient Lower Bounds","author":"Jordan T. Ash and Chicheng Zhang and Akshay Krishnamurthy and John Langford and Alekh Agarwal","meta_info":{"url":"https:\/\/openreview.net\/forum?id=ryghZJBKPS","year":"2020","booktitle":"International Conference on Learning Representations"}}
{"bib_id":"cortes2008sampling-bias","title":"Sample Selection Bias Correction Theory","author":"Cortes, Corinna\nand Mohri, Mehryar\nand Riley, Michael\nand Rostamizadeh, Afshin","meta_info":{"url":"https:\/\/dl.acm.org\/doi\/10.1007\/978-3-540-87987-9_8","isbn":"978-3-540-87987-9","pages":"38--53","address":"Berlin, Heidelberg","publisher":"Springer Berlin Heidelberg","year":"2008","booktitle":"Algorithmic Learning Theory","editor":"Freund, Yoav\nand Györfi, László\nand Turán, György\nand Zeugmann, Thomas"}}
{"bib_id":"baldock2021","title":"Deep Learning Through the Lens of Example Difficulty","author":"Baldock, Robert and Maennel, Hartmut and Neyshabur, Behnam","meta_info":{"year":"2021","volume":"34","url":"https:\/\/openreview.net\/forum?id=WWRBHhH158K","publisher":"Curran Associates, Inc.","pages":"10876--10889","editor":"M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan","booktitle":"Advances in Neural Information Processing Systems"}}
{"bib_id":"li-etal-2020-active-learning","title":"Active Learning for Coreference Resolution using Discrete Annotation","author":"Li, Belinda Z.  and\nStanovsky, Gabriel  and\nZettlemoyer, Luke","meta_info":{"abstract":"We improve upon pairwise annotation for active learning in coreference resolution, by asking annotators to identify mention antecedents if a presented mention pair is deemed not coreferent. This simple modification, when combined with a novel mention clustering algorithm for selecting which examples to label, is much more efficient in terms of the performance obtained per annotation budget. In experiments with existing benchmark coreference datasets, we show that the signal from this additional question leads to significant performance gains per human-annotation hour. Future work can use our annotation protocol to effectively develop coreference models for new domains. Our code is publicly available.","pages":"8320--8331","doi":"10.18653\/v1\/2020.acl-main.738","url":"https:\/\/aclanthology.org\/2020.acl-main.738","publisher":"Association for Computational Linguistics","address":"Online","year":"2020","month":"July","booktitle":"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"}}
{"bib_id":"bodo2011active","title":"Active Learning with Clustering","author":"Bodó, Zalán and Minier, Zsolt and Csató, Lehel","meta_info":{"organization":"JMLR Workshop and Conference Proceedings","url":"http:\/\/proceedings.mlr.press\/v16\/bodo11a\/bodo11a.pdf","year":"2011","pages":"127--139","booktitle":"Active Learning and Experimental Design workshop In conjunction with AISTATS 2010"}}
{"bib_id":"zhang-duh-2020-reproducible","title":"Reproducible and Efficient Benchmarks for Hyperparameter Optimization of Neural Machine Translation Systems","author":"Zhang, Xuan  and\nDuh, Kevin","meta_info":{"abstract":"Hyperparameter selection is a crucial part of building neural machine translation (NMT) systems across both academia and industry. Fine-grained adjustments to a model′s architecture or training recipe can mean the difference between a positive and negative research result or between a state-of-the-art and underperforming system. While recent literature has proposed methods for automatic hyperparameter optimization (HPO), there has been limited work on applying these methods to neural machine translation (NMT), due in part to the high costs associated with experiments that train large numbers of model variants. To facilitate research in this space, we introduce a lookup-based approach that uses a library of pre-trained models for fast, low cost HPO experimentation. Our contributions include (1) the release of a large collection of trained NMT models covering a wide range of hyperparameters, (2) the proposal of targeted metrics for evaluating HPO methods on NMT, and (3) a reproducible benchmark of several HPO methods against our model library, including novel graph-based and multiobjective methods.","pages":"393--408","doi":"10.1162\/tacl_a_00322","url":"https:\/\/aclanthology.org\/2020.tacl-1.26","publisher":"MIT Press","address":"Cambridge, MA","year":"2020","volume":"8","journal":"Transactions of the Association for Computational Linguistics"}}
{"bib_id":"dong2017learning","title":"Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon","author":"Dong, Xin and Chen, Shangyu and Pan, Sinno","meta_info":{"year":"2017","volume":"30","url":"https:\/\/proceedings.neurips.cc\/paper\/2017\/file\/c5dc3e08849bec07e33ca353de62ea04-Abstract.html","publisher":"Curran Associates, Inc.","pages":"","editor":"I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett","booktitle":"Advances in Neural Information Processing Systems"}}
{"bib_id":"hoefler2021sparsity","title":"Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks","author":"Hoefler, Torsten and Alistarh, Dan and Ben-Nun, Tal and Dryden, Nikoli and Peste, Alexandra","meta_info":{"url":"https:\/\/www.jmlr.org\/papers\/volume22\/21-0366\/21-0366.pdf","year":"2021","pages":"1--124","number":"241","volume":"22","journal":"Journal of Machine Learning Research"}}
{"bib_id":"sun-etal-2020-mobilebert","title":"MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices","author":"Sun, Zhiqing  and\nYu, Hongkun  and\nSong, Xiaodan  and\nLiu, Renjie  and\nYang, Yiming  and\nZhou, Denny","meta_info":{"abstract":"Natural Language Processing (NLP) has recently achieved great success by using huge pre-trained models with hundreds of millions of parameters. However, these models suffer from heavy model sizes and high latency such that they cannot be deployed to resource-limited mobile devices. In this paper, we propose MobileBERT for compressing and accelerating the popular BERT model. Like the original BERT, MobileBERT is task-agnostic, that is, it can be generically applied to various downstream NLP tasks via simple fine-tuning. Basically, MobileBERT is a thin version of BERT_LARGE, while equipped with bottleneck structures and a carefully designed balance between self-attentions and feed-forward networks. To train MobileBERT, we first train a specially designed teacher model, an inverted-bottleneck incorporated BERT_LARGE model. Then, we conduct knowledge transfer from this teacher to MobileBERT. Empirical studies show that MobileBERT is 4.3x smaller and 5.5x faster than BERT_BASE while achieving competitive results on well-known benchmarks. On the natural language inference tasks of GLUE, MobileBERT achieves a GLUE score of 77.7 (0.6 lower than BERT_BASE), and 62 ms latency on a Pixel 4 phone. On the SQuAD v1.1\/v2.0 question answering task, MobileBERT achieves a dev F1 score of 90.0\/79.2 (1.5\/2.1 higher than BERT_BASE).","pages":"2158--2170","doi":"10.18653\/v1\/2020.acl-main.195","url":"https:\/\/aclanthology.org\/2020.acl-main.195","publisher":"Association for Computational Linguistics","address":"Online","year":"2020","month":"July","booktitle":"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"}}
{"bib_id":"hinton-2015","title":"Distilling the Knowledge in a Neural Network","author":"Geoffrey Hinton and Oriol Vinyals and Jeffrey Dean","meta_info":{"booktitle":"NeurIPS Deep Learning and Representation Learning Workshop","url":"http:\/\/arxiv.org\/abs\/1503.02531","year":"2015"}}
{"bib_id":"iofinova2021well","title":"How Well Do Sparse ImageNet Models Transfer?","author":"Iofinova, Eugenia and Peste, Alexandra and Kurtz, Mark and Alistarh, Dan","meta_info":{"pages":"12266-12276","url":"https:\/\/openaccess.thecvf.com\/content\/CVPR2022\/html\/Iofinova_How_Well_Do_Sparse_ImageNet_Models_Transfer_CVPR_2022_paper.html","year":"2022","month":"June","booktitle":"Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"}}
{"bib_id":"dehghani2018universal","title":"Universal Transformers","author":"Mostafa Dehghani and Stephan Gouws and Oriol Vinyals and Jakob Uszkoreit and Lukasz Kaiser","meta_info":{"url":"https:\/\/openreview.net\/forum?id=HyzdRiR9Y7","year":"2019","booktitle":"International Conference on Learning Representations"}}
{"bib_id":"lan2019albert","title":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations","author":"Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu","meta_info":{"url":"https:\/\/openreview.net\/forum?id=H1eA7AEtvS","year":"2019","booktitle":"International Conference on Learning Representations"}}
{"bib_id":"reid-etal-2021-subformer-exploring","title":"Subformer: Exploring Weight Sharing for Parameter Efficiency in Generative Transformers","author":"Reid, Machel  and\nMarrese-Taylor, Edison  and\nMatsuo, Yutaka","meta_info":{"abstract":"Transformers have shown improved performance when compared to previous architectures for sequence processing such as RNNs. Despite their sizeable performance gains, as recently suggested, the model is computationally expensive to train and with a high parameter budget. In light of this, we explore parameter-sharing methods in Transformers with a specific focus on generative models. We perform an analysis of different parameter sharing\/reduction methods and develop the Subformer. Our model combines sandwich-style parameter sharing, which overcomes naive cross-layer parameter sharing in generative models, and self-attentive embedding factorization (SAFE). Experiments on machine translation, abstractive summarization and language modeling show that the Subformer can outperform the Transformer even when using significantly fewer parameters.","pages":"4081--4090","doi":"10.18653\/v1\/2021.findings-emnlp.344","url":"https:\/\/aclanthology.org\/2021.findings-emnlp.344","publisher":"Association for Computational Linguistics","address":"Punta Cana, Dominican Republic","year":"2021","month":"November","booktitle":"Findings of the Association for Computational Linguistics: EMNLP 2021"}}
{"bib_id":"jaegle2021perceiver","title":"Perceiver: General Perception with Iterative Attention","author":"Jaegle, Andrew and Gimeno, Felix and Brock, Andy and Vinyals, Oriol and Zisserman, Andrew and Carreira, Joao","meta_info":{"url":"http:\/\/proceedings.mlr.press\/v139\/jaegle21a\/jaegle21a.pdf","organization":"PMLR","year":"2021","pages":"4651--4664","booktitle":"International conference on machine learning"}}
{"bib_id":"lewis-etal-2020-bart","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension","author":"Lewis, Mike  and\nLiu, Yinhan  and\nGoyal, Naman  and\nGhazvininejad, Marjan  and\nMohamed, Abdelrahman  and\nLevy, Omer  and\nStoyanov, Veselin  and\nZettlemoyer, Luke","meta_info":{"abstract":"We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.","pages":"7871--7880","doi":"10.18653\/v1\/2020.acl-main.703","url":"https:\/\/aclanthology.org\/2020.acl-main.703","publisher":"Association for Computational Linguistics","address":"Online","year":"2020","month":"July","booktitle":"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"}}
{"bib_id":"cao-etal-2020-towards","title":"Towards Accurate and Reliable Energy Measurement of NLP Models","author":"Cao, Qingqing  and\nBalasubramanian, Aruna  and\nBalasubramanian, Niranjan","meta_info":{"abstract":"Accurate and reliable measurement of energy consumption is critical for making well-informed design choices when choosing and training large scale NLP models. In this work, we show that existing software-based energy estimations are not accurate because they do not take into account hardware differences and how resource utilization affects energy consumption. We conduct energy measurement experiments with four different models for a question answering task. We quantify the error of existing software-based energy estimations by using a hardware power meter that provides highly accurate energy measurements. Our key takeaway is the need for a more accurate energy estimation model that takes into account hardware variabilities and the non-linear relationship between resource utilization and energy consumption. We release the code and data at https:\/\/github.com\/csarron\/sustainlp2020-energy.","pages":"141--148","doi":"10.18653\/v1\/2020.sustainlp-1.19","url":"https:\/\/aclanthology.org\/2020.sustainlp-1.19","publisher":"Association for Computational Linguistics","address":"Online","year":"2020","month":"November","booktitle":"Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"}}
{"bib_id":"Barham2022","title":"Pathways: Asynchronous Distributed Dataflow for ML","author":"Barham, Paul and Chowdhery, Aakanksha and Dean, Jeff and Ghemawat, Sanjay and Hand, Steven and Hurt, Daniel and Isard, Michael and Lim, Hyeontaek and Pang, Ruoming and Roy, Sudip and Saeta, Brennan and Schuh, Parker and Sepassi, Ryan and Shafey , Laurent  and Thekkath, Chandu and Wu, Yonghui","meta_info":{"url":"https:\/\/proceedings.mlsys.org\/paper\/2022\/file\/98dce83da57b0395e163467c9dae521b-Paper.pdf","year":"2022","pages":"430--449","volume":"4","journal":"Proceedings of Machine Learning and Systems"}}
{"bib_id":"RFA","title":"Random Feature Attention","author":"Peng, Hao and Pappas, Nikolaos and Yogatama, Dani and Schwartz, Roy and Smith, Noah and Kong, Lingpeng","meta_info":{"url":"https:\/\/openreview.net\/forum?continueFlag=7fcad3444ce73135efa053c0f6709de5&id=QtTKTdVrFBB","year":"2020","booktitle":"International Conference on Learning Representations"}}
{"bib_id":"lewis2020retrieval","title":"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks","author":"Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and Küttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rocktäschel, Tim and Riedel, Sebastian and Kiela, Douwe","meta_info":{"year":"2020","volume":"33","url":"https:\/\/proceedings.neurips.cc\/paper\/2020\/file\/6b493230205f780e1bc26945df7481e5-Paper.pdf","publisher":"Curran Associates, Inc.","pages":"9459--9474","editor":"H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin","booktitle":"Advances in Neural Information Processing Systems"}}
{"bib_id":"khandelwal2019generalization","title":"Generalization through Memorization: Nearest Neighbor Language Models","author":"Urvashi Khandelwal and Omer Levy and Dan Jurafsky and Luke Zettlemoyer and Mike Lewis","meta_info":{"url":"https:\/\/openreview.net\/forum?id=HklBjCEKvH","year":"2020","booktitle":"International Conference on Learning Representations"}}
{"bib_id":"gu2018search","title":"Search Engine Guided Non-Parametric Neural Machine Translation","author":"Gu, Jiatao and Wang, Yong and Cho, Kyunghyun and Li, Victor O. K.","meta_info":{"url":"https:\/\/ojs.aaai.org\/index.php\/AAAI\/article\/view\/12013","booktitle":"Proceedings of the AAAI Conference on Artificial Intelligence","year":"2018","doi":"10.48550\/ARXIV.1705.07267"}}
{"bib_id":"martins2022efficient","title":"Efficient Machine Translation Domain Adaptation","author":"Martins, Pedro  and\nMarinho, Zita  and\nMartins, Andre","meta_info":{"pages":"23--29","doi":"10.18653\/v1\/2022.spanlp-1.3","url":"https:\/\/aclanthology.org\/2022.spanlp-1.3","publisher":"Association for Computational Linguistics","address":"Dublin, Ireland and Online","year":"2022","month":"May","booktitle":"Proceedings of the 1st Workshop on Semiparametric Methods in NLP: Decoupling Logic from Knowledge"}}
{"bib_id":"he2021efficient","title":"Efficient Nearest Neighbor Language Models","author":"He, Junxian and Neubig, Graham and Berg-Kirkpatrick, Taylor","meta_info":{"url":"https:\/\/arxiv.org\/abs\/2109.04212","year":"2021","pages":"5703--5714","booktitle":"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing"}}
{"bib_id":"souza2021","title":"A Tale Of Two Long Tails","author":"D'souza, Daniel and Nussbaum, Zach and Agarwal, Chirag and Hooker, Sara","meta_info":{"url":"https:\/\/arxiv.org\/abs\/2107.13098v1","year":"2021","journal":"arXiv preprint arXiv:2107.13098v1"}}
{"bib_id":"Agarwal_2022_CVPR","title":"Estimating Example Difficulty Using Variance of Gradients","author":"Agarwal, Chirag and D'souza, Daniel and Hooker, Sara","meta_info":{"url":"https:\/\/openaccess.thecvf.com\/content\/CVPR2022\/papers\/Agarwal_Estimating_Example_Difficulty_Using_Variance_of_Gradients_CVPR_2022_paper.pdf","pages":"10368-10378","year":"2022","month":"June","booktitle":"Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"}}
{"bib_id":"Shoaib2022","title":"Metadata Archaeology: Unearthing Data Subsets by Leveraging Training Dynamics","author":"Siddiqui, Shoaib Ahmed and Rajkumar, Nitarshan and Maharaj, Tegan and Krueger, David and Hooker, Sara","meta_info":{"copyright":"Creative Commons Attribution Non Commercial Share Alike 4.0 International","year":"2021","journal":"arXiv preprint arXiv:2209.10015v1","publisher":"arXiv","keywords":"Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences","url":"https:\/\/arxiv.org\/abs\/2209.10015v1","doi":"10.48550\/ARXIV.2209.10015"}}
{"bib_id":"yogatama2021adaptive","title":"Adaptive Semiparametric Language Models","author":"Yogatama, Dani  and\nde Masson d′Autume, Cyprien  and\nKong, Lingpeng","meta_info":{"pages":"362--373","doi":"10.1162\/tacl_a_00371","url":"https:\/\/aclanthology.org\/2021.tacl-1.22","publisher":"MIT Press","address":"Cambridge, MA","year":"2021","volume":"9","journal":"Transactions of the Association for Computational Linguistics"}}
{"bib_id":"Rajbhandari2022","title":"DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale","author":"Rajbhandari, Samyam and Li, Conglong and Yao, Zhewei and Zhang, Minjia and Aminabadi, Reza Yazdani and Awan, Ammar Ahmad and Rasley, Jeff and He, Yuxiong","meta_info":{"url":"https:\/\/proceedings.mlr.press\/v162\/rajbhandari22a.html","pdf":"https:\/\/proceedings.mlr.press\/v162\/rajbhandari22a\/rajbhandari22a.pdf","publisher":"PMLR","month":"17--23 Jul","series":"Proceedings of Machine Learning Research","volume":"162","editor":"Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan","year":"2022","pages":"18332--18346","booktitle":"Proceedings of the 39th International Conference on Machine Learning"}}
{"bib_id":"kaleab2021","title":"Keep the Gradients Flowing: Using Gradient Flow to Study Sparse Network Optimization","author":"Tessera, Kale-ab and Hooker, Sara and Rosman, Benjamin","meta_info":{"copyright":"arXiv.org perpetual, non-exclusive license","journal":"arXiv preprint arXiv:2102.01670v2","year":"2021","url":"https:\/\/arxiv.org\/abs\/2102.01670v2","publisher":"arXiv","keywords":"Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences","doi":"10.48550\/ARXIV.2102.01670"}}
{"bib_id":"tay2021long","title":"Long Range Arena : A Benchmark for Efficient Transformers","author":"Yi Tay and Mostafa Dehghani and Samira Abnar and Yikang Shen and Dara Bahri and Philip Pham and Jinfeng Rao and Liu Yang and Sebastian Ruder and Donald Metzler","meta_info":{"url":"https:\/\/openreview.net\/forum?id=qVyeW-grC2k","year":"2021","booktitle":"International Conference on Learning Representations"}}
{"bib_id":"xu-etal-2021-beyond","title":"Beyond Preserved Accuracy: Evaluating Loyalty and Robustness of BERT Compression","author":"Xu, Canwen  and\nZhou, Wangchunshu  and\nGe, Tao  and\nXu, Ke  and\nMcAuley, Julian  and\nWei, Furu","meta_info":{"abstract":"Recent studies on compression of pretrained language models (e.g., BERT) usually use preserved accuracy as the metric for evaluation. In this paper, we propose two new metrics, label loyalty and probability loyalty that measure how closely a compressed model (i.e., student) mimics the original model (i.e., teacher). We also explore the effect of compression with regard to robustness under adversarial attacks. We benchmark quantization, pruning, knowledge distillation and progressive module replacing with loyalty and robustness. By combining multiple compression techniques, we provide a practical strategy to achieve better accuracy, loyalty and robustness.","pages":"10653--10659","doi":"10.18653\/v1\/2021.emnlp-main.832","url":"https:\/\/aclanthology.org\/2021.emnlp-main.832","publisher":"Association for Computational Linguistics","address":"Online and Punta Cana, Dominican Republic","year":"2021","month":"November","booktitle":"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing"}}
{"bib_id":"hinton2022forward","title":"The Forward-Forward Algorithm: Some Preliminary Investigations","author":"Hinton, Geoffrey","meta_info":{"url":"https:\/\/arxiv.org\/abs\/2212.13345v1","year":"2022","journal":"arXiv preprint arXiv:2212.13345v1"}}
{"bib_id":"Lepikhin2021GShardSG","title":"\\GS\\hard: Scaling Giant Models with Conditional Computation and Automatic Sharding","author":"Dmitry Lepikhin and HyoukJoong Lee and Yuanzhong Xu and Dehao Chen and Orhan Firat and Yanping Huang and Maxim Krikun and Noam Shazeer and Zhifeng Chen","meta_info":{"url":"https:\/\/openreview.net\/forum?id=qrwe7XHTmYb","year":"2021","booktitle":"International Conference on Learning Representations"}}
{"bib_id":"liu-etal-2020-fastbert","title":"FastBERT: a Self-distilling BERT with Adaptive Inference Time","author":"Liu, Weijie  and\nZhou, Peng  and\nWang, Zhiruo  and\nZhao, Zhe  and\nDeng, Haotang  and\nJu, Qi","meta_info":{"abstract":"Pre-trained language models like BERT have proven to be highly performant. However, they are often computationally expensive in many practical scenarios, for such heavy models can hardly be readily implemented with limited resources. To improve their efficiency with an assured model performance, we propose a novel speed-tunable FastBERT with adaptive inference time. The speed at inference can be flexibly adjusted under varying demands, while redundant calculation of samples is avoided. Moreover, this model adopts a unique self-distillation mechanism at fine-tuning, further enabling a greater computational efficacy with minimal loss in performance. Our model achieves promising results in twelve English and Chinese datasets. It is able to speed up by a wide range from 1 to 12 times than BERT if given different speedup thresholds to make a speed-performance tradeoff.","pages":"6035--6044","doi":"10.18653\/v1\/2020.acl-main.537","url":"https:\/\/aclanthology.org\/2020.acl-main.537","publisher":"Association for Computational Linguistics","address":"Online","year":"2020","month":"July","booktitle":"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"}}
{"bib_id":"jiao-etal-2020-tinybert","title":"TinyBERT: Distilling BERT for Natural Language Understanding","author":"Jiao, Xiaoqi  and\nYin, Yichun  and\nShang, Lifeng  and\nJiang, Xin  and\nChen, Xiao  and\nLi, Linlin  and\nWang, Fang  and\nLiu, Qun","meta_info":{"pages":"4163--4174","doi":"10.18653\/v1\/2020.findings-emnlp.372","url":"https:\/\/aclanthology.org\/2020.findings-emnlp.372","publisher":"Association for Computational Linguistics","address":"Online","year":"2020","month":"November","booktitle":"Findings of the Association for Computational Linguistics: EMNLP 2020"}}
{"bib_id":"Jiaao2022","title":"FasterMoE: Modeling and Optimizing Training of Large-Scale Dynamic Pre-Trained Models","author":"He, Jiaao and Zhai, Jidong and Antunes, Tiago and Wang, Haojie and Luo, Fuwen and Shi, Shangfeng and Li, Qin","meta_info":{"series":"PPoPP '22","location":"Seoul, Republic of Korea","keywords":"parallelism, distributed deep learning, performance modeling","numpages":"15","pages":"120–134","booktitle":"Proceedings of the 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming","doi":"10.1145\/3503221.3508418","url":"https:\/\/doi.org\/10.1145\/3503221.3508418","address":"New York, NY, USA","publisher":"Association for Computing Machinery","isbn":"9781450392044","year":"2022"}}
{"bib_id":"alajrami-aletras-2022-pre","title":"How does the pre-training objective affect what large language models learn about linguistic properties?","author":"Alajrami, Ahmed  and\nAletras, Nikolaos","meta_info":{"abstract":"Several pre-training objectives, such as masked language modeling (MLM), have been proposed to pre-train language models (e.g. BERT) with the aim of learning better language representations. However, to the best of our knowledge, no previous work so far has investigated how different pre-training objectives affect what BERT learns about linguistics properties. We hypothesize that linguistically motivated objectives such as MLM should help BERT to acquire better linguistic knowledge compared to other non-linguistically motivated objectives that are not intuitive or hard for humans to guess the association between the input and the label to be predicted. To this end, we pre-train BERT with two linguistically motivated objectives and three non-linguistically motivated ones. We then probe for linguistic characteristics encoded in the representation of the resulting models. We find strong evidence that there are only small differences in probing performance between the representations learned by the two different types of objectives. These surprising results question the dominant narrative of linguistically informed pre-training.","pages":"131--147","doi":"10.18653\/v1\/2022.acl-short.16","url":"https:\/\/aclanthology.org\/2022.acl-short.16","publisher":"Association for Computational Linguistics","address":"Dublin, Ireland","year":"2022","month":"May","booktitle":"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"}}
{"bib_id":"choromanski2020rethinking","title":"Rethinking Attention with Performers","author":"Krzysztof Marcin Choromanski and Valerii Likhosherstov and David Dohan and Xingyou Song and Andreea Gane and Tamas Sarlos and Peter Hawkins and Jared Quincy Davis and Afroz Mohiuddin and Lukasz Kaiser and David Benjamin Belanger and Lucy J Colwell and Adrian Weller","meta_info":{"url":"https:\/\/openreview.net\/forum?id=Ua6zuk0WRH","year":"2021","booktitle":"International Conference on Learning Representations"}}
{"bib_id":"katharopoulos2020transformers","title":"Transformers are RNNs:\nFast Autoregressive Transformers with Linear Attention","author":"Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, François","meta_info":{"url":"http:\/\/proceedings.mlr.press\/v119\/katharopoulos20a\/katharopoulos20a.pdf","organization":"PMLR","year":"2020","pages":"5156--5165","booktitle":"International Conference on Machine Learning"}}
{"bib_id":"roy2021efficient","title":"Efficient Content-Based Sparse Attention with Routing Transformers","author":"Roy, Aurko  and\nSaffar, Mohammad  and\nVaswani, Ashish  and\nGrangier, David","meta_info":{"pages":"53--68","doi":"10.1162\/tacl_a_00353","url":"https:\/\/aclanthology.org\/2021.tacl-1.4","publisher":"MIT Press","address":"Cambridge, MA","year":"2021","volume":"9","journal":"Transactions of the Association for Computational Linguistics"}}
{"bib_id":"zaheer2020big","title":"Big Bird: Transformers for Longer Sequences","author":"Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and Ahmed, Amr","meta_info":{"year":"2020","volume":"33","url":"https:\/\/proceedings.neurips.cc\/paper\/2020\/file\/c8512d142a2d849725f31a9a7a361ab9-Paper.pdf","publisher":"Curran Associates, Inc.","pages":"17283--17297","editor":"H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin","booktitle":"Advances in Neural Information Processing Systems"}}
{"bib_id":"stock2021training","title":"Training with Quantization Noise for Extreme Model Compression","author":"Pierre Stock and Angela Fan and Benjamin Graham and Edouard Grave and Rémi Gribonval and Herve Jegou and Armand Joulin","meta_info":{"url":"https:\/\/openreview.net\/forum?id=dV19Yyi1fS3","year":"2021","booktitle":"International Conference on Learning Representations"}}
{"bib_id":"kitaev2020reformer","title":"Reformer: The Efficient Transformer","author":"Nikita Kitaev and Lukasz Kaiser and Anselm Levskaya","meta_info":{"url":"https:\/\/openreview.net\/forum?id=rkgNKkHtvB","year":"2020","booktitle":"International Conference on Learning Representations"}}
{"bib_id":"rae2019compressive","title":"Compressive Transformers for Long-Range Sequence Modelling","author":"Jack W. Rae and Anna Potapenko and Siddhant M. Jayakumar and Chloe Hillier and Timothy P. Lillicrap","meta_info":{"url":"https:\/\/openreview.net\/forum?id=SylKikSYDH","year":"2020","booktitle":"International Conference on Learning Representations"}}
{"bib_id":"dao2021","title":"Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models","author":"Beidi Chen and Tri Dao and Kaizhao Liang and Jiaming Yang and Zhao Song and Atri Rudra and Christopher Re","meta_info":{"url":"https:\/\/openreview.net\/forum?id=Nfl-iXa-y7R","year":"2022","booktitle":"International Conference on Learning Representations"}}
{"bib_id":"wang-etal-2020-hat","title":"HAT: Hardware-Aware Transformers for Efficient Natural Language Processing","author":"Wang, Hanrui  and\nWu, Zhanghao  and\nLiu, Zhijian  and\nCai, Han  and\nZhu, Ligeng  and\nGan, Chuang  and\nHan, Song","meta_info":{"pages":"7675--7688","doi":"10.18653\/v1\/2020.acl-main.686","url":"https:\/\/aclanthology.org\/2020.acl-main.686","publisher":"Association for Computational Linguistics","address":"Online","year":"2020","month":"July","booktitle":"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"}}
{"bib_id":"wang-etal-2020-structured","title":"Structured Pruning of Large Language Models","author":"Wang, Ziheng  and\nWohlwend, Jeremy  and\nLei, Tao","meta_info":{"pages":"6151--6162","doi":"10.18653\/v1\/2020.emnlp-main.496","url":"https:\/\/aclanthology.org\/2020.emnlp-main.496","publisher":"Association for Computational Linguistics","address":"Online","year":"2020","month":"November","booktitle":"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)"}}
{"bib_id":"hessenthaler-etal-2022-bridging","title":"Bridging Fairness and Environmental Sustainability in Natural Language Processing","author":"Hessenthaler, Marius  and\nStrubell, Emma  and\nHovy, Dirk  and\nLauscher, Anne","meta_info":{"pages":"7817--7836","url":"https:\/\/aclanthology.org\/2022.emnlp-main.533","publisher":"Association for Computational Linguistics","address":"Abu Dhabi, United Arab Emirates","year":"2022","month":"December","booktitle":"Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"}}
{"bib_id":"ogueji-etal-2022-intriguing","title":"Intriguing Properties of Compression on Multilingual Models","author":"Ogueji, Kelechi  and\nAhia, Orevaoghene  and\nOnilude, Gbemileke  and\nGehrmann, Sebastian  and\nHooker, Sara  and\nKreutzer, Julia","meta_info":{"pages":"9092--9110","url":"https:\/\/aclanthology.org\/2022.emnlp-main.619","publisher":"Association for Computational Linguistics","address":"Abu Dhabi, United Arab Emirates","year":"2022","month":"December","booktitle":"Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"}}
{"bib_id":"dai2019transformer","title":"Transformer-XL: Attentive Language Models beyond a Fixed-Length Context","author":"Dai, Zihang  and\nYang, Zhilin  and\nYang, Yiming  and\nCarbonell, Jaime  and\nLe, Quoc  and\nSalakhutdinov, Ruslan","meta_info":{"pages":"2978--2988","doi":"10.18653\/v1\/P19-1285","url":"https:\/\/aclanthology.org\/P19-1285","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"July","booktitle":"Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"}}
{"bib_id":"Hooker2021","title":"The hardware lottery","author":"Hooker, Sara","meta_info":{"doi":"10.1145\/3467017","journal":"Communications of the ACM","volume":"64","pages":"58-65","month":"12","year":"2021"}}
{"bib_id":"wu2022extreme","title":"Extreme Compression for Pre-trained Transformers Made Simple and Efficient","author":"Xiaoxia Wu and Zhewei Yao and Minjia Zhang and Conglong Li and Yuxiong He","meta_info":{"url":"https:\/\/openreview.net\/forum?id=xNeAhc2CNAl","year":"2022","editor":"Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho","booktitle":"Advances in Neural Information Processing Systems"}}
{"bib_id":"ruckle-etal-2021-adapterdrop","title":"AdapterDrop: On the Efficiency of Adapters in Transformers","author":"Rücklé, Andreas  and\nGeigle, Gregor  and\nGlockner, Max  and\nBeck, Tilman  and\nPfeiffer, Jonas  and\nReimers, Nils  and\nGurevych, Iryna","meta_info":{"abstract":"Transformer models are expensive to fine-tune, slow for inference, and have large storage requirements. Recent approaches tackle these shortcomings by training smaller models, dynamically reducing the model size, and by training light-weight adapters. In this paper, we propose AdapterDrop, removing adapters from lower transformer layers during training and inference, which incorporates concepts from all three directions. We show that AdapterDrop can dynamically reduce the computational overhead when performing inference over multiple tasks simultaneously, with minimal decrease in task performances. We further prune adapters from AdapterFusion, which improves the inference efficiency while maintaining the task performances entirely.","pages":"7930--7946","doi":"10.18653\/v1\/2021.emnlp-main.626","url":"https:\/\/aclanthology.org\/2021.emnlp-main.626","publisher":"Association for Computational Linguistics","address":"Online and Punta Cana, Dominican Republic","year":"2021","month":"November","booktitle":"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing"}}
{"bib_id":"pmlr-v139-hubara21a","title":"Accurate Post Training Quantization With Small Calibration Sets","author":"Hubara, Itay and Nahshan, Yury and Hanani, Yair and Banner, Ron and Soudry, Daniel","meta_info":{"abstract":"Lately, post-training quantization methods have gained considerable attention, as they are simple to use, and require only a small unlabeled calibration set. This small dataset cannot be used to fine-tune the model without significant over-fitting. Instead, these methods only use the calibration set to set the activations’ dynamic ranges. However, such methods always resulted in significant accuracy degradation, when used below 8-bits (except on small datasets). Here we aim to break the 8-bit barrier. To this end, we minimize the quantization errors of each layer or block separately by optimizing its parameters over the calibration set. We empirically demonstrate that this approach is: (1) much less susceptible to over-fitting than the standard fine-tuning approaches, and can be used even on a very small calibration set; and (2) more powerful than previous methods, which only set the activations’ dynamic ranges. We suggest two flavors for our method, parallel and sequential aim for a fixed and flexible bit-width allocation. For the latter, we demonstrate how to optimally allocate the bit-widths for each layer, while constraining accuracy degradation or model compression by proposing a novel integer programming formulation. Finally, we suggest model global statistics tuning, to correct biases introduced during quantization. Together, these methods yield state-of-the-art results for both vision and text models. For instance, on ResNet50, we obtain less than 1% accuracy degradation — with 4-bit weights and activations in all layers, but first and last. The suggested methods are two orders of magnitude faster than the traditional Quantize Aware Training approach used for lower than 8-bit quantization. We open-sourced our code \\textithttps:\/\/github.com\/papers-submission\/CalibTIP.","url":"https:\/\/proceedings.mlr.press\/v139\/hubara21a.html","pdf":"http:\/\/proceedings.mlr.press\/v139\/hubara21a\/hubara21a.pdf","publisher":"PMLR","month":"18--24 Jul","series":"Proceedings of Machine Learning Research","volume":"139","editor":"Meila, Marina and Zhang, Tong","year":"2021","pages":"4466--4475","booktitle":"Proceedings of the 38th International Conference on Machine Learning"}}
{"bib_id":"wang2022","title":"Robust Distillation for Worst-class Performance","author":"Wang, Serena and Narasimhan, Harikrishna and Zhou, Yichen and Hooker, Sara and Lukasik, Michal and Menon, Aditya Krishna","meta_info":{"url":"https:\/\/arxiv.org\/abs\/2206.06479v1","year":"2022","journal":"arXiv preprint arXiv:2206.06479v1"}}
{"bib_id":"zhu2022teach","title":"Teach Less, Learn More: On the Undistillable Classes in Knowledge Distillation","author":"Yichen Zhu and Ning Liu and Zhiyuan Xu and Xin Liu and Weibin Meng and Louis Wang and Zhicai Ou and Jian Tang","meta_info":{"url":"https:\/\/openreview.net\/forum?id=q6bZruC3dWJ","year":"2022","editor":"Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho","booktitle":"Advances in Neural Information Processing Systems"}}
{"bib_id":"stanton2021does","title":"Does Knowledge Distillation Really Work?","author":"Stanton, Samuel and Izmailov, Pavel and Kirichenko, Polina and Alemi, Alexander A and Wilson, Andrew G","meta_info":{"year":"2021","volume":"34","url":"https:\/\/proceedings.neurips.cc\/paper\/2021\/file\/376c6b9ff3bedbbea56751a84fffc10c-Paper.pdf","publisher":"Curran Associates, Inc.","pages":"6906--6919","editor":"M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan","booktitle":"Advances in Neural Information Processing Systems"}}
{"bib_id":"ahia-etal-2021-low-resource","title":"The Low-Resource Double Bind: An Empirical Study of Pruning for Low-Resource Machine Translation","author":"Ahia, Orevaoghene  and\nKreutzer, Julia  and\nHooker, Sara","meta_info":{"abstract":"A ``bigger is better″ explosion in the number of parameters in deep neural networks has made it increasingly challenging to make state-of-the-art networks accessible in compute-restricted environments. Compression techniques have taken on renewed importance as a way to bridge the gap. However, evaluation of the trade-offs incurred by popular compression techniques has been centered on high-resource datasets. In this work, we instead consider the impact of compression in a data-limited regime. We introduce the term low-resource double bind to refer to the co-occurrence of data limitations and compute resource constraints. This is a common setting for NLP for low-resource languages, yet the trade-offs in performance are poorly studied. Our work offers surprising insights into the relationship between capacity and generalization in data-limited regimes for the task of machine translation. Our experiments on magnitude pruning for translations from English into Yoruba, Hausa, Igbo and German show that in low-resource regimes, sparsity preserves performance on frequent sentences but has a disparate impact on infrequent ones. However, it improves robustness to out-of-distribution shifts, especially for datasets that are very distinct from the training distribution. Our findings suggest that sparsity can play a beneficial role at curbing memorization of low frequency attributes, and therefore offers a promising solution to the low-resource double bind.","pages":"3316--3333","doi":"10.18653\/v1\/2021.findings-emnlp.282","url":"https:\/\/aclanthology.org\/2021.findings-emnlp.282","publisher":"Association for Computational Linguistics","address":"Punta Cana, Dominican Republic","year":"2021","month":"November","booktitle":"Findings of the Association for Computational Linguistics: EMNLP 2021"}}
{"bib_id":"Mohammadshahi2022","title":"What Do Compressed Multilingual Machine Translation Models Forget?","author":"Mohammadshahi, Alireza  and\nNikoulina, Vassilina  and\nBerard, Alexandre  and\nBrun, Caroline  and\nHenderson, James  and\nBesacier, Laurent","meta_info":{"pages":"4308--4329","url":"https:\/\/aclanthology.org\/2022.findings-emnlp.317","publisher":"Association for Computational Linguistics","address":"Abu Dhabi, United Arab Emirates","year":"2022","month":"December","booktitle":"Findings of the Association for Computational Linguistics: EMNLP 2022"}}
{"bib_id":"LeeThorp2021FNetMT","title":"FNet: Mixing Tokens with Fourier Transforms","author":"Lee-Thorp, James  and\nAinslie, Joshua  and\nEckstein, Ilya  and\nOntanon, Santiago","meta_info":{"abstract":"We show that Transformer encoder architectures can be sped up, with limited accuracy costs, by replacing the self-attention sublayers with simple linear transformations that ``mix″ input tokens. Most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder with a standard, unparameterized Fourier Transform achieves 92-97% of the accuracy of BERT counterparts on the GLUE benchmark, but trains 80% faster on GPUs and 70% faster on TPUs at standard 512 input lengths. At longer input lengths, our FNet model is significantly faster: when compared to the ``efficient Transformers″ on the Long Range Arena benchmark, FNet matches the accuracy of the most accurate models, while outpacing the fastest models across all sequence lengths on GPUs (and across relatively shorter lengths on TPUs). Finally, FNet has a light memory footprint and is particularly efficient at smaller model sizes; for a fixed speed and accuracy budget, small FNet models outperform Transformer counterparts.","pages":"4296--4313","doi":"10.18653\/v1\/2022.naacl-main.319","url":"https:\/\/aclanthology.org\/2022.naacl-main.319","publisher":"Association for Computational Linguistics","address":"Seattle, United States","year":"2022","month":"July","booktitle":"Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"}}
{"bib_id":"Hooker2020","title":"Characterising Bias in Compressed Models","author":"Hooker, Sara and Moorosi, Nyalleng and Clark, Gregory and Bengio, Samy and Denton, Emily","meta_info":{"url":"https:\/\/arxiv.org\/abs\/2010.03058v1","year":"2020","journal":"arXiv preprint arXiv:2010.03058v1"}}
{"bib_id":"Hoffmann2022","title":"An Empirical Analysis of Compute-Optimal Large Language Model Training","author":"Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katherine Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Oriol Vinyals and Jack William Rae and Laurent Sifre","meta_info":{"url":"https:\/\/openreview.net\/forum?id=iBBcRUlOAPR","year":"2022","editor":"Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho","booktitle":"Advances in Neural Information Processing Systems"}}
{"bib_id":"gale2019","title":"The State of Sparsity in Deep Neural Networks","author":"Gale, Trevor and Elsen, Erich and Hooker, Sara","meta_info":{"copyright":"arXiv.org perpetual, non-exclusive license","year":"2019","publisher":"arXiv","journal":"arXiv preprint arXiv:1902.09574v1","keywords":"Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences","url":"https:\/\/arxiv.org\/abs\/1902.09574v1","doi":"10.48550\/ARXIV.1902.09574"}}
{"bib_id":"lee-etal-2022-deduplicating","title":"Deduplicating Training Data Makes Language Models Better","author":"Lee, Katherine  and\nIppolito, Daphne  and\nNystrom, Andrew  and\nZhang, Chiyuan  and\nEck, Douglas  and\nCallison-Burch, Chris  and\nCarlini, Nicholas","meta_info":{"abstract":"We find that existing language modeling datasets contain many near-duplicate examples and long repetitive substrings.As a result, over 1% of the unprompted output of language models trained on these datasets is copied verbatim from the training data.We develop two tools that allow us to deduplicate training datasets---for example removing from C4 a single 61 word English sentence that is repeated over 60,000 times.Deduplication allows us to train models that emit memorized text ten times less frequently and require fewer training steps to achieve the same or better accuracy.We can also reduce train-test overlap, which affects over 4% of the validation set of standard datasets, thus allowing for more accurate evaluation.Code for deduplication is released at https:\/\/github.com\/google-research\/deduplicate-text-datasets.","pages":"8424--8445","doi":"10.18653\/v1\/2022.acl-long.577","url":"https:\/\/aclanthology.org\/2022.acl-long.577","publisher":"Association for Computational Linguistics","address":"Dublin, Ireland","year":"2022","month":"May","booktitle":"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"}}
{"bib_id":"dehghani2021","title":"The Efficiency Misnomer","author":"Mostafa Dehghani and Yi Tay and Anurag Arnab and Lucas Beyer and Ashish Vaswani","meta_info":{"url":"https:\/\/openreview.net\/forum?id=iulEMLYh1uR","year":"2022","booktitle":"International Conference on Learning Representations"}}
{"bib_id":"shazeer2017outrageously","title":"Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer","author":"Noam Shazeer and Azalia Mirhoseini and Krzysztof Maziarz and Andy Davis and Quoc Le and Geoffrey Hinton and Jeff Dean","meta_info":{"url":"https:\/\/openreview.net\/forum?id=B1ckMDqlg","year":"2017","booktitle":"International Conference on Learning Representations"}}
{"bib_id":"jacobs1991adaptive","title":"Adaptive Mixtures of Local Experts","author":"Jacobs, Robert A and Jordan, Michael I and Nowlan, Steven J and Hinton, Geoffrey E","meta_info":{"url":"https:\/\/ieeexplore.ieee.org\/document\/6797059","publisher":"MIT Press","year":"1991","pages":"79--87","number":"1","volume":"3","journal":"Neural computation"}}
{"bib_id":"martins2022infty","title":"$ınfty$-former: Infinite Memory Transformer","author":"Martins, Pedro Henrique  and\nMarinho, Zita  and\nMartins, Andre","meta_info":{"pages":"5468--5485","doi":"10.18653\/v1\/2022.acl-long.375","url":"https:\/\/aclanthology.org\/2022.acl-long.375","publisher":"Association for Computational Linguistics","address":"Dublin, Ireland","year":"2022","month":"May","booktitle":"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"}}
{"bib_id":"gu2021efficiently","title":"Efficiently Modeling Long Sequences with Structured State Spaces","author":"Albert Gu and Karan Goel and Christopher Re","meta_info":{"url":"https:\/\/openreview.net\/forum?id=uYLFoz1vlAC","year":"2022","booktitle":"International Conference on Learning Representations"}}
{"bib_id":"borgeaud2021improving","title":"Improving Language Models by Retrieving from Trillions of Tokens","author":"Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and Van Den Driessche, George Bm and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and De Las Casas, Diego and Guy, Aurelia and Menick, Jacob and Ring, Roman and Hennigan, Tom and Huang, Saffron and Maggiore, Loren and Jones, Chris and Cassirer, Albin and Brock, Andy and Paganini, Michela and Irving, Geoffrey and Vinyals, Oriol and Osindero, Simon and Simonyan, Karen and Rae, Jack and Elsen, Erich and Sifre, Laurent","meta_info":{"url":"https:\/\/proceedings.mlr.press\/v162\/borgeaud22a.html","pdf":"https:\/\/proceedings.mlr.press\/v162\/borgeaud22a\/borgeaud22a.pdf","publisher":"PMLR","month":"17--23 Jul","series":"Proceedings of Machine Learning Research","volume":"162","editor":"Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan","year":"2022","pages":"2206--2240","booktitle":"Proceedings of the 39th International Conference on Machine Learning"}}
{"bib_id":"fedus2021switch","title":"Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity","author":"William Fedus and Barret Zoph and Noam Shazeer","meta_info":{"url":"http:\/\/jmlr.org\/papers\/v23\/21-0998.html","pages":"1--39","number":"120","volume":"23","year":"2022","journal":"Journal of Machine Learning Research"}}
{"bib_id":"Dao2022-yl","title":"FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness","author":"Tri Dao and Daniel Y Fu and Stefano Ermon and Atri Rudra and Christopher Re","meta_info":{"url":"https:\/\/openreview.net\/forum?id=H4DqfPSibmx","year":"2022","editor":"Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho","booktitle":"Advances in Neural Information Processing Systems"}}
{"bib_id":"Bach2022-di","title":"PromptSource: An Integrated Development Environment and\nRepository for Natural Language Prompts","author":"Bach, Stephen and Sanh, Victor and Yong, Zheng Xin and Webson,\nAlbert and Raffel, Colin and Nayak, Nihal V and Sharma, Abheesht\nand Kim, Taewoon and Bari, M Saiful and Fevry, Thibault and\nAlyafeai, Zaid and Dey, Manan and Santilli, Andrea and Sun,\nZhiqing and Ben-david, Srulik and Xu, Canwen and Chhablani,\nGunjan and Wang, Han and Fries, Jason and Al-shaibani, Maged and\nSharma, Shanya and Thakker, Urmish and Almubarak, Khalid and\nTang, Xiangru and Radev, Dragomir and Jiang, Mike Tian-Jian and\nRush, Alexander","meta_info":{"address":"Dublin, Ireland","doi":"10.18653\/v1\/2022.acl-demo.9","url":"https:\/\/aclanthology.org\/2022.acl-demo.9","year":"2022","month":"May","pages":"93--104","publisher":"Association for Computational Linguistics","booktitle":"Proceedings of the 60th Annual Meeting of the Association for\nComputational Linguistics: System Demonstrations"}}
{"bib_id":"jevons1866coal","title":"The Coal Question; An Inquiry Concerning the Progress of the Nation, and the Probable Exhaustion of Our Coal Mines","author":"Jevons, William Stanley","meta_info":{"isbn":"978-0-678-00107-3","publisher":"Macmillan & Co. London","year":"1866"}}
{"bib_id":"swayamdipta-etal-2020-dataset","title":"Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics","author":"Swayamdipta, Swabha  and\nSchwartz, Roy  and\nLourie, Nicholas  and\nWang, Yizhong  and\nHajishirzi, Hannaneh  and\nSmith, Noah A.  and\nChoi, Yejin","meta_info":{"pages":"9275--9293","doi":"10.18653\/v1\/2020.emnlp-main.746","url":"https:\/\/aclanthology.org\/2020.emnlp-main.746","publisher":"Association for Computational Linguistics","address":"Online","year":"2020","month":"November","booktitle":"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)"}}
{"bib_id":"Settles2012","title":"Active Learning","author":"Settles, Burr","meta_info":{"publisher":"Morgan & Claypool","volume":"18","series":"Synthesis Lectures on Artificial Intelligence and Machine Learning","year":"2012"}}
{"bib_id":"lowell-etal-2019-practical","title":"Practical Obstacles to Deploying Active Learning","author":"Lowell, David  and\nLipton, Zachary C.  and\nWallace, Byron C.","meta_info":{"pages":"21--30","doi":"10.18653\/v1\/D19-1003","url":"https:\/\/aclanthology.org\/D19-1003","publisher":"Association for Computational Linguistics","address":"Hong Kong, China","year":"2019","month":"November","booktitle":"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)"}}
{"bib_id":"yuan-etal-2020-cold","title":"Cold-start Active Learning through Self-supervised Language Modeling","author":"Yuan, Michelle  and\nLin, Hsuan-Tien  and\nBoyd-Graber, Jordan","meta_info":{"pages":"7935--7948","doi":"10.18653\/v1\/2020.emnlp-main.637","url":"https:\/\/aclanthology.org\/2020.emnlp-main.637","publisher":"Association for Computational Linguistics","address":"Online","year":"2020","month":"November","booktitle":"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)"}}
{"bib_id":"ein-dor-etal-2020-active","title":"Active Learning for BERT: An Empirical Study","author":"Ein-Dor, Liat  and\nHalfon, Alon  and\nGera, Ariel  and\nShnarch, Eyal  and\nDankin, Lena  and\nChoshen, Leshem  and\nDanilevsky, Marina  and\nAharonov, Ranit  and\nKatz, Yoav  and\nSlonim, Noam","meta_info":{"pages":"7949--7962","doi":"10.18653\/v1\/2020.emnlp-main.638","url":"https:\/\/aclanthology.org\/2020.emnlp-main.638","publisher":"Association for Computational Linguistics","address":"Online","year":"2020","month":"November","booktitle":"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)"}}
{"bib_id":"settles2008active","title":"Active Learning with Real Annotation Costs","author":"Settles, Burr and Craven, Mark and Friedland, Lewis","meta_info":{"url":"https:\/\/burrsettles.com\/pub\/settles.nips08ws.pdf","year":"2008","booktitle":"Proceedings of the NIPS workshop on cost-sensitive learning (Vol. 1)."}}
{"bib_id":"lee-klie-gurevych-2022","title":"Annotation Curricula to Implicitly Train Non-Expert Annotators","author":"Lee, Ji-Ung and Klie, Jan-Christoph and Gurevych, Iryna","meta_info":{"eprint":"https:\/\/direct.mit.edu\/coli\/article-pdf\/48\/2\/343\/2029108\/coli_a_00436.pdf","url":"https:\/\/doi.org\/10.1162\/coli_a_00436","doi":"10.1162\/coli_a_00436","issn":"0891-2017","abstract":"Annotation studies often require annotators to familiarize themselves with the task, its annotation scheme, and the data domain. This can be overwhelming in the beginning, mentally taxing, and induce errors into the resulting annotations; especially in citizen science or crowdsourcing scenarios where domain expertise is not required. To alleviate these issues, this work proposes annotation curricula, a novel approach to implicitly train annotators. The goal is to gradually introduce annotators into the task by ordering instances to be annotated according to a learning curriculum. To do so, this work formalizes annotation curricula for sentence- and paragraph-level annotation tasks, defines an ordering strategy, and identifies well-performing heuristics and interactively trained models on three existing English datasets. Finally, we provide a proof of concept for annotation curricula in a carefully designed user study with 40 voluntary participants who are asked to identify the most fitting misconception for English tweets about the Covid-19 pandemic. The results indicate that using a simple heuristic to order instances can already significantly reduce the total annotation time while preserving a high annotation quality. Annotation curricula thus can be a promising research direction to improve data collection. To facilitate future research—for instance, to adapt annotation curricula to specific tasks and expert annotation scenarios—all code and data from the user study consisting of 2,400 annotations is made available.1","month":"06","year":"2022","pages":"343-373","number":"2","volume":"48","journal":"Computational Linguistics"}}
{"bib_id":"silva-etal-2021-towards","title":"Towards a Comprehensive Understanding and Accurate Evaluation of Societal Biases in Pre-Trained Transformers","author":"Silva, Andrew  and\nTambwekar, Pradyumna  and\nGombolay, Matthew","meta_info":{"abstract":"The ease of access to pre-trained transformers has enabled developers to leverage large-scale language models to build exciting applications for their users. While such pre-trained models offer convenient starting points for researchers and developers, there is little consideration for the societal biases captured within these model risking perpetuation of racial, gender, and other harmful biases when these models are deployed at scale. In this paper, we investigate gender and racial bias across ubiquitous pre-trained language models, including GPT-2, XLNet, BERT, RoBERTa, ALBERT and DistilBERT. We evaluate bias within pre-trained transformers using three metrics: WEAT, sequence likelihood, and pronoun ranking. We conclude with an experiment demonstrating the ineffectiveness of word-embedding techniques, such as WEAT, signaling the need for more robust bias testing in transformers.","pages":"2383--2389","doi":"10.18653\/v1\/2021.naacl-main.189","url":"https:\/\/aclanthology.org\/2021.naacl-main.189","publisher":"Association for Computational Linguistics","address":"Online","year":"2021","month":"June","booktitle":"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"}}
{"bib_id":"xu2022can","title":"Can Model Compression Improve NLP Fairness","author":"Xu, Guangxuan and Hu, Qingyuan","meta_info":{"year":"2022","url":"https:\/\/arxiv.org\/abs\/2201.08542v1","journal":"arXiv preprint arXiv:2201.08542v1"}}
{"bib_id":"10.1145\/3472291","title":"A Survey of Deep Active Learning","author":"Ren, Pengzhen and Xiao, Yun and Chang, Xiaojun and Huang, Po-Yao and Li, Zhihui and Gupta, Brij B. and Chen, Xiaojiang and Wang, Xin","meta_info":{"keywords":"active learning, deep active learning, Deep learning","numpages":"40","articleno":"180","month":"oct","journal":"ACM Comput. Surv.","doi":"10.1145\/3472291","url":"https:\/\/doi.org\/10.1145\/3472291","issn":"0360-0300","number":"9","volume":"54","address":"New York, NY, USA","publisher":"Association for Computing Machinery","issue_date":"December 2022","year":"2021"}}
{"bib_id":"karamcheti-etal-2021-mind","title":"Mind Your Outliers! Investigating the Negative Impact of Outliers on Active Learning for Visual Question Answering","author":"Karamcheti, Siddharth  and\nKrishna, Ranjay  and\nFei-Fei, Li  and\nManning, Christopher","meta_info":{"pages":"7265--7281","doi":"10.18653\/v1\/2021.acl-long.564","url":"https:\/\/aclanthology.org\/2021.acl-long.564","publisher":"Association for Computational Linguistics","address":"Online","year":"2021","month":"August","booktitle":"Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)"}}
{"bib_id":"yuan-etal-2022-adapting","title":"Adapting Coreference Resolution Models through Active Learning","author":"Yuan, Michelle  and\nXia, Patrick  and\nMay, Chandler  and\nVan Durme, Benjamin  and\nBoyd-Graber, Jordan","meta_info":{"pages":"7533--7549","doi":"10.18653\/v1\/2022.acl-long.519","url":"https:\/\/aclanthology.org\/2022.acl-long.519","publisher":"Association for Computational Linguistics","address":"Dublin, Ireland","year":"2022","month":"May","booktitle":"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"}}
{"bib_id":"kirsch2019batchbald","title":"BatchBALD: Efficient and Diverse Batch Acquisition for Deep Bayesian Active Learning","author":"Kirsch, Andreas and van Amersfoort, Joost and Gal, Yarin","meta_info":{"year":"2019","volume":"32","url":"https:\/\/proceedings.neurips.cc\/paper\/2019\/file\/95323660ed2124450caaac2c46b5ed90-Paper.pdf","publisher":"Curran Associates, Inc.","pages":"","editor":"H. Wallach and H. Larochelle and A. Beygelzimer and F. d' Alché-Buc and E. Fox and R. Garnett","booktitle":"Advances in Neural Information Processing Systems"}}
{"bib_id":"gal2017deep","title":"Deep Bayesian Active Learning with Image Data","author":"Gal, Yarin and Islam, Riashat and Ghahramani, Zoubin","meta_info":{"url":"https:\/\/proceedings.mlr.press\/v70\/gal17a\/gal17a.pdf","organization":"PMLR","year":"2017","pages":"1183--1192","booktitle":"International Conference on Machine Learning"}}
{"bib_id":"sener2018active","title":"Active Learning for Convolutional Neural Networks: A Core-Set Approach","author":"Ozan Sener and Silvio Savarese","meta_info":{"url":"https:\/\/openreview.net\/forum?id=H1aIuk-RW","year":"2018","booktitle":"International Conference on Learning Representations"}}
{"bib_id":"dao2022monarch","title":"Monarch: Expressive Structured Matrices for Efficient and Accurate Training","author":"Dao, Tri and Chen, Beidi and Sohoni, Nimit S and Desai, Arjun and Poli, Michael and Grogan, Jessica and Liu, Alexander and Rao, Aniruddh and Rudra, Atri and Ré, Christopher","meta_info":{"url":"https:\/\/proceedings.mlr.press\/v162\/dao22a.html","organization":"PMLR","year":"2022","pages":"4690--4721","booktitle":"International Conference on Machine Learning"}}
{"bib_id":"kreutzer-etal-2022-quality","title":"Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets","author":"Kreutzer, Julia  and\nCaswell, Isaac  and\nWang, Lisa  and\nWahab, Ahsan  and\nvan Esch, Daan  and\nUlzii-Orshikh, Nasanbayar  and\nTapo, Allahsera  and\nSubramani, Nishant  and\nSokolov, Artem  and\nSikasote, Claytone  and\nSetyawan, Monang  and\nSarin, Supheakmungkol  and\nSamb, Sokhar  and\nSagot, Benoı̂t  and\nRivera, Clara  and\nRios, Annette  and\nPapadimitriou, Isabel  and\nOsei, Salomey  and\nSuarez, Pedro Ortiz  and\nOrife, Iroro  and\nOgueji, Kelechi  and\nRubungo, Andre Niyongabo  and\nNguyen, Toan Q.  and\nMüller, Mathias  and\nMüller, André  and\nMuhammad, Shamsuddeen Hassan  and\nMuhammad, Nanda  and\nMnyakeni, Ayanda  and\nMirzakhalov, Jamshidbek  and\nMatangira, Tapiwanashe  and\nLeong, Colin  and\nLawson, Nze  and\nKudugunta, Sneha  and\nJernite, Yacine  and\nJenny, Mathias  and\nFirat, Orhan  and\nDossou, Bonaventure F. P.  and\nDlamini, Sakhile  and\nde Silva, Nisansa  and\nÇabuk Ballı, Sakine  and\nBiderman, Stella  and\nBattisti, Alessia  and\nBaruwa, Ahmed  and\nBapna, Ankur  and\nBaljekar, Pallavi  and\nAzime, Israel Abebe  and\nAwokoya, Ayodele  and\nAtaman, Duygu  and\nAhia, Orevaoghene  and\nAhia, Oghenefego  and\nAgrawal, Sweta  and\nAdeyemi, Mofetoluwa","meta_info":{"abstract":"With the success of large-scale pre-training and multilingual modeling in Natural Language Processing (NLP), recent years have seen a proliferation of large, Web-mined text datasets covering hundreds of languages. We manually audit the quality of 205 language-specific corpora released with five major public datasets (CCAligned, ParaCrawl, WikiMatrix, OSCAR, mC4). Lower-resource corpora have systematic issues: At least 15 corpora have no usable text, and a significant fraction contains less than 50% sentences of acceptable quality. In addition, many are mislabeled or use nonstandard\/ambiguous language codes. We demonstrate that these issues are easy to detect even for non-proficient speakers, and supplement the human audit with automatic analyses. Finally, we recommend techniques to evaluate and improve multilingual corpora and discuss potential risks that come with low-quality data releases.","pages":"50--72","doi":"10.1162\/tacl_a_00447","url":"https:\/\/aclanthology.org\/2022.tacl-1.4","publisher":"MIT Press","address":"Cambridge, MA","year":"2022","volume":"10","journal":"Transactions of the Association for Computational Linguistics"}}
{"bib_id":"mishra-sachdeva-2020-need","title":"Do We Need to Create Big Datasets to Learn a Task?","author":"Mishra, Swaroop  and\nSachdeva, Bhavdeep Singh","meta_info":{"abstract":"Deep Learning research has been largely accelerated by the development of huge datasets such as Imagenet. The general trend has been to create big datasets to make a deep neural network learn. A huge amount of resources is being spent in creating these big datasets, developing models, training them, and iterating this process to dominate leaderboards. We argue that the trend of creating bigger datasets needs to be revised by better leveraging the power of pre-trained language models. Since the language models have already been pre-trained with huge amount of data and have basic linguistic knowledge, there is no need to create big datasets to learn a task. Instead, we need to create a dataset that is sufficient for the model to learn various task-specific terminologies, such as `Entailment′, `Neutral′, and `Contradiction′ for NLI. As evidence, we show that RoBERTA is able to achieve near-equal performance on 2% data of SNLI. We also observe competitive zero-shot generalization on several OOD datasets. In this paper, we propose a baseline algorithm to find the optimal dataset for learning a task.","pages":"169--173","doi":"10.18653\/v1\/2020.sustainlp-1.23","url":"https:\/\/aclanthology.org\/2020.sustainlp-1.23","publisher":"Association for Computational Linguistics","address":"Online","year":"2020","month":"November","booktitle":"Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing"}}
{"bib_id":"press-etal-2021-shortformer","title":"Shortformer: Better Language Modeling using Shorter Inputs","author":"Press, Ofir  and\nSmith, Noah A.  and\nLewis, Mike","meta_info":{"abstract":"Increasing the input length has been a driver of progress in language modeling with transformers. We identify conditions where shorter inputs are not harmful, and achieve perplexity and efficiency improvements through two new methods that decrease input length. First, we show that initially training a model on short subsequences before moving on to longer ones both reduces overall training time and, surprisingly, substantially improves perplexity. Second, we show how to improve the efficiency of recurrence methods in transformers, which let models condition on previously processed tokens when generating sequences that exceed the maximal length the transformer can handle at once. Existing methods require computationally expensive relative position embeddings; we introduce a simple alternative of adding absolute position embeddings to queries and keys instead of to word embeddings, which efficiently produces superior results. We show that these recurrent models also benefit from short input lengths. Combining these techniques speeds up training by a factor of 1.65, reduces memory usage, and substantially improves perplexity on WikiText-103, without adding any parameters.","pages":"5493--5505","doi":"10.18653\/v1\/2021.acl-long.427","url":"https:\/\/aclanthology.org\/2021.acl-long.427","publisher":"Association for Computational Linguistics","address":"Online","year":"2021","month":"August","booktitle":"Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)"}}
{"bib_id":"wettig2022should","title":"Should You Mask 15% in Masked Language Modeling?","author":"Wettig, Alexander and Gao, Tianyu and Zhong, Zexuan and Chen, Danqi","meta_info":{"year":"2022","url":"https:\/\/arxiv.org\/abs\/2202.08005v1","journal":"arXiv preprint arXiv:2202.08005v1"}}
{"bib_id":"wan-etal-2020-self","title":"Self-Paced Learning for Neural Machine Translation","author":"Wan, Yu  and\nYang, Baosong  and\nWong, Derek F.  and\nZhou, Yikai  and\nChao, Lidia S.  and\nZhang, Haibo  and\nChen, Boxing","meta_info":{"abstract":"Recent studies have proven that the training of neural machine translation (NMT) can be facilitated by mimicking the learning process of humans. Nevertheless, achievements of such kind of curriculum learning rely on the quality of artificial schedule drawn up with the handcrafted features, e.g. sentence length or word rarity. We ameliorate this procedure with a more flexible manner by proposing self-paced learning, where NMT model is allowed to 1) automatically quantify the learning confidence over training examples; and 2) flexibly govern its learning via regulating the loss in each iteration step. Experimental results over multiple translation tasks demonstrate that the proposed model yields better performance than strong baselines and those models trained with human-designed curricula on both translation quality and convergence speed.","pages":"1074--1080","doi":"10.18653\/v1\/2020.emnlp-main.80","url":"https:\/\/aclanthology.org\/2020.emnlp-main.80","publisher":"Association for Computational Linguistics","address":"Online","year":"2020","month":"November","booktitle":"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)"}}
{"bib_id":"agrawal-etal-2021-role","title":"On the Role of Corpus Ordering in Language Modeling","author":"Agrawal, Ameeta  and\nSingh, Suresh  and\nSchneider, Lauren  and\nSamuels, Michael","meta_info":{"abstract":"Language models pretrained on vast corpora of unstructured text using self-supervised learning framework are used in numerous natural language understanding and generation tasks. Many studies show that language acquisition in humans follows a rather structured simple-to-complex pattern and guided by this intuition, curriculum learning, which enables training of computational models in a meaningful order, such as processing easy samples before hard ones, has been shown to potentially reduce training time. The question remains whether curriculum learning can benefit pretraining of language models. In this work, we perform comprehensive experiments involving multiple curricula strategies varying the criteria for complexity and the training schedules. Empirical results of training transformer language models on English corpus and evaluating it intrinsically as well as after fine-tuning across eight tasks from the GLUE benchmark, show consistent improvement gains over conventional vanilla training. Interestingly, in our experiments, when evaluated on one epoch, the best model following a document-level hard-to-easy curriculum, outperforms the vanilla model by 1.7 points (average GLUE score) and it takes the vanilla model twice as many training steps to reach comparable performance.","pages":"142--154","doi":"10.18653\/v1\/2021.sustainlp-1.15","url":"https:\/\/aclanthology.org\/2021.sustainlp-1.15","publisher":"Association for Computational Linguistics","address":"Virtual","year":"2021","month":"November","booktitle":"Proceedings of the Second Workshop on Simple and Efficient Natural Language Processing"}}
{"bib_id":"zhang2022opt","title":"OPT: Open Pre-trained Transformer Language Models","author":"Susan Zhang and Stephen Roller and Naman Goyal and Mikel Artetxe and Moya Chen and Shuohui Chen and Christopher Dewan and Mona Diab and Xian Li and Xi Victoria Lin and Todor Mihaylov and Myle Ott and Sam Shleifer and Kurt Shuster and Daniel Simig and Punit Singh Koura and Anjali Sridhar and Tianlu Wang and Luke Zettlemoyer","meta_info":{"year":"2022","url":"https:\/\/arxiv.org\/abs\/2205.01068v4","journal":"arXiv preprint arXiv:2205.01068v4"}}
{"bib_id":"bowman-etal-2015-large","title":"A large annotated corpus for learning natural language inference","author":"Bowman, Samuel R.  and\nAngeli, Gabor  and\nPotts, Christopher  and\nManning, Christopher D.","meta_info":{"pages":"632--642","doi":"10.18653\/v1\/D15-1075","url":"https:\/\/aclanthology.org\/D15-1075","publisher":"Association for Computational Linguistics","address":"Lisbon, Portugal","year":"2015","month":"September","booktitle":"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing"}}
{"bib_id":"gissin2019discriminative","title":"Discriminative Active Learning","author":"Gissin, Daniel and Shalev-Shwartz, Shai","meta_info":{"year":"2019","url":"https:\/\/arxiv.org\/abs\/1907.06347v1","journal":"arXiv preprint arXiv:1907.06347v1"}}
{"bib_id":"Bengio2009","title":"Curriculum Learning","author":"Bengio, Yoshua and Louradour, Jérôme and Collobert, Ronan and Weston, Jason","meta_info":{"url":"https:\/\/dl.acm.org\/doi\/pdf\/10.1145\/1553374.1553380","numpages":"8","pages":"41--48","year":"2009","booktitle":"Proceedings of the 26th Annual International Conference on Machine Learning"}}
{"bib_id":"tay-etal-2019-simple","title":"Simple and Effective Curriculum Pointer-Generator Networks for Reading Comprehension over Long Narratives","author":"Tay, Yi  and\nWang, Shuohang  and\nLuu, Anh Tuan  and\nFu, Jie  and\nPhan, Minh C.  and\nYuan, Xingdi  and\nRao, Jinfeng  and\nHui, Siu Cheung  and\nZhang, Aston","meta_info":{"abstract":"This paper tackles the problem of reading comprehension over long narratives where documents easily span over thousands of tokens. We propose a curriculum learning (CL) based Pointer-Generator framework for reading\/sampling over large documents, enabling diverse training of the neural model based on the notion of alternating contextual difficulty. This can be interpreted as a form of domain randomization and\/or generative pretraining during training. To this end, the usage of the Pointer-Generator softens the requirement of having the answer within the context, enabling us to construct diverse training samples for learning. Additionally, we propose a new Introspective Alignment Layer (IAL), which reasons over decomposed alignments using block-based self-attention. We evaluate our proposed method on the NarrativeQA reading comprehension benchmark, achieving state-of-the-art performance, improving existing baselines by 51% relative improvement on BLEU-4 and 17% relative improvement on Rouge-L. Extensive ablations confirm the effectiveness of our proposed IAL and CL components.","pages":"4922--4931","doi":"10.18653\/v1\/P19-1486","url":"https:\/\/aclanthology.org\/P19-1486","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"July","booktitle":"Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"}}
{"bib_id":"xu-etal-2020-curriculum","title":"Curriculum Learning for Natural Language Understanding","author":"Xu, Benfeng  and\nZhang, Licheng  and\nMao, Zhendong  and\nWang, Quan  and\nXie, Hongtao  and\nZhang, Yongdong","meta_info":{"pages":"6095--6104","doi":"10.18653\/v1\/2020.acl-main.542","url":"https:\/\/aclanthology.org\/2020.acl-main.542","publisher":"Association for Computational Linguistics","address":"Online","year":"2020","month":"July","booktitle":"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"}}
{"bib_id":"zhao2020reinforced","title":"Reinforced Curriculum Learning on Pre-Trained Neural Machine Translation Models","author":"Zhao, Mingjun and Wu, Haijiang and Niu, Di and Wang, Xiaoli","meta_info":{"url":"https:\/\/ojs.aaai.org\/index.php\/AAAI\/article\/view\/6513","year":"2020","pages":"9652--9659","volume":"34","booktitle":"Proceedings of the AAAI Conference on Artificial Intelligence"}}
{"bib_id":"kumar2010self","title":"Self-Paced Learning for Latent Variable Models","author":"Kumar, M. and Packer, Benjamin and Koller, Daphne","meta_info":{"year":"2010","volume":"23","url":"https:\/\/proceedings.neurips.cc\/paper\/2010\/file\/e57c6b956a6521b28495f2886ca0977a-Paper.pdf","publisher":"Curran Associates, Inc.","pages":"","editor":"J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta","booktitle":"Advances in Neural Information Processing Systems"}}
{"bib_id":"zhu-etal-2021-combining-curriculum-learning","title":"Combining Curriculum Learning and Knowledge Distillation for Dialogue Generation","author":"Zhu, Qingqing  and\nChen, Xiuying  and\nWu, Pengfei  and\nLiu, JunFei  and\nZhao, Dongyan","meta_info":{"abstract":"Curriculum learning, a machine training strategy that feeds training instances to the model from easy to hard, has been proven to facilitate the dialogue generation task. Meanwhile, knowledge distillation, a knowledge transformation methodology among teachers and students networks can yield significant performance boost for student models. Hence, in this paper, we introduce a combination of curriculum learning and knowledge distillation for efficient dialogue generation models, where curriculum learning can help knowledge distillation from data and model aspects. To start with, from the data aspect, we cluster the training cases according to their complexity, which is calculated by various types of features such as sentence length and coherence between dialog pairs. Furthermore, we employ an adversarial training strategy to identify the complexity of cases from model level. The intuition is that, if a discriminator can tell the generated response is from the teacher or the student, then the case is difficult that the student model has not adapted to yet. Finally, we use self-paced learning, which is an extension to curriculum learning to assign weights for distillation. In conclusion, we arrange a hierarchical curriculum based on the above two aspects for the student model under the guidance from the teacher model. Experimental results demonstrate that our methods achieve improvements compared with competitive baselines.","pages":"1284--1295","doi":"10.18653\/v1\/2021.findings-emnlp.111","url":"https:\/\/aclanthology.org\/2021.findings-emnlp.111","publisher":"Association for Computational Linguistics","address":"Punta Cana, Dominican Republic","year":"2021","month":"November","booktitle":"Findings of the Association for Computational Linguistics: EMNLP 2021"}}
{"bib_id":"lewis1994sequential","title":"A Sequential Algorithm for Training Text Classifiers","author":"Lewis, David D.\nand Gale, William A.","meta_info":{"url":"https:\/\/link.springer.com\/chapter\/10.1007\/978-1-4471-2099-5_1","isbn":"978-1-4471-2099-5","pages":"3--12","address":"London","publisher":"Springer London","year":"1994","booktitle":"SIGIR '94","editor":"Croft, Bruce W.\nand van Rijsbergen, C. J."}}
{"bib_id":"tang-etal-2002-active","title":"Active Learning for Statistical Natural Language Parsing","author":"Tang, Min  and\nLuo, Xiaoqiang  and\nRoukos, Salim","meta_info":{"pages":"120--127","doi":"10.3115\/1073083.1073105","url":"https:\/\/aclanthology.org\/P02-1016","publisher":"Association for Computational Linguistics","address":"Philadelphia, Pennsylvania, USA","year":"2002","month":"July","booktitle":"Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics"}}
{"bib_id":"zhang-etal-2019-curriculum","title":"Curriculum Learning for Domain Adaptation in Neural Machine Translation","author":"Zhang, Xuan  and\nShapiro, Pamela  and\nKumar, Gaurav  and\nMcNamee, Paul  and\nCarpuat, Marine  and\nDuh, Kevin","meta_info":{"abstract":"We introduce a curriculum learning approach to adapt generic neural machine translation models to a specific domain. Samples are grouped by their similarities to the domain of interest and each group is fed to the training algorithm with a particular schedule. This approach is simple to implement on top of any neural framework or architecture, and consistently outperforms both unadapted and adapted baselines in experiments with two distinct domains and two language pairs.","pages":"1903--1915","doi":"10.18653\/v1\/N19-1189","url":"https:\/\/aclanthology.org\/N19-1189","publisher":"Association for Computational Linguistics","address":"Minneapolis, Minnesota","year":"2019","month":"June","booktitle":"Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)"}}
{"bib_id":"kuzmin2022fp","title":"FP8 Quantization: The Power of the Exponent","author":"Andrey Kuzmin and Mart Van Baalen and Yuwei Ren and Markus Nagel and Jorn Peters and Tijmen Blankevoort","meta_info":{"url":"https:\/\/openreview.net\/forum?id=H3Gv7XEGzYV","year":"2022","editor":"Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho","booktitle":"Advances in Neural Information Processing Systems"}}
{"bib_id":"noune20228","title":"8-bit Numerical Formats for Deep Neural Networks","author":"Noune, Badreddine and Jones, Philip and Justus, Daniel and Masters, Dominic and Luschi, Carlo","meta_info":{"url":"https:\/\/arxiv.org\/abs\/2206.02915v1","year":"2022","journal":"arXiv preprint arXiv:2206.02915v1"}}
{"bib_id":"han2015learning","title":"Learning both Weights and Connections for Efficient Neural Networks","author":"Han, Song and Pool, Jeff and Tran, John and Dally, William","meta_info":{"url":"https:\/\/papers.nips.cc\/paper\/2015\/file\/ae0eb3eed39d2bcef4622b2499a05fe6-Paper.pdf","year":"2015","volume":"28","journal":"Advances in neural information processing systems"}}
{"bib_id":"ICML-2019-MostafaW","title":"Parameter Efficient Training of Deep Convolutional Neural Networks by Dynamic Sparse Reparameterization","author":"Hesham Mostafa and Xin Wang","meta_info":{"url":"http:\/\/proceedings.mlr.press\/v97\/mostafa19a\/mostafa19a.pdf","year":"2019","publisher":"PMLR","pages":"4646--4655","ee":"http:\/\/proceedings.mlr.press\/v97\/mostafa19a.html","booktitle":"Proceedings of the 36th International Conference on Machine Learning"}}
{"bib_id":"zhou-etal-2020-uncertainty","title":"Uncertainty-Aware Curriculum Learning for Neural Machine Translation","author":"Zhou, Yikai  and\nYang, Baosong  and\nWong, Derek F.  and\nWan, Yu  and\nChao, Lidia S.","meta_info":{"abstract":"Neural machine translation (NMT) has proven to be facilitated by curriculum learning which presents examples in an easy-to-hard order at different training stages. The keys lie in the assessment of data difficulty and model competence. We propose uncertainty-aware curriculum learning, which is motivated by the intuition that: 1) the higher the uncertainty in a translation pair, the more complex and rarer the information it contains; and 2) the end of the decline in model uncertainty indicates the completeness of current training stage. Specifically, we serve cross-entropy of an example as its data difficulty and exploit the variance of distributions over the weights of the network to present the model uncertainty. Extensive experiments on various translation tasks reveal that our approach outperforms the strong baseline and related methods on both translation quality and convergence speed. Quantitative analyses reveal that the proposed strategy offers NMT the ability to automatically govern its learning schedule.","pages":"6934--6944","doi":"10.18653\/v1\/2020.acl-main.620","url":"https:\/\/aclanthology.org\/2020.acl-main.620","publisher":"Association for Computational Linguistics","address":"Online","year":"2020","month":"July","booktitle":"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"}}
{"bib_id":"he2019rethinking","title":"Rethinking ImageNet pre-training","author":"He, Kaiming and Girshick, Ross and Dollár, Piotr","meta_info":{"url":"https:\/\/openaccess.thecvf.com\/content_ICCV_2019\/html\/He_Rethinking_ImageNet_Pre-Training_ICCV_2019_paper.html","year":"2019","booktitle":"Proceedings of the IEEE\/CVF International Conference on Computer Vision"}}
{"bib_id":"neyshabur2020being","title":"What is being transferred in transfer learning? ","author":"Neyshabur, Behnam and Sedghi, Hanie and Zhang, Chiyuan","meta_info":{"year":"2020","volume":"33","url":"https:\/\/proceedings.neurips.cc\/paper\/2020\/file\/0607f4c705595b911a4f3e7a127b44e0-Paper.pdf","publisher":"Curran Associates, Inc.","pages":"512--523","editor":"H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin","booktitle":"Advances in Neural Information Processing Systems"}}
{"bib_id":"zaken2021bitfit","title":"BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models","author":"Ben Zaken, Elad  and\nGoldberg, Yoav  and\nRavfogel, Shauli","meta_info":{"pages":"1--9","doi":"10.18653\/v1\/2022.acl-short.1","url":"https:\/\/aclanthology.org\/2022.acl-short.1","publisher":"Association for Computational Linguistics","address":"Dublin, Ireland","year":"2022","month":"May","booktitle":"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"}}
{"bib_id":"houlsby2019parameter","title":"Parameter-Efficient Transfer Learning for NLP","author":"Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain","meta_info":{"url":"http:\/\/proceedings.mlr.press\/v97\/houlsby19a\/houlsby19a.pdf","year":"2019","booktitle":"International Conference on Machine Learning"}}
{"bib_id":"bapna2019simple","title":"Simple, Scalable Adaptation for Neural Machine Translation","author":"Bapna, Ankur  and\nFirat, Orhan","meta_info":{"pages":"1538--1548","doi":"10.18653\/v1\/D19-1165","url":"https:\/\/aclanthology.org\/D19-1165","publisher":"Association for Computational Linguistics","address":"Hong Kong, China","year":"2019","month":"November","booktitle":"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)"}}
{"bib_id":"rebuffi2017learning","title":"Learning multiple visual domains with residual adapters","author":"Rebuffi, Sylvestre-Alvise and Bilen, Hakan and Vedaldi, Andrea","meta_info":{"year":"2017","volume":"30","url":"https:\/\/proceedings.neurips.cc\/paper\/2017\/file\/e7b24b112a44fdd9ee93bdf998c6ca0e-Paper.pdf","publisher":"Curran Associates, Inc.","pages":"","editor":"I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett","booktitle":"Advances in Neural Information Processing Systems"}}
{"bib_id":"mahabadi2021compacter","title":"Compacter:\nEfficient Low-Rank Hypercomplex Adapter Layers","author":"Karimi Mahabadi, Rabeeh and Henderson, James and Ruder, Sebastian","meta_info":{"url":"https:\/\/openreview.net\/pdf?id=bqGK5PyI6-N","year":"2021","volume":"34","booktitle":"Advances in Neural Information Processing Systems"}}
{"bib_id":"karimi-mahabadi-etal-2022-prompt","title":"Prompt-free and Efficient Few-shot Learning with Language Models","author":"Karimi Mahabadi, Rabeeh  and\nZettlemoyer, Luke  and\nHenderson, James  and\nMathias, Lambert  and\nSaeidi, Marzieh  and\nStoyanov, Veselin  and\nYazdani, Majid","meta_info":{"abstract":"Current methods for few-shot fine-tuning of pretrained masked language models (PLMs) require carefully engineered prompts and verbalizers for each new task to convert examples into a cloze-format that the PLM can score. In this work, we propose Perfect, a simple and efficient method for few-shot fine-tuning of PLMs without relying on any such handcrafting, which is highly effective given as few as 32 data points. Perfect makes two key design choices: First, we show that manually engineered task prompts can be replaced with task-specific adapters that enable sample-efficient fine-tuning and reduce memory and storage costs by roughly factors of 5 and 100, respectively. Second, instead of using handcrafted verbalizers, we learn new multi-token label embeddings during fine-tuning, which are not tied to the model vocabulary and which allow us to avoid complex auto-regressive decoding. These embeddings are not only learnable from limited data but also enable nearly 100x faster training and inference. Experiments on a wide range of few shot NLP tasks demonstrate that Perfect, while being simple and efficient, also outperforms existing state-of-the-art few-shot learning methods. Our code is publicly available at https:\/\/github.com\/rabeehk\/perfect.","pages":"3638--3652","doi":"10.18653\/v1\/2022.acl-long.254","url":"https:\/\/aclanthology.org\/2022.acl-long.254","publisher":"Association for Computational Linguistics","address":"Dublin, Ireland","year":"2022","month":"May","booktitle":"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"}}
{"bib_id":"hu2022lora","title":"LoRA: Low-Rank Adaptation of Large Language Models","author":"Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen","meta_info":{"year":"2022","url":"https:\/\/openreview.net\/forum?id=nZeVKeeFYf9","booktitle":"International Conference on Learning Representations"}}
{"bib_id":"sung2021training","title":"Training Neural Networks with Fixed Sparse Masks","author":"Sung, Yi-Lin and Nair, Varun and Raffel, Colin A","meta_info":{"year":"2021","volume":"34","url":"https:\/\/proceedings.neurips.cc\/paper\/2021\/file\/cb2653f548f8709598e8b5156738cc51-Paper.pdf","publisher":"Curran Associates, Inc.","pages":"24193--24205","editor":"M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan","booktitle":"Advances in Neural Information Processing Systems"}}
{"bib_id":"guo2021parameter","title":"Parameter-Efficient Transfer Learning with Diff Pruning","author":"Guo, Demi  and\nRush, Alexander  and\nKim, Yoon","meta_info":{"pages":"4884--4896","doi":"10.18653\/v1\/2021.acl-long.378","url":"https:\/\/aclanthology.org\/2021.acl-long.378","publisher":"Association for Computational Linguistics","address":"Online","year":"2021","month":"August","booktitle":"Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)"}}
{"bib_id":"lester2021power","title":"The Power of Scale for Parameter-Efficient Prompt Tuning","author":"Lester, Brian  and\nAl-Rfou, Rami  and\nConstant, Noah","meta_info":{"pages":"3045--3059","doi":"10.18653\/v1\/2021.emnlp-main.243","url":"https:\/\/aclanthology.org\/2021.emnlp-main.243","publisher":"Association for Computational Linguistics","address":"Online and Punta Cana, Dominican Republic","year":"2021","month":"November","booktitle":"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing"}}
{"bib_id":"liu2022few","title":"Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning","author":"Haokun Liu and Derek Tam and Muqeeth Mohammed and Jay Mohta and Tenghao Huang and Mohit Bansal and Colin Raffel","meta_info":{"url":"https:\/\/openreview.net\/forum?id=rBCvMG-JsPd","year":"2022","editor":"Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho","booktitle":"Advances in Neural Information Processing Systems"}}
{"bib_id":"aghajanyan2021intrinsic","title":"Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning","author":"Aghajanyan, Armen  and\nGupta, Sonal  and\nZettlemoyer, Luke","meta_info":{"pages":"7319--7328","doi":"10.18653\/v1\/2021.acl-long.568","url":"https:\/\/aclanthology.org\/2021.acl-long.568","publisher":"Association for Computational Linguistics","address":"Online","year":"2021","month":"August","booktitle":"Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)"}}
{"bib_id":"li2018measuring","title":"Measuring the Intrinsic Dimension of Objective Landscapes","author":"Chunyuan Li and Heerad Farkhoor and Rosanne Liu and Jason Yosinski","meta_info":{"url":"https:\/\/openreview.net\/forum?id=ryup8-WCW","year":"2018","booktitle":"International Conference on Learning Representations"}}
{"bib_id":"ruder2017overview","title":"An Overview of Multi-Task Learning in Deep Neural Networks","author":"Ruder, Sebastian","meta_info":{"year":"2017","url":"https:\/\/arxiv.org\/abs\/1706.05098v1","journal":"arXiv preprint arXiv:1706.05098v1"}}
{"bib_id":"caruana1997multitask","title":"Multitask Learning","author":"Caruana, Rich","meta_info":{"url":"https:\/\/doi.org\/10.1023\/A:1007379606734","doi":"10.1023\/A:1007379606734","issn":"1573-0565","pages":"41-75","number":"1","volume":"28","day":"01","month":"Jul","year":"1997","journal":"Machine Learning"}}
{"bib_id":"blalock2020state","title":"What is the State of Neural Network Pruning?","author":"Blalock, Davis and Gonzalez Ortiz, Jose Javier and Frankle, Jonathan and Guttag, John","meta_info":{"year":"2020","url":"https:\/\/proceedings.mlsys.org\/paper\/2020\/file\/d2ddea18f00665ce8623e36bd4e3c7c5-Paper.pdf","pages":"129--146","volume":"2","journal":"Proceedings of machine learning and systems"}}
{"bib_id":"kim-rush-2016-sequence","title":"Sequence-Level Knowledge Distillation","author":"Kim, Yoon  and\nRush, Alexander M.","meta_info":{"pages":"1317--1327","doi":"10.18653\/v1\/D16-1139","url":"https:\/\/aclanthology.org\/D16-1139","publisher":"Association for Computational Linguistics","address":"Austin, Texas","year":"2016","month":"November","booktitle":"Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing"}}
{"bib_id":"lacoste2019quantifying","title":"Quantifying the Carbon Emissions of Machine Learning","author":"Luccioni, Sasha and Schmidt, Victor and Lacoste, Alexandre and Dandres, Thomas","meta_info":{"year":"2019","url":"https:\/\/www.climatechange.ai\/papers\/neurips2019\/22","booktitle":"NeurIPS 2019 Workshop on Tackling Climate Change with Machine Learning"}}
{"bib_id":"henderson2020towards","title":"Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning","author":"Henderson, Peter and Hu, Jieru and Romoff, Joshua and Brunskill, Emma and Jurafsky, Dan and Pineau, Joelle","meta_info":{"url":"https:\/\/jmlr.org\/papers\/volume21\/20-312\/20-312.pdf","year":"2020","pages":"1--43","number":"248","volume":"21","journal":"Journal of Machine Learning Research"}}
{"bib_id":"hershcovich2022towards","title":"Towards Climate Awareness in NLP Research","author":"Hershcovich, Daniel  and\nWebersinke, Nicolas  and\nKraus, Mathias  and\nBingler, Julia  and\nLeippold, Markus","meta_info":{"pages":"2480--2494","url":"https:\/\/aclanthology.org\/2022.emnlp-main.159","publisher":"Association for Computational Linguistics","address":"Abu Dhabi, United Arab Emirates","year":"2022","month":"December","booktitle":"Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"}}
{"bib_id":"anthony2020carbontracker","title":"CarbonTracker: Tracking and predicting the carbon footprint of training deep learning models","author":"Anthony, Lasse F Wolff and Kanding, Benjamin and Selvan, Raghavendra","meta_info":{"url":"https:\/\/arxiv.org\/abs\/2007.03051","year":"2020","booktitle":"Proceedings of the workshop on Challenges in Deploying and monitoring Machine Learning Systems, ICML"}}
{"bib_id":"Dodge:2020","title":"Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping","author":"Dodge, Jesse and Ilharco, Gabriel and Schwartz, Roy and Farhadi, Ali and Hajishirzi, Hannaneh and Smith, Noah","meta_info":{"url":"https:\/\/arxiv.org\/abs\/2002.06305v1","year":"2020","journal":"arXiv preprint arXiv:2002.06305v1"}}
{"bib_id":"schwartz-etal-2020-right","title":"The Right Tool for the Job: Matching Model and Instance Complexities","author":"Schwartz, Roy  and\nStanovsky, Gabriel  and\nSwayamdipta, Swabha  and\nDodge, Jesse  and\nSmith, Noah A.","meta_info":{"abstract":"As NLP models become larger, executing a trained model requires significant computational resources incurring monetary and environmental costs. To better respect a given inference budget, we propose a modification to contextual representation fine-tuning which, during inference, allows for an early (and fast) ``exit″ from neural network calculations for simple instances, and late (and accurate) exit for hard instances. To achieve this, we add classifiers to different layers of BERT and use their calibrated confidence scores to make early exit decisions. We test our proposed modification on five different datasets in two tasks: three text classification datasets and two natural language inference benchmarks. Our method presents a favorable speed\/accuracy tradeoff in almost all cases, producing models which are up to five times faster than the state of the art, while preserving their accuracy. Our method also requires almost no additional training resources (in either time or parameters) compared to the baseline BERT model. Finally, our method alleviates the need for costly retraining of multiple models at different levels of efficiency; we allow users to control the inference speed\/accuracy tradeoff using a single trained model, by setting a single variable at inference time. We publicly release our code.","pages":"6640--6651","doi":"10.18653\/v1\/2020.acl-main.593","url":"https:\/\/aclanthology.org\/2020.acl-main.593","publisher":"Association for Computational Linguistics","address":"Online","year":"2020","month":"July","booktitle":"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"}}
{"bib_id":"xin-etal-2020-deebert","title":"DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference","author":"Xin, Ji  and\nTang, Raphael  and\nLee, Jaejun  and\nYu, Yaoliang  and\nLin, Jimmy","meta_info":{"abstract":"Large-scale pre-trained language models such as BERT have brought significant improvements to NLP applications. However, they are also notorious for being slow in inference, which makes them difficult to deploy in real-time applications. We propose a simple but effective method, DeeBERT, to accelerate BERT inference. Our approach allows samples to exit earlier without passing through the entire model. Experiments show that DeeBERT is able to save up to ~40% inference time with minimal degradation in model quality. Further analyses show different behaviors in the BERT transformer layers and also reveal their redundancy. Our work provides new ideas to efficiently apply deep transformer-based models to downstream tasks. Code is available at https:\/\/github.com\/castorini\/DeeBERT.","pages":"2246--2251","doi":"10.18653\/v1\/2020.acl-main.204","url":"https:\/\/aclanthology.org\/2020.acl-main.204","publisher":"Association for Computational Linguistics","address":"Online","year":"2020","month":"July","booktitle":"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"}}
{"bib_id":"Dabre:2020","title":"Balancing Cost and Benefit with Tied-Multi Transformers","author":"Dabre, Raj  and\nRubino, Raphael  and\nFujita, Atsushi","meta_info":{"pages":"24--34","doi":"10.18653\/v1\/2020.ngt-1.3","url":"https:\/\/aclanthology.org\/2020.ngt-1.3","publisher":"Association for Computational Linguistics","address":"Online","year":"2020","month":"July","booktitle":"Proceedings of the Fourth Workshop on Neural Generation and Translation"}}
{"bib_id":"Elbayad:2020","title":"Depth-Adaptive Transformer","author":"Maha Elbayad and Jiatao Gu and Edouard Grave and Michael Auli","meta_info":{"url":"https:\/\/openreview.net\/forum?id=SJg7KhVKPH","year":"2020","booktitle":"International Conference on Learning Representations"}}
{"bib_id":"yang2021tuning","title":"Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer","author":"Yang, Ge and Hu, Edward and Babuschkin, Igor and Sidor, Szymon and Liu, Xiaodong and Farhi, David and Ryder, Nick and Pachocki, Jakub and Chen, Weizhu and Gao, Jianfeng","meta_info":{"year":"2021","volume":"34","url":"https:\/\/proceedings.neurips.cc\/paper\/2021\/file\/8df7c2e3c3c3be098ef7b382bd2c37ba-Paper.pdf","publisher":"Curran Associates, Inc.","pages":"17084--17097","editor":"M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan","booktitle":"Advances in Neural Information Processing Systems"}}
{"bib_id":"wei2021meta","title":"Meta-learning Hyperparameter Performance Prediction with Neural Processes","author":"Wei, Ying and Zhao, Peilin and Huang, Junzhou","meta_info":{"url":"https:\/\/proceedings.mlr.press\/v139\/wei21c.html","pdf":"http:\/\/proceedings.mlr.press\/v139\/wei21c\/wei21c.pdf","publisher":"PMLR","month":"18--24 Jul","series":"Proceedings of Machine Learning Research","volume":"139","editor":"Meila, Marina and Zhang, Tong","year":"2021","pages":"11058--11067","booktitle":"Proceedings of the 38th International Conference on Machine Learning"}}
{"bib_id":"liu-wang-2021-empirical","title":"An Empirical Study on Hyperparameter Optimization for Fine-Tuning Pre-trained Language Models","author":"Liu, Xueqing  and\nWang, Chi","meta_info":{"abstract":"The performance of fine-tuning pre-trained language models largely depends on the hyperparameter configuration. In this paper, we investigate the performance of modern hyperparameter optimization methods (HPO) on fine-tuning pre-trained language models. First, we study and report three HPO algorithms′ performances on fine-tuning two state-of-the-art language models on the GLUE dataset. We find that using the same time budget, HPO often fails to outperform grid search due to two reasons: insufficient time budget and overfitting. We propose two general strategies and an experimental procedure to systematically troubleshoot HPO′s failure cases. By applying the procedure, we observe that HPO can succeed with more appropriate settings in the search space and time budget; however, in certain cases overfitting remains. Finally, we make suggestions for future work. Our implementation can be found in r̆lhttps:\/\/github.com\/microsoft\/FLAML\/tree\/main\/flaml\/nlp\/","pages":"2286--2300","doi":"10.18653\/v1\/2021.acl-long.178","url":"https:\/\/aclanthology.org\/2021.acl-long.178","publisher":"Association for Computational Linguistics","address":"Online","year":"2021","month":"August","booktitle":"Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)"}}
{"bib_id":"li2020system","title":"A System for Massively Parallel Hyperparameter Tuning","author":"Liam Li and Kevin Jamieson and Afshin Rostamizadeh and Ekaterina Gonina and Jonathan Ben-tzur and Moritz Hardt and Benjamin Recht and Ameet Talwalkar","meta_info":{"url":"https:\/\/proceedings.mlsys.org\/paper\/2020\/file\/f4b9ec30ad9f68f89b29639786cb62ef-Paper.pdf","booktitle":"Third Conference on Systems and Machine Learning","year":"2020"}}
{"bib_id":"feurer2015efficient","title":"Efficient and Robust Automated Machine Learning","author":"Feurer, Matthias and Klein, Aaron and Eggensperger, Katharina and Springenberg, Jost and Blum, Manuel and Hutter, Frank","meta_info":{"url":"https:\/\/papers.nips.cc\/paper\/2015\/hash\/11d0e6287202fced83f79975ec59a3a6-Abstract.html","year":"2015","volume":"28","journal":"Advances in neural information processing systems"}}
{"bib_id":"jamieson2016non","title":"Non-stochastic Best Arm Identification and Hyperparameter Optimization","author":"Jamieson, Kevin and Talwalkar, Ameet","meta_info":{"url":"https:\/\/proceedings.mlr.press\/v51\/jamieson16.html","organization":"PMLR","year":"2016","pages":"240--248","booktitle":"Artificial intelligence and statistics"}}
{"bib_id":"snoek2012practical","title":"Practical Bayesian Optimization of Machine Learning Algorithms","author":"Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P","meta_info":{"year":"2012","volume":"25","url":"https:\/\/proceedings.neurips.cc\/paper\/2012\/file\/05311655a15b75fab86956663e1819cd-Paper.pdf","publisher":"Curran Associates, Inc.","pages":"","editor":"F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger","booktitle":"Advances in Neural Information Processing Systems"}}
{"bib_id":"lindauer2022smac3","title":"SMAC3: A Versatile Bayesian Optimization Package for Hyperparameter Optimization","author":"Lindauer, Marius and Eggensperger, Katharina and Feurer, Matthias and Biedenkapp, André and Deng, Difan and Benjamins, Carolin and Ruhkopf, Tim and Sass, René and Hutter, Frank","meta_info":{"url":"https:\/\/www.jmlr.org\/papers\/volume23\/21-0888\/21-0888.pdf","year":"2022","pages":"54--1","volume":"23","journal":"Journal of Machine Learning Research"}}
{"bib_id":"Fan:2020","title":"Reducing Transformer Depth on Demand with Structured Dropout","author":"Angela Fan and Edouard Grave and Armand Joulin","meta_info":{"url":"https:\/\/openreview.net\/forum?id=SylO2yStDr","year":"2020","booktitle":"International Conference on Learning Representations"}}
{"bib_id":"renduchintala-etal-2021-gender","title":"Gender bias amplification during Speed-Quality optimization in Neural Machine Translation","author":"Renduchintala, Adithya  and\nDiaz, Denise  and\nHeafield, Kenneth  and\nLi, Xian  and\nDiab, Mona","meta_info":{"abstract":"Is bias amplified when neural machine translation (NMT) models are optimized for speed and evaluated on generic test sets using BLEU? We investigate architectures and techniques commonly used to speed up decoding in Transformer-based models, such as greedy search, quantization, average attention networks (AANs) and shallow decoder models and show their effect on gendered noun translation. We construct a new gender bias test set, SimpleGEN, based on gendered noun phrases in which there is a single, unambiguous, correct answer. While we find minimal overall BLEU degradation as we apply speed optimizations, we observe that gendered noun translation performance degrades at a much faster rate.","pages":"99--109","doi":"10.18653\/v1\/2021.acl-short.15","url":"https:\/\/aclanthology.org\/2021.acl-short.15","publisher":"Association for Computational Linguistics","address":"Online","year":"2021","month":"August","booktitle":"Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)"}}
{"bib_id":"Clark:2020","title":"ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators","author":"Clark, Kevin and Luong, Minh-Thang and Le, Quoc V. and Manning, Christopher D.","meta_info":{"url":"https:\/\/openreview.net\/forum?id=r1xMH1BtvB","year":"2020","booktitle":"International Conference on Learning Representations","doi":"10.48550\/ARXIV.2003.10555"}}
{"bib_id":"he2021debertav3","title":"DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing","author":"Pengcheng He and Jianfeng Gao and Weizhu Chen","meta_info":{"url":"https:\/\/openreview.net\/forum?id=sE7-XhLxHA","year":"2023","booktitle":"The Eleventh International Conference on Learning Representations "}}
{"bib_id":"Devlin:2019","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","author":"Devlin, Jacob  and\nChang, Ming-Wei  and\nLee, Kenton  and\nToutanova, Kristina","meta_info":{"pages":"4171--4186","doi":"10.18653\/v1\/N19-1423","url":"https:\/\/aclanthology.org\/N19-1423","publisher":"Association for Computational Linguistics","address":"Minneapolis, Minnesota","year":"2019","month":"June","booktitle":"Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)"}}
{"bib_id":"Bitton:2021","title":"Data Efficient Masked Language Modeling for Vision and Language","author":"Bitton, Yonatan  and\nElhadad, Michael  and\nStanovsky, Gabriel  and\nSchwartz, Roy","meta_info":{"pages":"3013--3028","doi":"10.18653\/v1\/2021.findings-emnlp.259","url":"https:\/\/aclanthology.org\/2021.findings-emnlp.259","publisher":"Association for Computational Linguistics","address":"Punta Cana, Dominican Republic","year":"2021","month":"November","booktitle":"Findings of the Association for Computational Linguistics: EMNLP 2021"}}
{"bib_id":"Lakim2022AHA","title":"A Holistic Assessment of the Carbon Footprint of Noor, a Very Large Arabic Language Model","author":"Lakim, Imad  and\nAlmazrouei, Ebtesam  and\nAbualhaol, Ibrahim  and\nDebbah, Merouane  and\nLaunay, Julien","meta_info":{"note_":"model_size_by_param.png figure","pages":"84--94","doi":"10.18653\/v1\/2022.bigscience-1.8","url":"https:\/\/aclanthology.org\/2022.bigscience-1.8","publisher":"Association for Computational Linguistics","address":"virtual+Dublin","year":"2022","month":"May","booktitle":"Proceedings of BigScience Episode #5 -- Workshop on Challenges & Perspectives in Creating Large Language Models"}}
{"bib_id":"Schwartz:2020","title":"Green AI","author":"Schwartz, Roy and Dodge, Jesse and Smith, Noah A. and Etzioni, Oren","meta_info":{"numpages":"10","pages":"54-63","month":"November","journal":"Communications of the ACM (CACM)","doi":"10.1145\/3381831","url":"https:\/\/doi.org\/10.1145\/3381831","issn":"0001-0782","number":"12","volume":"63","address":"New York, NY, USA","publisher":"Association for Computing Machinery","issue_date":"December 2020","year":"2020"}}
{"bib_id":"Strubell:2019","title":"Energy and Policy Considerations for Deep Learning in NLP","author":"Strubell, Emma  and\nGanesh, Ananya  and\nMcCallum, Andrew","meta_info":{"pages":"3645--3650","doi":"10.18653\/v1\/P19-1355","url":"https:\/\/aclanthology.org\/P19-1355","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"July","booktitle":"Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"}}
{"bib_id":"hsu2005towards","title":"Towards efficient supercomputing: A quest for the right metric","author":"Hsu, C-H and Feng, W-C and Archuleta, Jeremy S","meta_info":{"url":"https:\/\/ieeexplore.ieee.org\/document\/1420148","organization":"IEEE","year":"2005","pages":"8--pp","booktitle":"19th IEEE International Parallel and Distributed Processing Symposium"}}
{"bib_id":"raffel2019exploring","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer","author":"Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu","meta_info":{"url":"http:\/\/jmlr.org\/papers\/v21\/20-074.html","pages":"1--67","number":"140","volume":"21","year":"2020","journal":"Journal of Machine Learning Research"}}
{"bib_id":"he2022towards","title":"Towards a Unified View of Parameter-Efficient Transfer Learning","author":"Junxian He and Chunting Zhou and Xuezhe Ma and Taylor Berg-Kirkpatrick and Graham Neubig","meta_info":{"url":"https:\/\/openreview.net\/forum?id=0RDcd5Axok","year":"2022","booktitle":"International Conference on Learning Representations"}}
{"bib_id":"aghajanyan2021muppet","title":"Muppet: Massive Multi-task Representations with Pre-Finetuning","author":"Aghajanyan, Armen  and\nGupta, Anchit  and\nShrivastava, Akshat  and\nChen, Xilun  and\nZettlemoyer, Luke  and\nGupta, Sonal","meta_info":{"pages":"5799--5811","doi":"10.18653\/v1\/2021.emnlp-main.468","url":"https:\/\/aclanthology.org\/2021.emnlp-main.468","publisher":"Association for Computational Linguistics","address":"Online and Punta Cana, Dominican Republic","year":"2021","month":"November","booktitle":"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing"}}
{"bib_id":"aribandi2021ext5","title":"ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning","author":"Vamsi Aribandi and Yi Tay and Tal Schuster and Jinfeng Rao and Huaixiu Steven Zheng and Sanket Vaibhav Mehta and Honglei Zhuang and Vinh Q. Tran and Dara Bahri and Jianmo Ni and Jai Gupta and Kai Hui and Sebastian Ruder and Donald Metzler","meta_info":{"url":"https:\/\/openreview.net\/forum?id=Vzh1BFUCiIX","year":"2022","booktitle":"International Conference on Learning Representations"}}
{"bib_id":"radford2017learning","title":"Learning to Generate Reviews and Discovering Sentiment","author":"Radford, Alec and Jozefowicz, Rafal and Sutskever, Ilya","meta_info":{"year":"2017","url":"https:\/\/arxiv.org\/abs\/1704.01444v2","journal":"arXiv preprint arXiv:1704.01444v2"}}
{"bib_id":"radford2019language","title":"Language Models are Unsupervised Multitask Learners","author":"Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya","meta_info":{"url":"https:\/\/cdn.openai.com\/better-language-models\/language_models_are_unsupervised_multitask_learners.pdf","year":"2019","pages":"9","number":"8","volume":"1","journal":"OpenAI blog"}}
{"bib_id":"DarasSMYRF2020","title":"SMYRF - Efficient Attention using Asymmetric Clustering","author":"Daras, Giannis and Kitaev, Nikita and Odena, Augustus and Dimakis, Alexandros G","meta_info":{"year":"2020","volume":"33","url":"https:\/\/proceedings.neurips.cc\/paper\/2020\/file\/47d40767c7e9df50249ebfd9c7cfff77-Abstract.html","publisher":"Curran Associates, Inc.","pages":"6476--6489","editor":"H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin","booktitle":"Advances in Neural Information Processing Systems"}}
{"bib_id":"brown2020language","title":"Language Models are Few-Shot Learners","author":"Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario","meta_info":{"year":"2020","volume":"33","url":"https:\/\/proceedings.neurips.cc\/paper\/2020\/file\/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf","publisher":"Curran Associates, Inc.","pages":"1877--1901","editor":"H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin","booktitle":"Advances in Neural Information Processing Systems"}}
{"bib_id":"LeCun:1989","title":"Optimal Brain Damage","author":"LeCun, Yann and Denker, John and Solla, Sara","meta_info":{"year":"1989","volume":"2","url":"https:\/\/proceedings.neurips.cc\/paper\/1989\/file\/6c9882bbac1c7093bd25041881277658-Abstract.html","publisher":"Morgan-Kaufmann","pages":"","editor":"D. Touretzky","booktitle":"Advances in Neural Information Processing Systems"}}
{"bib_id":"sanh2022multitask","title":"Multitask Prompted Training Enables Zero-Shot Task Generalization","author":"Victor Sanh and Albert Webson and Colin Raffel and Stephen Bach and Lintang Sutawika and Zaid Alyafeai and Antoine Chaffin and Arnaud Stiegler and Arun Raja and Manan Dey and M Saiful Bari and Canwen Xu and Urmish Thakker and Shanya Sharma Sharma and Eliza Szczechla and Taewoon Kim and Gunjan Chhablani and Nihal Nayak and Debajyoti Datta and Jonathan Chang and Mike Tian-Jian Jiang and Han Wang and Matteo Manica and Sheng Shen and Zheng Xin Yong and Harshit Pandey and Rachel Bawden and Thomas Wang and Trishala Neeraj and Jos Rozen and Abheesht Sharma and Andrea Santilli and Thibault Fevry and Jason Alan Fries and Ryan Teehan and Teven Le Scao and Stella Biderman and Leo Gao and Thomas Wolf and Alexander M Rush","meta_info":{"year":"2022","url":"https:\/\/openreview.net\/forum?id=9Vrb9D0WI4","booktitle":"International Conference on Learning Representations"}}
{"bib_id":"wei2022finetuned","title":"Finetuned Language Models are Zero-Shot Learners","author":"Jason Wei and Maarten Bosma and Vincent Zhao and Kelvin Guu and Adams Wei Yu and Brian Lester and Nan Du and Andrew M. Dai and Quoc V Le","meta_info":{"url":"https:\/\/openreview.net\/forum?id=gEZrGCozdqR","year":"2022","booktitle":"International Conference on Learning Representations"}}
{"bib_id":"quinn-ballesteros-2018-pieces","title":"Pieces of Eight: 8-bit Neural Machine Translation","author":"Quinn, Jerry  and\nBallesteros, Miguel","meta_info":{"abstract":"Neural machine translation has achieved levels of fluency and adequacy that would have been surprising a short time ago. Output quality is extremely relevant for industry purposes, however it is equally important to produce results in the shortest time possible, mainly for latency-sensitive applications and to control cloud hosting costs. In this paper we show the effectiveness of translating with 8-bit quantization for models that have been trained using 32-bit floating point values. Results show that 8-bit translation makes a non-negligible impact in terms of speed with no degradation in accuracy and adequacy.","pages":"114--120","doi":"10.18653\/v1\/N18-3014","url":"https:\/\/aclanthology.org\/N18-3014","publisher":"Association for Computational Linguistics","address":"New Orleans - Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 3 (Industry Papers)"}}
{"bib_id":"louizos2018learning","title":"Learning Sparse Neural Networks through L$_0$ Regularization","author":"Christos Louizos and Max Welling and Diederik P. Kingma","meta_info":{"url":"https:\/\/openreview.net\/forum?id=H1Y8hhg0b","year":"2018","booktitle":"International Conference on Learning Representations"}}
{"bib_id":"kim-etal-2019-research","title":"From Research to Production and Back: Ludicrously Fast Neural Machine Translation","author":"Kim, Young Jin  and\nJunczys-Dowmunt, Marcin  and\nHassan, Hany  and\nFikri Aji, Alham  and\nHeafield, Kenneth  and\nGrundkiewicz, Roman  and\nBogoychev, Nikolay","meta_info":{"abstract":"This paper describes the submissions of the ``Marian″ team to the WNGT 2019 efficiency shared task. Taking our dominating submissions to the previous edition of the shared task as a starting point, we develop improved teacher-student training via multi-agent dual-learning and noisy backward-forward translation for Transformer-based student models. For efficient CPU-based decoding, we propose pre-packed 8-bit matrix products, improved batched decoding, cache-friendly student architectures with parameter sharing and light-weight RNN-based decoder architectures. GPU-based decoding benefits from the same architecture changes, from pervasive 16-bit inference and concurrent streams. These modifications together with profiler-based C++ code optimization allow us to push the Pareto frontier established during the 2018 edition towards 24x (CPU) and 14x (GPU) faster models at comparable or higher BLEU values. Our fastest CPU model is more than 4x faster than last year′s fastest submission at more than 3 points higher BLEU. Our fastest GPU model at 1.5 seconds translation time is slightly faster than last year′s fastest RNN-based submissions, but outperforms them by more than 4 BLEU and 10 BLEU points respectively.","pages":"280--288","doi":"10.18653\/v1\/D19-5632","url":"https:\/\/aclanthology.org\/D19-5632","publisher":"Association for Computational Linguistics","address":"Hong Kong","year":"2019","month":"November","booktitle":"Proceedings of the 3rd Workshop on Neural Generation and Translation"}}
{"bib_id":"bogoychev-etal-2020-edinburghs","title":"Edinburgh′s Submissions to the 2020 Machine Translation Efficiency Task","author":"Bogoychev, Nikolay  and\nGrundkiewicz, Roman  and\nAji, Alham Fikri  and\nBehnke, Maximiliana  and\nHeafield, Kenneth  and\nKashyap, Sidharth  and\nFarsarakis, Emmanouil-Ioannis  and\nChudyk, Mateusz","meta_info":{"abstract":"We participated in all tracks of the Workshop on Neural Generation and Translation 2020 Efficiency Shared Task: single-core CPU, multi-core CPU, and GPU. At the model level, we use teacher-student training with a variety of student sizes, tie embeddings and sometimes layers, use the Simpler Simple Recurrent Unit, and introduce head pruning. On GPUs, we used 16-bit floating-point tensor cores. On CPUs, we customized 8-bit quantization and multiple processes with affinity for the multi-core setting. To reduce model size, we experimented with 4-bit log quantization but use floats at runtime. In the shared task, most of our submissions were Pareto optimal with respect the trade-off between time and quality.","pages":"218--224","doi":"10.18653\/v1\/2020.ngt-1.26","url":"https:\/\/aclanthology.org\/2020.ngt-1.26","publisher":"Association for Computational Linguistics","address":"Online","year":"2020","month":"July","booktitle":"Proceedings of the Fourth Workshop on Neural Generation and Translation"}}
{"bib_id":"behnke-heafield-2021-pruning","title":"Pruning Neural Machine Translation for Speed Using Group Lasso","author":"Behnke, Maximiliana  and\nHeafield, Kenneth","meta_info":{"abstract":"Unlike most work on pruning neural networks, we make inference faster. Group lasso regularisation enables pruning entire rows, columns or blocks of parameters that result in a smaller dense network. Because the network is still dense, efficient matrix multiply routines are still used and only minimal software changes are required to support variable layer sizes. Moreover, pruning is applied during training so there is no separate pruning step. Experiments on top of English-\\textgreaterGerman models, which already have state-of-the-art speed and size, show that two-thirds of feedforward connections can be removed with 0.2 BLEU loss. With 6 decoder layers, the pruned model is 34% faster; with 2 tied decoder layers, the pruned model is 14% faster. Pruning entire heads and feedforward connections in a 12--1 encoder-decoder architecture gains an additional 51% speed-up. These push the Pareto frontier with respect to the trade-off between time and quality compared to strong baselines. In the WMT 2021 Efficiency Task, our pruned and quantised models are 1.9--2.7x faster at the cost 0.9--1.7 BLEU in comparison to the unoptimised baselines. Across language pairs, we see similar sparsity patterns: an ascending or U-shaped distribution in encoder feedforward and attention layers and an ascending distribution in the decoder.","pages":"1074--1086","url":"https:\/\/aclanthology.org\/2021.wmt-1.116","publisher":"Association for Computational Linguistics","address":"Online","year":"2021","month":"November","booktitle":"Proceedings of the Sixth Conference on Machine Translation"}}
{"bib_id":"liutowards","title":"Towards Efficient NLP: A Standard Evaluation and A Strong Baseline","author":"Liu, Xiangyang  and\nSun, Tianxiang  and\nHe, Junliang  and\nWu, Jiawen  and\nWu, Lingling  and\nZhang, Xinyu  and\nJiang, Hao  and\nCao, Zhao  and\nHuang, Xuanjing  and\nQiu, Xipeng","meta_info":{"pages":"3288--3303","doi":"10.18653\/v1\/2022.naacl-main.240","url":"https:\/\/aclanthology.org\/2022.naacl-main.240","publisher":"Association for Computational Linguistics","address":"Seattle, United States","year":"2022","month":"July","booktitle":"Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"}}
{"bib_id":"zafrir2021prune","title":"Prune Once for All: Sparse Pre-Trained Language Models","author":"Zafrir, Ofir and Larey, Ariel and Boudoukh, Guy and Shen, Haihao and Wasserblat, Moshe","meta_info":{"url":"https:\/\/arxiv.org\/abs\/2111.05754v1","year":"2021","journal":"arXiv preprint arXiv:2111.05754v1"}}
{"bib_id":"de2021hyperparameter","title":"Hyperparameter Power Impact in Transformer Language Model Training","author":"Puvis de Chavannes, Lucas Høyberg  and\nKongsbak, Mads Guldborg Kjeldgaard  and\nRantzau, Timmie  and\nDerczynski, Leon","meta_info":{"pages":"96--118","doi":"10.18653\/v1\/2021.sustainlp-1.12","url":"https:\/\/aclanthology.org\/2021.sustainlp-1.12","publisher":"Association for Computational Linguistics","address":"Virtual","year":"2021","month":"November","booktitle":"Proceedings of the Second Workshop on Simple and Efficient Natural Language Processing"}}
{"bib_id":"derczynski2020power","title":"Power Consumption Variation over Activation Functions","author":"Derczynski, Leon","meta_info":{"year":"2020","url":"https:\/\/arxiv.org\/abs\/2006.07237v1","journal":"arXiv preprint arXiv:2006.07237v1"}}
{"bib_id":"fedus2022review","title":"A Review of Sparse Expert Models in Deep Learning","author":"Fedus, William and Dean, Jeff and Zoph, Barret","meta_info":{"year":"2022","url":"https:\/\/arxiv.org\/abs\/2209.01667v1","journal":"arXiv preprint arXiv:2209.01667v1"}}
{"bib_id":"ethayarajh2022understanding","title":"Understanding Dataset Difficulty with $\\mathcalV$-Usable Information","author":"Ethayarajh, Kawin and Choi, Yejin and Swayamdipta, Swabha","meta_info":{"url":"https:\/\/proceedings.mlr.press\/v162\/ethayarajh22a\/ethayarajh22a.pdf","organization":"PMLR","year":"2022","pages":"5988--6008","booktitle":"International Conference on Machine Learning"}}
{"bib_id":"palm","title":"PaLM: Scaling Language Modeling with Pathways","author":"Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and Schuh, Parker and Shi, Kensen and Tsvyashchenko, Sasha and Maynez, Joshua and Rao, Abhishek and Barnes, Parker and Tay, Yi and Shazeer, Noam and Prabhakaran, Vinodkumar and Reif, Emily and Du, Nan and Hutchinson, Ben and Pope, Reiner and Bradbury, James and Austin, Jacob and Isard, Michael and Gur-Ari, Guy and Yin, Pengcheng and Duke, Toju and Levskaya, Anselm and Ghemawat, Sanjay and Dev, Sunipa and Michalewski, Henryk and Garcia, Xavier and Misra, Vedant and Robinson, Kevin and Fedus, Liam and Zhou, Denny and Ippolito, Daphne and Luan, David and Lim, Hyeontaek and Zoph, Barret and Spiridonov, Alexander and Sepassi, Ryan and Dohan, David and Agrawal, Shivani and Omernick, Mark and Dai, Andrew M. and Pillai, Thanumalayan Sankaranarayana and Pellat, Marie and Lewkowycz, Aitor and Moreira, Erica and Child, Rewon and Polozov, Oleksandr and Lee, Katherine and Zhou, Zongwei and Wang, Xuezhi and Saeta, Brennan and Diaz, Mark and Firat, Orhan and Catasta, Michele and Wei, Jason and Meier-Hellstern, Kathy and Eck, Douglas and Dean, Jeff and Petrov, Slav and Fiedel, Noah","meta_info":{"note":"arXiv:2204.02311v5","year":"2022","url":"https:\/\/arxiv.org\/abs\/2204.02311v5","doi":"10.48550\/ARXIV.2204.02311"}}
{"bib_id":"khandelwal2020nearest","title":"Nearest Neighbor Machine Translation","author":"Urvashi Khandelwal and Angela Fan and Dan Jurafsky and Luke Zettlemoyer and Mike Lewis","meta_info":{"url":"https:\/\/openreview.net\/forum?id=7wCBOfJ8hJM","year":"2021","booktitle":"International Conference on Learning Representations"}}
{"bib_id":"meng2021fast","title":"Fast Nearest Neighbor Machine Translation","author":"Meng, Yuxian  and\nLi, Xiaoya  and\nZheng, Xiayu  and\nWu, Fei  and\nSun, Xiaofei  and\nZhang, Tianwei  and\nLi, Jiwei","meta_info":{"pages":"555--565","doi":"10.18653\/v1\/2022.findings-acl.47","url":"https:\/\/aclanthology.org\/2022.findings-acl.47","publisher":"Association for Computational Linguistics","address":"Dublin, Ireland","year":"2022","month":"May","booktitle":"Findings of the Association for Computational Linguistics: ACL 2022"}}
{"bib_id":"martins2022chunk","title":"Chunk-based Nearest Neighbor Machine Translation","author":"Martins, Pedro Henrique  and\nMarinho, Zita  and\nMartins, André F. T.","meta_info":{"pages":"4228--4245","url":"https:\/\/aclanthology.org\/2022.emnlp-main.284","publisher":"Association for Computational Linguistics","address":"Abu Dhabi, United Arab Emirates","year":"2022","month":"December","booktitle":"Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"}}
{"bib_id":"wang2021faster","title":"Faster Nearest Neighbor Machine Translation","author":"Wang, Shuhe and Li, Jiwei and Meng, Yuxian and Ouyang, Rongbin and Wang, Guoyin and Li, Xiaoya and Zhang, Tianwei and Zong, Shi","meta_info":{"journal":"arXiv preprint arXiv:2112.08152v1","url":"https:\/\/arxiv.org\/abs\/2112.08152v1","year":"2021"}}
{"bib_id":"alon2022neuro","title":"Neuro-Symbolic Language Modeling with Automaton-augmented Retrieval","author":"Alon, Uri and Xu, Frank and He, Junxian and Sengupta, Sudipta and Roth, Dan and Neubig, Graham","meta_info":{"url":"https:\/\/proceedings.mlr.press\/v162\/alon22a.html","pdf":"https:\/\/proceedings.mlr.press\/v162\/alon22a\/alon22a.pdf","publisher":"PMLR","month":"17--23 Jul","series":"Proceedings of Machine Learning Research","volume":"162","editor":"Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan","year":"2022","pages":"468--485","booktitle":"Proceedings of the 39th International Conference on Machine Learning"}}
{"bib_id":"li2022survey","title":"A Survey on Retrieval-Augmented Text Generation","author":"Li, Huayang and Su, Yixuan and Cai, Deng and Wang, Yan and Liu, Lemao","meta_info":{"year":"2022","url":"https:\/\/arxiv.org\/abs\/2202.01110v1","journal":"arXiv preprint arXiv:2202.01110v1"}}
{"bib_id":"treviso-etal-2022-predicting","title":"Predicting Attention Sparsity in Transformers","author":"Treviso, Marcos  and\nGóis, António  and\nFernandes, Patrick  and\nFonseca, Erick  and\nMartins, Andre","meta_info":{"pages":"67--81","doi":"10.18653\/v1\/2022.spnlp-1.7","url":"https:\/\/aclanthology.org\/2022.spnlp-1.7","publisher":"Association for Computational Linguistics","address":"Dublin, Ireland","year":"2022","month":"May","booktitle":"Proceedings of the Sixth Workshop on Structured Prediction for NLP"}}
{"bib_id":"wei_emergent_2022","title":"Emergent Abilities of Large Language Models","author":"Jason Wei and Yi Tay and Rishi Bommasani and Colin Raffel and Barret Zoph and Sebastian Borgeaud and Dani Yogatama and Maarten Bosma and Denny Zhou and Donald Metzler and Ed H. Chi and Tatsunori Hashimoto and Oriol Vinyals and Percy Liang and Jeff Dean and William Fedus","meta_info":{"note":"Survey Certification","url":"https:\/\/openreview.net\/forum?id=yzkSU5zdwD","year":"2022","issn":"2835-8856","journal":"Transactions on Machine Learning Research"}}
{"bib_id":"ren_zero-offload_2021","title":"ZeRO-Offload: Democratizing Billion-Scale Model Training","author":"Jie Ren and Samyam Rajbhandari and Reza Yazdani Aminabadi and Olatunji Ruwase and Shuangyan Yang and Minjia Zhang and Dong Li and Yuxiong He","meta_info":{"publisher":"USENIX Association","url":"https:\/\/www.usenix.org\/conference\/atc21\/presentation\/ren-jie","pages":"551--564","isbn":"978-1-939133-23-6","year":"2021","booktitle":"2021 USENIX Annual Technical Conference (USENIX ATC 21)"}}
{"bib_id":"dettmers2022bit","title":"8-bit Optimizers via Block-wise Quantization","author":"Tim Dettmers and Mike Lewis and Sam Shleifer and Luke Zettlemoyer","meta_info":{"url":"https:\/\/openreview.net\/forum?id=shpkpVXzo3h","year":"2022","booktitle":"International Conference on Learning Representations"}}
{"bib_id":"liu2021pre","title":"Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing","author":"Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham","meta_info":{"year":"2023","number":"9","volume":"55","doi":"https:\/\/doi.org\/10.1145\/3560815","url":"https:\/\/dl.acm.org\/doi\/full\/10.1145\/3560815","journal":"ACM Computing Surveys"}}
{"bib_id":"li-liang-2021-prefix","title":"Prefix-Tuning: Optimizing Continuous Prompts for Generation","author":"Li, Xiang Lisa  and\nLiang, Percy","meta_info":{"pages":"4582--4597","doi":"10.18653\/v1\/2021.acl-long.353","url":"https:\/\/aclanthology.org\/2021.acl-long.353","publisher":"Association for Computational Linguistics","address":"Online","year":"2021","month":"August","booktitle":"Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)"}}
{"bib_id":"tay2020efficient","title":"Efficient Transformers: A Survey","author":"Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald","meta_info":{"keywords":"neural networks, transformers, attention, deep learning","month":"apr","journal":"ACM Comput. Surv.","doi":"10.1145\/3530811","url":"https:\/\/doi.org\/10.1145\/3530811","issn":"0360-0300","address":"New York, NY, USA","publisher":"Association for Computing Machinery","year":"2022"}}
{"bib_id":"petroni-etal-2019-language","title":"Language Models as Knowledge Bases?","author":"Petroni, Fabio  and\nRocktäschel, Tim  and\nRiedel, Sebastian  and\nLewis, Patrick  and\nBakhtin, Anton  and\nWu, Yuxiang  and\nMiller, Alexander","meta_info":{"pages":"2463--2473","doi":"10.18653\/v1\/D19-1250","url":"https:\/\/aclanthology.org\/D19-1250","publisher":"Association for Computational Linguistics","address":"Hong Kong, China","year":"2019","month":"November","booktitle":"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)"}}
{"bib_id":"schick-schutze-2021-just","title":"It′s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners","author":"Schick, Timo  and\nSchütze, Hinrich","meta_info":{"pages":"2339--2352","doi":"10.18653\/v1\/2021.naacl-main.185","url":"https:\/\/aclanthology.org\/2021.naacl-main.185","publisher":"Association for Computational Linguistics","address":"Online","year":"2021","month":"June","booktitle":"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"}}
{"bib_id":"shin-etal-2020-autoprompt","title":"AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts","author":"Shin, Taylor  and\nRazeghi, Yasaman  and\nLogan IV, Robert L.  and\nWallace, Eric  and\nSingh, Sameer","meta_info":{"pages":"4222--4235","doi":"10.18653\/v1\/2020.emnlp-main.346","url":"https:\/\/aclanthology.org\/2020.emnlp-main.346","publisher":"Association for Computational Linguistics","address":"Online","year":"2020","month":"November","booktitle":"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)"}}
{"bib_id":"peters-martins-2021-smoothing","title":"Smoothing and Shrinking the Sparse Seq2Seq Search Space","author":"Peters, Ben  and\nMartins, André F. T.","meta_info":{"pages":"2642--2654","doi":"10.18653\/v1\/2021.naacl-main.210","url":"https:\/\/aclanthology.org\/2021.naacl-main.210","publisher":"Association for Computational Linguistics","address":"Online","year":"2021","month":"June","booktitle":"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"}}
{"bib_id":"rae2022scaling","title":"Scaling Language Models: Methods, Analysis & Insights from Training Gopher","author":"Jack W. Rae and\nSebastian Borgeaud and\nTrevor Cai and\nKatie Millican and\nJordan Hoffmann and\nH. Francis Song and\nJohn Aslanides and\nSarah Henderson and\nRoman Ring and\nSusannah Young and\nEliza Rutherford and\nTom Hennigan and\nJacob Menick and\nAlbin Cassirer and\nRichard Powell and\nGeorge van den Driessche and\nLisa Anne Hendricks and\nMaribeth Rauh and\nPo-Sen Huang and\nAmelia Glaese and\nJohannes Welbl and\nSumanth Dathathri and\nSaffron Huang and\nJonathan Uesato and\nJohn Mellor and\nIrina Higgins and\nAntonia Creswell and\nNat McAleese and\nAmy Wu and\nErich Elsen and\nSiddhant M. Jayakumar and\nElena Buchatskaya and\nDavid Budden and\nEsme Sutherland and\nKaren Simonyan and\nMichela Paganini and\nLaurent Sifre and\nLena Martens and\nXiang Lorraine Li and\nAdhiguna Kuncoro and\nAida Nematzadeh and\nElena Gribovskaya and\nDomenic Donato and\nAngeliki Lazaridou and\nArthur Mensch and\nJean-Baptiste Lespiau and\nMaria Tsimpoukelli and\nNikolai Grigorev and\nDoug Fritz and\nThibault Sottiaux and\nMantas Pajarskas and\nToby Pohlen and\nZhitao Gong and\nDaniel Toyama and\nCyprien de Masson d'Autume and\nYujia Li and\nTayfun Terzi and\nVladimir Mikulik and\nIgor Babuschkin and\nAidan Clark and\nDiego de Las Casas and\nAurelia Guy and\nChris Jones and\nJames Bradbury and\nMatthew J. Johnson and\nBlake A. Hechtman and\nLaura Weidinger and\nIason Gabriel and\nWilliam S. Isaac and\nEdward Lockhart and\nSimon Osindero and\nLaura Rimell and\nChris Dyer and\nOriol Vinyals and\nKareem Ayoub and\nJeff Stanway and\nLorrayne Bennett and\nDemis Hassabis and\nKoray Kavukcuoglu and\nGeoffrey Irving","meta_info":{"year":"2021","url":"https:\/\/arxiv.org\/abs\/2112.11446v2","journal":"arXiv preprint arXiv:2112.11446v2"}}
{"bib_id":"Peters:2018","title":"Deep Contextualized Word Representations","author":"Peters, Matthew E.  and\nNeumann, Mark  and\nIyyer, Mohit  and\nGardner, Matt  and\nClark, Christopher  and\nLee, Kenton  and\nZettlemoyer, Luke","meta_info":{"pages":"2227--2237","doi":"10.18653\/v1\/N18-1202","url":"https:\/\/aclanthology.org\/N18-1202","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"}}
{"bib_id":"feurer2020auto","title":"Auto-Sklearn 2.0: Hands-free AutoML via Meta-Learning","author":"Matthias Feurer and Katharina Eggensperger and Stefan Falkner and Marius Lindauer and Frank Hutter","meta_info":{"url":"http:\/\/jmlr.org\/papers\/v23\/21-0992.html","pages":"1--61","number":"261","volume":"23","year":"2022","journal":"Journal of Machine Learning Research"}}
{"bib_id":"zimmer2021auto","title":"Auto-Pytorch: Multi-Fidelity MetaLearning for Efficient and Robust AutoDL","author":"Zimmer, Lucas and Lindauer, Marius and Hutter, Frank","meta_info":{"url":"https:\/\/ieeexplore.ieee.org\/document\/9382913","publisher":"IEEE","year":"2021","pages":"3079--3090","number":"9","volume":"43","journal":"IEEE Transactions on Pattern Analysis and Machine Intelligence"}}
{"bib_id":"Gou:2021","title":"Knowledge Distillation: A Survey","author":"Gou, Jianping and Yu, Baosheng and Maybank, Stephen J. and Tao, Dacheng","meta_info":{"keywords":"Teacher–student architecture, Knowledge transfer, Knowledge distillation, Model compression, Deep neural networks","numpages":"31","pages":"1789–1819","month":"jun","journal":"Int. J. Comput. Vision","doi":"10.1007\/s11263-021-01453-z","url":"https:\/\/doi.org\/10.1007\/s11263-021-01453-z","issn":"0920-5691","number":"6","volume":"129","address":"USA","publisher":"Kluwer Academic Publishers","issue_date":"Jun 2021","year":"2021"}}
{"bib_id":"Dodge:2022","title":"Measuring the Carbon Intensity of AI in Cloud Instances","author":"Dodge, Jesse and Prewitt, Taylor and Tachet des Combes, Remi and Odmark, Erika and Schwartz, Roy and Strubell, Emma and Luccioni, Alexandra Sasha and Smith, Noah A. and DeCario, Nicole and Buchanan, Will","meta_info":{"series":"FAccT '22","location":"Seoul, Republic of Korea","keywords":"cloud, CO2, emissions, carbon awareness, grid, carbon intensity","numpages":"18","pages":"1877–1894","booktitle":"2022 ACM Conference on Fairness, Accountability, and Transparency","doi":"10.1145\/3531146.3533234","url":"https:\/\/doi.org\/10.1145\/3531146.3533234","address":"New York, NY, USA","publisher":"Association for Computing Machinery","isbn":"9781450393522","year":"2022"}}
{"bib_id":"ponti2022combining","title":"Combining Modular Skills in Multitask Learning","author":"Ponti, Edoardo M and Sordoni, Alessandro and Reddy, Siva","meta_info":{"year":"2022","url":"https:\/\/arxiv.org\/abs\/2202.13914v1","journal":"arXiv preprint arXiv:2202.13914v1"}}
{"bib_id":"wu2022sustainable","title":"Sustainable AI: Environmental Implications, Challenges and Opportunities","author":"Wu, Carole-Jean and Raghavendra, Ramya and Gupta, Udit and Acun, Bilge and Ardalani, Newsha and Maeng, Kiwan and Chang, Gloria and Aga, Fiona and Huang, Jinshi and Bai, Charles and Gschwind, Michael and Gupta, Anurag and Ott, Myle and Melnikov, Anastasia and Candido, Salvatore and Brooks, David and Chauhan, Geeta and Lee, Benjamin and Lee, Hsien-Hsin and Akyildiz, Bugra and Balandat, Maximilian and Spisak, Joe and Jain, Ravi and Rabbat, Mike and Hazelwood, Kim","meta_info":{"year":"2022","volume":"4","url":"https:\/\/proceedings.mlsys.org\/paper\/2022\/file\/ed3d2c21991e3bef5e069713af9fa6ca-Paper.pdf","pages":"795--813","editor":"D. Marculescu and Y. Chi and C. Wu","booktitle":"Proceedings of Machine Learning and Systems"}}
{"bib_id":"patterson2021carbon","title":"Carbon Emissions and Large Neural Network Training","author":"Patterson, David and Gonzalez, Joseph and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David and Texier, Maud and Dean, Jeff","meta_info":{"year":"2021","url":"https:\/\/arxiv.org\/abs\/2104.10350v3","journal":"arXiv preprint arXiv:2104.10350v3"}}
{"bib_id":"ma2022mega","title":"Mega: Moving Average Equipped Gated Attention","author":"Xuezhe Ma and Chunting Zhou and Xiang Kong and Junxian He and Liangke Gui and Graham Neubig and Jonathan May and Luke Zettlemoyer","meta_info":{"url":"https:\/\/openreview.net\/forum?id=qNLe3iq2El","year":"2023","booktitle":"The Eleventh International Conference on Learning Representations "}}
{"bib_id":"zhai2021attention","title":"An Attention Free Transformer","author":"Zhai, Shuangfei and Talbott, Walter and Srivastava, Nitish and Huang, Chen and Goh, Hanlin and Zhang, Ruixiang and Susskind, Josh","meta_info":{"year":"2021","url":"https:\/\/arxiv.org\/abs\/2105.14103v1","journal":"arXiv preprint arXiv:2105.14103v1"}}
{"bib_id":"peters-etal-2019-sparse","title":"Sparse Sequence-to-Sequence Models","author":"Peters, Ben  and\nNiculae, Vlad  and\nMartins, André F. T.","meta_info":{"pages":"1504--1519","doi":"10.18653\/v1\/P19-1146","url":"https:\/\/www.aclweb.org\/anthology\/P19-1146","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"July","booktitle":"Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"}}
{"bib_id":"xia-etal-2022-structured","title":"Structured Pruning Learns Compact and Accurate Models","author":"Xia, Mengzhou  and\nZhong, Zexuan  and\nChen, Danqi","meta_info":{"pages":"1513--1528","doi":"10.18653\/v1\/2022.acl-long.107","url":"https:\/\/aclanthology.org\/2022.acl-long.107","publisher":"Association for Computational Linguistics","address":"Dublin, Ireland","year":"2022","month":"May","booktitle":"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"}}
{"bib_id":"li2022makes","title":"What Makes Convolutional Models Great on Long Sequence Modeling?","author":"Li, Yuhong and Cai, Tianle and Zhang, Yi and Chen, Deming and Dey, Debadeepta","meta_info":{"url":"https:\/\/arxiv.org\/abs\/2210.09298v1","year":"2022","journal":"arXiv preprint arXiv:2210.09298v1"}}
{"bib_id":"mustafa2022multimodal","title":"Multimodal Contrastive Learning with LIMoE: The Language-Image Mixture of Experts","author":"Basil Mustafa and Carlos Riquelme Ruiz and Joan Puigcerver and Rodolphe Jenatton and Neil Houlsby","meta_info":{"url":"https:\/\/openreview.net\/forum?id=Qy1D9JyMBg0","year":"2022","editor":"Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho","booktitle":"Advances in Neural Information Processing Systems"}}
{"bib_id":"zoph2022designing","title":"Designing Effective Sparse Expert Models","author":"Zoph, Barret and Bello, Irwan and Kumar, Sameer and Du, Nan and Huang, Yanping and Dean, Jeff and Shazeer, Noam and Fedus, William","meta_info":{"url":"https:\/\/arxiv.org\/abs\/2202.07101","organization":"IEEE Computer Society","year":"2022","pages":"1044--1044","booktitle":"2022 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)"}}
{"bib_id":"gu2022parameterization","title":"On the Parameterization and Initialization of Diagonal State Space Models","author":"Albert Gu and Karan Goel and Ankit Gupta and Christopher Ré","meta_info":{"url":"https:\/\/openreview.net\/forum?id=yJE7iQSAep","year":"2022","editor":"Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho","booktitle":"Advances in Neural Information Processing Systems"}}
{"bib_id":"gupta2022diagonal","title":"Diagonal State Spaces are as Effective as Structured State Spaces","author":"Ankit Gupta and Albert Gu and Jonathan Berant","meta_info":{"url":"https:\/\/openreview.net\/forum?id=RjS0j6tsSrf","year":"2022","editor":"Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho","booktitle":"Advances in Neural Information Processing Systems"}}
{"bib_id":"mehta2022long","title":"Long Range Language Modeling via Gated State Spaces","author":"Harsh Mehta and Ankit Gupta and Ashok Cutkosky and Behnam Neyshabur","meta_info":{"url":"https:\/\/openreview.net\/forum?id=5MkYIYCbva","year":"2023","booktitle":"The Eleventh International Conference on Learning Representations "}}
{"bib_id":"press2022train","title":"Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation","author":"Ofir Press and Noah Smith and Mike Lewis","meta_info":{"url":"https:\/\/openreview.net\/forum?id=R8sQPpGCv0","year":"2022","booktitle":"International Conference on Learning Representations"}}
{"bib_id":"dubois-etal-2020-location","title":"Location Attention for Extrapolation to Longer Sequences","author":"Dubois, Yann  and\nDagan, Gautier  and\nHupkes, Dieuwke  and\nBruni, Elia","meta_info":{"pages":"403--413","doi":"10.18653\/v1\/2020.acl-main.39","url":"https:\/\/aclanthology.org\/2020.acl-main.39","publisher":"Association for Computational Linguistics","address":"Online","year":"2020","month":"July","booktitle":"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"}}
{"bib_id":"shaw-etal-2018-self","title":"Self-Attention with Relative Position Representations","author":"Shaw, Peter  and\nUszkoreit, Jakob  and\nVaswani, Ashish","meta_info":{"pages":"464--468","doi":"10.18653\/v1\/N18-2074","url":"https:\/\/aclanthology.org\/N18-2074","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)"}}
{"bib_id":"kaplan2020scaling","title":"Scaling Laws for Neural Language Models","author":"Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario","meta_info":{"url":"https:\/\/arxiv.org\/abs\/2001.08361v1","year":"2020","journal":"arXiv preprint arXiv:2001.08361v1"}}
{"bib_id":"xu2022survey","title":"A Survey on Dynamic Neural Networks for Natural Language Processing","author":"Xu, Canwen and McAuley, Julian","meta_info":{"year":"2023","url":"https:\/\/arxiv.org\/abs\/2202.07101v2","booktitle":"Findings of EACL"}}
{"bib_id":"dettmers2022llm","title":"GPT3.int8(): 8-bit Matrix Multiplication for Transformers at Scale","author":"Tim Dettmers and Mike Lewis and Younes Belkada and Luke Zettlemoyer","meta_info":{"url":"https:\/\/openreview.net\/forum?id=dXiGWqBoxaD","year":"2022","editor":"Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho","booktitle":"Advances in Neural Information Processing Systems"}}
{"bib_id":"Hassid:2022","title":"How Much Does Attention Actually Attend? Questioning the Importance of Attention in Pretrained Transformers","author":"Hassid, Michael  and\nPeng, Hao  and\nRotem, Daniel  and\nKasai, Jungo  and\nMontero, Ivan  and\nSmith, Noah A.  and\nSchwartz, Roy","meta_info":{"pages":"1403--1416","url":"https:\/\/aclanthology.org\/2022.findings-emnlp.101","publisher":"Association for Computational Linguistics","address":"Abu Dhabi, United Arab Emirates","year":"2022","month":"December","booktitle":"Findings of the Association for Computational Linguistics: EMNLP 2022"}}
{"bib_id":"iandolaSqueezeBERTWhatCan2020a","title":"SqueezeBERT: What can computer vision teach NLP about efficient neural networks?","author":"Iandola, Forrest and Shaw, Albert and Krishna, Ravi and Keutzer, Kurt","meta_info":{"abstract":"Humans read and write hundreds of billions of messages every day. Further, due to the availability of large datasets, large computing systems, and better neural network models, natural language processing (NLP) technology has made significant strides in understanding, proofreading, and organizing these messages. Thus, there is a significant opportunity to deploy NLP in myriad applications to help web users, social networks, and businesses. Toward this end, we consider smartphones and other mobile devices as crucial platforms for deploying NLP models at scale. However, today's highly-accurate NLP neural network models such as BERT and RoBERTa are extremely computationally expensive, with BERT-base taking 1.7 seconds to classify a text snippet on a Pixel 3 smartphone. To begin to address this problem, we draw inspiration from the computer vision community, where work such as MobileNet has demonstrated that grouped convolutions (e.g. depthwise convolutions) can enable speedups without sacrificing accuracy. We demonstrate how to replace several operations in self-attention layers with grouped convolutions, and we use this technique in a novel network architecture called SqueezeBERT, which runs 4.3x faster than BERT-base on the Pixel 3 while achieving competitive accuracy on the GLUE test set. A PyTorch-based implementation of SqueezeBERT is available as part of the Hugging Face Transformers library: https:\/\/huggingface.co\/squeezebert","urldate":"2022-07-04","url":"https:\/\/aclanthology.org\/2020.sustainlp-1.17","doi":"10.18653\/v1\/2020.sustainlp-1.17","pages":"124--135","address":"Online","publisher":"Association for Computational Linguistics","booktitle":"Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing","month":"November","year":"2020","shorttitle":"SqueezeBERT"}}
{"bib_id":"sankarProFormerOnDeviceLSH2021","title":"ProFormer: Towards On-Device LSH Projection Based Transformers","author":"Sankar, Chinnadhurai and Ravi, Sujith and Kozareva, Zornitsa","meta_info":{"abstract":"At the heart of text based neural models lay word representations, which are powerful but occupy a lot of memory making it challenging to deploy to devices with memory constraints such as mobile phones, watches and IoT. To surmount these challenges, we introduce ProFormer – a projection based transformer architecture that is faster and lighter making it suitable to deploy to memory constraint devices and preserve user privacy. We use LSH projection layer to dynamically generate word representations on-the-fly without embedding lookup tables leading to significant memory footprint reduction from O(V.d) to O(T), where V is the vocabulary size, d is the embedding dimension size and T is the dimension of the LSH projection representation.We also propose a local projection attention (LPA) layer, which uses self-attention to transform the input sequence of N LSH word projections into a sequence of N\/K representations reducing the computations quadratically by O(K\\textasciicircum2). We evaluate ProFormer on multiple text classification tasks and observed improvements over prior state-of-the-art on-device approaches for short text classification and comparable performance for long text classification tasks. ProFormer is also competitive with other popular but highly resource-intensive approaches like BERT and even outperforms small-sized BERT variants with significant resource savings – reduces the embedding memory footprint from 92.16 MB to 1.7 KB and requires 16x less computation overhead, which is very impressive making it the fastest and smallest on-device model.","urldate":"2022-07-04","url":"https:\/\/aclanthology.org\/2021.eacl-main.246","doi":"10.18653\/v1\/2021.eacl-main.246","pages":"2823--2828","address":"Online","publisher":"Association for Computational Linguistics","booktitle":"Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume","month":"April","year":"2021","shorttitle":"ProFormer"}}
{"bib_id":"howardMobileNetsEfficientConvolutional2017","title":"MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications","author":"Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig","meta_info":{"note_":"arXiv: 1704.04861","year":"2017","month":"April","journal":"arXiv preprint arXiv:1704.04861v1","urldate":"2020-06-05","abstract":"We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.","url":"http:\/\/arxiv.org\/abs\/1704.04861v1","shorttitle":"MobileNets"}}
{"bib_id":"wu*LiteTransformerLongShort2022","title":"Lite Transformer with Long-Short Range Attention","author":"Zhanghao Wu and Zhijian Liu and Ji Lin and Yujun Lin and Song Han","meta_info":{"url":"https:\/\/openreview.net\/forum?id=ByeMPlHKPH","year":"2020","booktitle":"International Conference on Learning Representations"}}
{"bib_id":"huangGhostBERTGenerateMore2021","title":"GhostBERT: Generate More Features with Cheap Operations for BERT","author":"Huang, Zhiqi and Hou, Lu and Shang, Lifeng and Jiang, Xin and Chen, Xiao and Liu, Qun","meta_info":{"urldate":"2022-07-04","url":"https:\/\/aclanthology.org\/2021.acl-long.509","doi":"10.18653\/v1\/2021.acl-long.509","pages":"6512--6523","address":"Online","publisher":"Association for Computational Linguistics","booktitle":"Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)","month":"August","year":"2021","shorttitle":"GhostBERT"}}
{"bib_id":"geEdgeFormerParameterEfficientTransformer2022","title":"EdgeFormer: A Parameter-Efficient Transformer for On-Device Seq2seq Generation","author":"Ge, Tao  and\nChen, Si-Qing  and\nWei, Furu","meta_info":{"pages":"10786--10798","url":"https:\/\/aclanthology.org\/2022.emnlp-main.741","publisher":"Association for Computational Linguistics","address":"Abu Dhabi, United Arab Emirates","year":"2022","month":"December","booktitle":"Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"}}
{"bib_id":"Zellers:2018","title":"SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference","author":"Zellers, Rowan  and\nBisk, Yonatan  and\nSchwartz, Roy  and\nChoi, Yejin","meta_info":{"pages":"93--104","doi":"10.18653\/v1\/D18-1009","url":"https:\/\/aclanthology.org\/D18-1009","publisher":"Association for Computational Linguistics","address":"Brussels, Belgium","year":"2018","month":"October-November","booktitle":"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing"}}
{"bib_id":"LeBras:2020","title":"Adversarial Filters of Dataset Biases","author":"Le Bras, Ronan and Swayamdipta, Swabha and Bhagavatula, Chandra and Zellers, Rowan and Peters, Matthew and Sabharwal, Ashish and Choi, Yejin","meta_info":{"url":"https:\/\/proceedings.mlr.press\/v119\/bras20a.html","pdf":"http:\/\/proceedings.mlr.press\/v119\/bras20a\/bras20a.pdf","publisher":"PMLR","month":"13--18 Jul","series":"Proceedings of Machine Learning Research","volume":"119","editor":"III, Hal Daumé and Singh, Aarti","year":"2020","pages":"1078--1088","booktitle":"Proceedings of the 37th International Conference on Machine Learning"}}
{"bib_id":"pmlr-v119-li20m","title":"Train Big, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers","author":"Li, Zhuohan and Wallace, Eric and Shen, Sheng and Lin, Kevin and Keutzer, Kurt and Klein, Dan and Gonzalez, Joey","meta_info":{"abstract":"Since hardware resources are limited, the objective of training deep learning models is typically to maximize accuracy subject to the time and memory constraints of training and inference. We study the impact of model size in this setting, focusing on Transformer models for NLP tasks that are limited by compute: self-supervised pretraining and high-resource machine translation. We first show that even though smaller Transformer models execute faster per iteration, wider and deeper models converge in significantly fewer steps. Moreover, this acceleration in convergence typically outpaces the additional computational overhead of using larger models. Therefore, the most compute-efficient training strategy is to counterintuitively train extremely large models but stop after a small number of iterations. This leads to an apparent trade-off between the training efficiency of large Transformer models and the inference efficiency of small Transformer models. However, we show that large models are more robust to compression techniques such as quantization and pruning than small models. Consequently, one can get the best of both worlds: heavily compressed, large models achieve higher accuracy than lightly compressed, small models.","url":"https:\/\/proceedings.mlr.press\/v119\/li20m.html","pdf":"http:\/\/proceedings.mlr.press\/v119\/li20m\/li20m.pdf","publisher":"PMLR","month":"13--18 Jul","series":"Proceedings of Machine Learning Research","volume":"119","editor":"III, Hal Daumé and Singh, Aarti","year":"2020","pages":"5958--5968","booktitle":"Proceedings of the 37th International Conference on Machine Learning"}}
{"bib_id":"Dodge:2019","title":"Show Your Work: Improved Reporting of Experimental Results","author":"Dodge, Jesse  and\nGururangan, Suchin  and\nCard, Dallas  and\nSchwartz, Roy  and\nSmith, Noah A.","meta_info":{"pages":"2185--2194","doi":"10.18653\/v1\/D19-1224","url":"https:\/\/aclanthology.org\/D19-1224","publisher":"Association for Computational Linguistics","address":"Hong Kong, China","year":"2019","month":"November","booktitle":"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)"}}
{"bib_id":"li_efficient_2021","title":"Efficient Methods for Mapping Neural Machine Translator on FPGAs","author":"Li, Qin and Zhang, Xiaofan and Xiong, Jinjun and Hwu, Wen-Mei and Chen, Deming","meta_info":{"pages":"1866--1877","keywords":"Computational modeling, Decoding, Dictionaries, Field programmable gate arrays, FPGA, Hardware, Hardware-efficient inference, high level synthesis, IP networks, neural machine translation, Task analysis","note":"Conference Name: IEEE Transactions on Parallel and Distributed Systems","year":"2021","month":"July","journal":"IEEE Transactions on Parallel and Distributed Systems","number":"7","abstract":"Neural machine translation (NMT) is one of the most critical applications in natural language processing (NLP) with the main idea of converting text in one language to another using deep neural networks. In recent year, we have seen continuous development of NMT by integrating more emerging technologies, such as bidirectional gated recurrent units (GRU), attention mechanisms, and beam-search algorithms, for improved translation quality. However, with the increasing problem size, the real-life NMT models have become much more complicated and difficult to implement on hardware for acceleration opportunities. In this article, we aim to exploit the capability of FPGAs to deliver highly efficient implementations for real-life NMT applications. We map the inference of a large-scale NMT model with total computation of 172 GFLOP to a highly optimized high-level synthesis (HLS) IP and integrate the IP into Xilinx VCU118 FPGA platform. The model has widely used key features for NMTs, including the bidirectional GRU layer, attention mechanism, and beam search. We quantize the model to mixed-precision representation in which parameters and portions of calculations are in 16-bit half precision, and others remain as 32-bit floating-point. Compared to the float NMT implementation on FPGA, we achieve 13.1× speedup with an end-to-end performance of 22.0 GFLOPS without any accuracy degradation. Based on our knowledge, this is the first work that successfully implements a real-life end-to-end NMT model to an FPGA on board.","doi":"10.1109\/TPDS.2020.3047371","issn":"1558-2183","volume":"32"}}
{"bib_id":"liu_hardware_2021","title":"Hardware Acceleration of Fully Quantized BERT for Efficient Natural Language Processing","author":"Liu, Zejian and Li, Gang and Cheng, Jian","meta_info":{"keywords":"Computer Science - Hardware Architecture, Computer Science - Computation and Language","note_":"arXiv: 2103.02800","year":"2021","month":"February","booktitle":"Design, Automation & Test in Europe Conference & Exhibition (DATE)","urldate":"2021-06-28","abstract":"BERT is the most recent Transformer-based model that achieves state-of-the-art performance in various NLP tasks. In this paper, we investigate the hardware acceleration of BERT on FPGA for edge computing. To tackle the issue of huge computational complexity and memory footprint, we propose to fully quantize the BERT (FQ-BERT), including weights, activations, softmax, layer normalization, and all the intermediate results. Experiments demonstrate that the FQ-BERT can achieve 7.94x compression for weights with negligible performance loss. We then propose an accelerator tailored for the FQ-BERT and evaluate on Xilinx ZCU102 and ZCU111 FPGA. It can achieve a performance-per-watt of 3.18 fps\/W, which is 28.91x and 12.72x over Intel(R) Core(TM) i7-8700 CPU and NVIDIA K80 GPU, respectively.","url":"http:\/\/arxiv.org\/abs\/2103.02800"}}
{"bib_id":"zadeh_gobo_2020","title":"GOBO: Quantizing Attention-Based NLP Models for Low Latency and Energy Efficient Inference","author":"Zadeh, A. H. and Edo, I. and Awad, O. M. and Moshovos, A.","meta_info":{"file":"IEEE Xplore Abstract Record:C\\:\\\\Users\\χck\\\\Zotero\\\\storage\\\\WDW5CF3E\\\\9251854.html:text\/html;Submitted Version:C\\:\\\\Users\\χck\\\\Zotero\\\\storage\\\\6HSWNNX8\\\\Zadeh et al. - 2020 - GOBO Quantizing Attention-Based NLP Models for Lo.pdf:application\/pdf","pages":"811--824","keywords":"Hardware, Tensors, Computational modeling, Quantization (signal), Task analysis, natural language processing, attention-based models, attention-based NLP models, BERT models, data compression, Energy efficiency, energy efficient inference, floating point arithmetic, floating-point parameters, GOBO architecture, GOBO memory compression mechanism, Memory management, memory storage, microprocessor chips, model quantization technique, n\/a, natural language understanding tasks, quantization methods","year":"2020","month":"October","booktitle":"2020 53rd Annual IEEE\/ACM International Symposium on Microarchitecture (MICRO)","abstract":"Attention-based models have demonstrated remarkable success in various natural language understanding tasks. However, efficient execution remains a challenge for these models which are memory-bound due to their massive number of parameters. We present GOBO, a model quantization technique that compresses the vast majority (typically 99.9%) of the 32-bit floating-point parameters of state-of-the-art BERT models and their variants to 3 bits while maintaining their accuracy. Unlike other quantization methods, GOBO does not require fine-tuning nor retraining to compensate for the quantization error. We present two practical hardware applications of GOBO. In the first GOBO reduces memory storage and traffic and as a result inference latency and energy consumption. This GOBO memory compression mechanism is plug-in compatible with many architectures; we demonstrate it with the TPU, Eyeriss, and an architecture using Tensor Cores-like units. Second, we present a co-designed hardware architecture that also reduces computation. Uniquely, the GOBO architecture maintains most of the weights in 3b even during computation, a property that: (i) makes the processing elements area efficient, allowing us to pack more compute power per unit area, (ii) replaces most multiply-accumulations with additions, and (iii) reduces the off-chip traffic by amplifying on-chip memory capacity.","doi":"10.1109\/MICRO50266.2020.00071","shorttitle":"GOBO"}}
{"bib_id":"lu_hardware_2020","title":"Hardware Accelerator for Multi-Head Attention and Position-Wise Feed-Forward in the Transformer","author":"Lu, Siyuan and Wang, Meiqi and Liang, Shuang and Lin, Jun and Wang, Zhongfeng","meta_info":{"url":"https:\/\/ieeexplore.ieee.org\/document\/9524802\/","organization":"IEEE","year":"2020","pages":"84--89","booktitle":"2020 IEEE 33rd International System-on-Chip Conference (SOCC)"}}
{"bib_id":"ham_3_2020","title":"A\\textasciicircum3: Accelerating Attention Mechanisms in Neural Networks with Approximation","author":"Ham, Tae Jun and Jung, Sung Jun and Kim, Seonghak and Oh, Young H. and Park, Yeonhong and Song, Yoonho and Park, Jung-Hun and Lee, Sanghee and Park, Kyoung and Lee, Jae W. and Jeong, Deog-Kyoon","meta_info":{"file":"IEEE Xplore Abstract Record:C\\:\\\\Users\\χck\\\\Zotero\\\\storage\\\\YCCZE6TL\\\\9065498.html:text\/html;Submitted Version:C\\:\\\\Users\\χck\\\\Zotero\\\\storage\\\\R78HNU3Y\\a̋m et al. - 2020 - A^3 Accelerating Attention Mechanisms in Neural N.pdf:application\/pdf","pages":"328--341","keywords":"Hardware, Computational modeling, Data models, Task analysis, Artificial neural networks, neural network, accelerators, approximation, attention mechanism, Bit error rate, Domain Specific Architectures","note":"ISSN: 2378-203X","year":"2020","month":"February","booktitle":"2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)","abstract":"With the increasing computational demands of the neural networks, many hardware accelerators for the neural networks have been proposed. Such existing neural network accelerators often focus on popular neural network types such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs); however, not much attention has been paid to attention mechanisms, an emerging neural network primitive that enables neural networks to retrieve most relevant information from a knowledge-base, external memory, or past states. The attention mechanism is widely adopted by many state-of-the-art neural networks for computer vision, natural language processing, and machine translation, and accounts for a large portion of total execution time. We observe today's practice of implementing this mechanism using matrix-vector multiplication is suboptimal as the attention mechanism is semantically a content-based search where a large portion of computations ends up not being used. Based on this observation, we design and architect A3, which accelerates attention mechanisms in neural networks with algorithmic approximation and hardware specialization. Our proposed accelerator achieves multiple orders of magnitude improvement in energy efficiency (performance\/watt) as well as substantial speedup over the state-of-the-art conventional hardware.","doi":"10.1109\/HPCA47549.2020.00035","shorttitle":"A\\textasciicircum3"}}
{"bib_id":"wang_spatten_2021","title":"SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning","author":"Wang, Hanrui and Zhang, Zhekai and Han, Song","meta_info":{"file":"IEEE Xplore Abstract Record:C\\:\\\\Users\\χck\\\\Zotero\\\\storage\\\\QUIMWH59\\\\9407232.html:text\/html;Submitted Version:C\\:\\\\Users\\χck\\\\Zotero\\\\storage\\\\8656GZYN\\\\Wang et al. - 2021 - SpAtten Efficient Sparse Attention Architecture w.pdf:application\/pdf","pages":"97--110","keywords":"Throughput, Random access memory, Quantization (signal), Space exploration, Memory management, Algorithm-Architecture Co-design, Attention, Domain-Specific Accelerator, Natural language processing, Natural Language Processing, Pruning, Quantization, Redundancy","note":"ISSN: 2378-203X","year":"2021","month":"February","booktitle":"2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)","abstract":"The attention mechanism is becoming increasingly popular in Natural Language Processing (NLP) applications, showing superior performance than convolutional and recurrent architectures. However, general-purpose platforms such as CPUs and GPUs are inefficient when performing attention inference due to complicated data movement and low arithmetic intensity. Moreover, existing NN accelerators mainly focus on optimizing convolutional or recurrent models, and cannot efficiently support attention. In this paper, we present SpAtten, an efficient algorithm-architecture co-design that leverages token sparsity, head sparsity, and quantization opportunities to reduce the attention computation and memory access. Inspired by the high redundancy of human languages, we propose the novel cascade token pruning to prune away unimportant tokens in the sentence. We also propose cascade head pruning to remove unessential heads. Cascade pruning is fundamentally different from weight pruning since there is no trainable weight in the attention mechanism, and the pruned tokens and heads are selected on the fly. To efficiently support them on hardware, we design a novel top-k engine to rank token and head importance scores with high throughput. Furthermore, we propose progressive quantization that first fetches MSBs only and performs the computation; if the confidence is low, it fetches LSBs and recomputes the attention outputs, trading computation for memory reduction.Extensive experiments on 30 benchmarks show that, on average, SpAtten reduces DRAM access by 10.0× with no accuracy loss, and achieves 1.6×, 3.0×, 162×, 347× speedup, and 1.4×, 3.2×, 1193×, 4059× energy savings over A3 accelerator, MNNFast accelerator, TITAN Xp GPU, Xeon CPU, respectively.","doi":"10.1109\/HPCA51647.2021.00018","shorttitle":"SpAtten"}}
{"bib_id":"qu_dota_2022","title":"DOTA: detect and omit weak attentions for scalable transformer acceleration","author":"Qu, Zheng and Liu, Liu and Tu, Fengbin and Chen, Zhaodong and Ding, Yufei and Xie, Yuan","meta_info":{"file":"Full Text PDF:C\\:\\\\Users\\χck\\\\Zotero\\\\storage\\C̋R997EM\\\\Qu et al. - 2022 - DOTA detect and omit weak attentions for scalable.pdf:application\/pdf","pages":"14--26","keywords":"Sparse Architecture, SW-HW Co-design, Transformer Acceleration","year":"2022","month":"February","publisher":"Association for Computing Machinery","booktitle":"Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems","urldate":"2022-05-12","abstract":"Transformer Neural Networks have demonstrated leading performance in many applications spanning over language understanding, image processing, and generative modeling. Despite the impressive performance, long-sequence Transformer processing is expensive due to quadratic computation complexity and memory consumption of self-attention. In this paper, we present DOTA, an algorithm-architecture co-design that effectively addresses the challenges of scalable Transformer inference. Based on the insight that not all connections in an attention graph are equally important, we propose to jointly optimize a lightweight Detector with the Transformer model to accurately detect and omit weak connections during runtime. Furthermore, we design a specialized system architecture for end-to-end Transformer acceleration using the proposed attention detection mechanism. Experiments on a wide range of benchmarks demonstrate the superior performance of DOTA over other solutions. In summary, DOTA achieves 152.6x and 4.5x performance speedup and orders of magnitude energy-efficiency improvements over GPU and customized hardware, respectively.","doi":"10.1145\/3503222.3507738","url":"https:\/\/doi.org\/10.1145\/3503222.3507738","shorttitle":"DOTA","isbn":"978-1-4503-9205-1","series":"ASPLOS 2022","address":"New York, NY, USA"}}
{"bib_id":"ham_elsa_2021","title":"ELSA: Hardware-Software Co-design for Efficient, Lightweight Self-Attention Mechanism in Neural Networks","author":"Ham, Tae Jun and Lee, Yejin and Seo, Seong Hoon and Kim, Soosung and Choi, Hyunji and Jung, Sung Jun and Lee, Jae W.","meta_info":{"pages":"692--705","keywords":"Hardware, Graphics processing units, Computational modeling, Energy efficiency, Runtime, Artificial neural networks, neural network, hardware accelerator, Natural language processing, attention","note":"ISSN: 2575-713X","year":"2021","month":"June","booktitle":"2021 ACM\/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)","abstract":"The self-attention mechanism is rapidly emerging as one of the most important key primitives in neural networks (NNs) for its ability to identify the relations within input entities. The self-attention-oriented NN models such as Google Transformer and its variants have established the state-of-the-art on a very wide range of natural language processing tasks, and many other self-attention-oriented models are achieving competitive results in computer vision and recommender systems as well. Unfortunately, despite its great benefits, the self-attention mechanism is an expensive operation whose cost increases quadratically with the number of input entities that it processes, and thus accounts for a significant portion of the inference runtime. Thus, this paper presents ELSA (Efficient, Lightweight Self-Attention), a hardware-software co-designed solution to substantially reduce the runtime as well as energy spent on the self-attention mechanism. Specifically, based on the intuition that not all relations are equal, we devise a novel approximation scheme that significantly reduces the amount of computation by efficiently filtering out relations that are unlikely to affect the final output. With the specialized hardware for this approximate self-attention mechanism, ELSA achieves a geomean speedup of 58.1× as well as over three orders of magnitude improvements in energy efficiency compared to GPU on self-attention computation in modern NN models while maintaining less than 1% loss in the accuracy metric.","doi":"10.1109\/ISCA52012.2021.00060","shorttitle":"ELSA"}}
{"bib_id":"zadeh_mokey_2022","title":"Mokey: enabling narrow fixed-point inference for out-of-the-box floating-point transformer models","author":"Zadeh, Ali Hadi and Mahmoud, Mostafa and Abdelhadi, Ameer and Moshovos, Andreas","meta_info":{"pages":"888--901","keywords":"quantization, natural language processing, transformer models","year":"2022","month":"June","publisher":"Association for Computing Machinery","booktitle":"Proceedings of the 49th Annual International Symposium on Computer Architecture","urldate":"2022-06-22","abstract":"Increasingly larger and better Transformer models keep advancing state-of-the-art accuracy and capability for Natural Language Processing applications. These models demand more computational power, storage, and energy. Mokey reduces the footprint of state-of-the-art 32-bit or 16-bit floating-point transformer models by quantizing all values to 4-bit indexes into dictionaries of representative 16-bit fixed-point centroids. Mokey does not need fine-tuning, an essential feature as often the training resources or datasets are not available to many. Exploiting the range of values that naturally occur in transformer models, Mokey selects centroid values to also fit an exponential curve. This unique feature enables Mokey to replace the bulk of the original multiply-accumulate operations with narrow 3b fixed-point additions resulting in an area- and energy-efficient hardware accelerator design. Over a set of state-of-the-art transformer models, the Mokey accelerator delivers an order of magnitude improvements in energy efficiency over a Tensor Cores-based accelerator while improving performance by at least 4× and as much as 15× depending on the model and on-chip buffering capacity. Optionally, Mokey can be used as memory compression assist for any other accelerator transparently stashing wide floating-point or fixed-point activations or weights into narrow 4-bit indexes. Mokey proves superior to prior state-of-the-art quantization methods for Transformers.","doi":"10.1145\/3470496.3527438","url":"https:\/\/doi.org\/10.1145\/3470496.3527438","shorttitle":"Mokey","isbn":"978-1-4503-8610-4","series":"ISCA '22","address":"New York, NY, USA"}}
{"bib_id":"tambe_edgebert_2021","title":"EdgeBERT: Sentence-Level Energy Optimizations for Latency-Aware Multi-Task NLP Inference","author":"Tambe, Thierry and Hooper, Coleman and Pentecost, Lillian and Jia, Tianyu and Yang, En-Yu and Donato, Marco and Sanh, Victor and Whatmough, Paul and Rush, Alexander M. and Brooks, David and Wei, Gu-Yeon","meta_info":{"series":"MICRO '21","location":"Virtual Event, Greece","keywords":"latency-aware, embedded non-volatile memories, software and hardware co-design, natural language processing","numpages":"15","pages":"830–844","booktitle":"MICRO-54: 54th Annual IEEE\/ACM International Symposium on Microarchitecture","doi":"10.1145\/3466752.3480095","url":"https:\/\/doi.org\/10.1145\/3466752.3480095","address":"New York, NY, USA","publisher":"Association for Computing Machinery","isbn":"9781450385572","year":"2021"}}
{"bib_id":"zhang_ternarybert_2020","title":"TernaryBERT: Distillation-aware Ultra-low Bit BERT","author":"Zhang, Wei  and\nHou, Lu  and\nYin, Yichun  and\nShang, Lifeng  and\nChen, Xiao  and\nJiang, Xin  and\nLiu, Qun","meta_info":{"pages":"509--521","doi":"10.18653\/v1\/2020.emnlp-main.37","url":"https:\/\/aclanthology.org\/2020.emnlp-main.37","publisher":"Association for Computational Linguistics","address":"Online","year":"2020","month":"November","booktitle":"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)"}}
{"bib_id":"bai_binarybert_2020","title":"BinaryBERT: Pushing the Limit of BERT Quantization","author":"Bai, Haoli  and\nZhang, Wei  and\nHou, Lu  and\nShang, Lifeng  and\nJin, Jin  and\nJiang, Xin  and\nLiu, Qun  and\nLyu, Michael  and\nKing, Irwin","meta_info":{"pages":"4334--4348","doi":"10.18653\/v1\/2021.acl-long.334","url":"https:\/\/aclanthology.org\/2021.acl-long.334","publisher":"Association for Computational Linguistics","address":"Online","year":"2021","month":"August","booktitle":"Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)"}}
{"bib_id":"ravula_etc_2020","title":"ETC: Encoding Long and Structured Inputs in Transformers","author":"Ainslie, Joshua  and\nOntanon, Santiago  and\nAlberti, Chris  and\nCvicek, Vaclav  and\nFisher, Zachary  and\nPham, Philip  and\nRavula, Anirudh  and\nSanghai, Sumit  and\nWang, Qifan  and\nYang, Li","meta_info":{"pages":"268--284","doi":"10.18653\/v1\/2020.emnlp-main.19","url":"https:\/\/aclanthology.org\/2020.emnlp-main.19","publisher":"Association for Computational Linguistics","address":"Online","year":"2020","month":"November","booktitle":"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)"}}
{"bib_id":"zafrir_q8bert_2019","title":"Q8BERT: Quantized 8Bit BERT","author":"Zafrir, Ofir and Boudoukh, Guy and Izsak, Peter and Wasserblat, Moshe","meta_info":{"doi":"10.1109\/EMC2-NIPS53020.2019.00016","pages":"36-39","number":"","volume":"","year":"2019","booktitle":"2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing - NeurIPS Edition (EMC$^2$-NIPS)"}}
{"bib_id":"bhandare_efficient_2019","title":"Efficient 8-Bit Quantization of Transformer Neural Machine Language Translation Model","author":"Bhandare, Aishwarya and Sripathi, Vamsi and Karkada, Deepthi and Menon, Vivek and Choi, Sun and Datta, Kushal and Saletore, Vikram","meta_info":{"keywords":"Computer Science - Machine Learning","year":"2019","month":"June","booktitle":"Proceedings of the Joint Workshop on On-Device Machine Learning & Compact Deep Neural Network Representations, 36th International Conference on Machine Learning","urldate":"2021-01-25","url":"http:\/\/arxiv.org\/abs\/1906.00532v2"}}
{"bib_id":"shen_q-bert_2020","title":"Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT","author":"Shen, Sheng and Dong, Zhen and Ye, Jiayu and Ma, Linjian and Yao, Zhewei and Gholami, Amir and Mahoney, Michael W. and Keutzer, Kurt","meta_info":{"file":"Full Text PDF:C\\:\\\\Users\\χck\\\\Zotero\\\\storage\\\\EWAIC36K\\\\Shen et al. - 2020 - Q-BERT Hessian Based Ultra Low Precision Quantiza.pdf:application\/pdf;Snapshot:C\\:\\\\Users\\χck\\\\Zotero\\\\storage\\\\KDHJYTS9\\\\6409.html:text\/html","pages":"8815--8821","note":"Number: 05","year":"2020","month":"April","journal":"Proceedings of the AAAI Conference on Artificial Intelligence","urldate":"2021-01-25","number":"05","language":"en","doi":"10.1609\/aaai.v34i05.6409","url":"https:\/\/ojs.aaai.org\/index.php\/AAAI\/article\/view\/6409","shorttitle":"Q-BERT","issn":"2374-3468","copyright":"Copyright (c) 2020 Association for the Advancement of Artificial Intelligence","volume":"34"}}
{"bib_id":"correia_adaptively_2019","title":"Adaptively Sparse Transformers","author":"Correia, Gonçalo M. and Niculae, Vlad and Martins, André F. T.","meta_info":{"file":"Full Text PDF:C\\:\\\\Users\\χck\\\\Zotero\\\\storage\\\\IS6HL9JG\\\\Correia et al. - 2019 - Adaptively Sparse Transformers.pdf:application\/pdf","pages":"2174--2184","year":"2019","month":"November","publisher":"Association for Computational Linguistics","booktitle":"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)","urldate":"2021-01-22","abstract":"Attention mechanisms have become ubiquitous in NLP. Recent architectures, notably the Transformer, learn powerful context-aware word representations through layered, multi-headed attention. The multiple heads learn diverse types of word relationships. However, with standard softmax attention, all attention heads are dense, assigning a non-zero weight to all context words. In this work, we introduce the adaptively sparse Transformer, wherein attention heads have flexible, context-dependent sparsity patterns. This sparsity is accomplished by replacing softmax with alpha-entmax: a differentiable generalization of softmax that allows low-scoring words to receive precisely zero weight. Moreover, we derive a method to automatically learn the alpha parameter – which controls the shape and sparsity of alpha-entmax – allowing attention heads to choose between focused or spread-out behavior. Our adaptively sparse Transformer improves interpretability and head diversity when compared to softmax Transformers on machine translation datasets. Findings of the quantitative and qualitative analysis of our approach include that heads in different layers learn different sparsity preferences and tend to be more diverse in their attention distributions than softmax Transformers. Furthermore, at no cost in accuracy, sparsity in attention heads helps to uncover different head specializations.","doi":"10.18653\/v1\/D19-1223","url":"https:\/\/www.aclweb.org\/anthology\/D19-1223","address":"Hong Kong, China"}}
{"bib_id":"voita_analyzing_2019","title":"Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned","author":"Voita, Elena  and\nTalbot, David  and\nMoiseev, Fedor  and\nSennrich, Rico  and\nTitov, Ivan","meta_info":{"pages":"5797--5808","doi":"10.18653\/v1\/P19-1580","url":"https:\/\/aclanthology.org\/P19-1580","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"July","booktitle":"Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"}}
{"bib_id":"sanh_distilbert_2019","title":"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter","author":"Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas","meta_info":{"year":"2019","url":"http:\/\/arxiv.org\/abs\/1910.01108v4","booktitle":"NeurIPS EMC$^2$ Workshop"}}
{"bib_id":"sanh_movement_2020","title":"Movement Pruning: Adaptive Sparsity by Fine-Tuning","author":"Sanh, Victor and Wolf, Thomas and Rush, Alexander","meta_info":{"year":"2020","volume":"33","url":"https:\/\/proceedings.neurips.cc\/paper\/2020\/file\/eae15aabaa768ae4a5993a8a4f4fa6e4-Paper.pdf","publisher":"Curran Associates, Inc.","pages":"20378--20389","editor":"H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin","booktitle":"Advances in Neural Information Processing Systems"}}
{"bib_id":"michel_are_2019","title":"Are Sixteen Heads Really Better than One?","author":"Michel, Paul and Levy, Omer and Neubig, Graham","meta_info":{"file":"Full Text:C\\:\\\\Users\\χck\\\\Zotero\\\\storage\\\\PSMR23I2\\\\Michel et al. - 2019 - Are Sixteen Heads Really Better than One.pdf:application\/pdf","pages":"14014--14024","year":"2019","editor":"Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d\\textquotesingle and Fox, E. and Garnett, R.","publisher":"Curran Associates, Inc.","booktitle":"Advances in Neural Information Processing Systems","url":"https:\/\/proceedings.neurips.cc\/paper\/2019\/file\/2c601ad9d2ff9bc8b282670cdd54f69f-Paper.pdf","volume":"32"}}
{"bib_id":"sajjad_poor_2020","title":"On the effect of dropping layers of pre-trained transformer models","author":"Sajjad, Hassan and Dalvi, Fahim and Durrani, Nadir and Nakov, Preslav","meta_info":{"url":"https:\/\/www.sciencedirect.com\/science\/article\/pii\/S0885230822000596","publisher":"Elsevier","year":"2023","pages":"101429","volume":"77","journal":"Computer Speech & Language"}}
{"bib_id":"gordon_compressing_2020","title":"Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning","author":"Gordon, Mitchell  and\nDuh, Kevin  and\nAndrews, Nicholas","meta_info":{"pages":"143--155","doi":"10.18653\/v1\/2020.repl4nlp-1.18","url":"https:\/\/aclanthology.org\/2020.repl4nlp-1.18","publisher":"Association for Computational Linguistics","address":"Online","year":"2020","month":"July","booktitle":"Proceedings of the 5th Workshop on Representation Learning for NLP"}}
{"bib_id":"kim_i-bert_2021","title":"I-BERT: Integer-only BERT Quantization","author":"Kim, Sehoon and Gholami, Amir and Yao, Zhewei and Mahoney, Michael W. and Keutzer, Kurt","meta_info":{"url":"https:\/\/proceedings.mlr.press\/v139\/kim21d.html","pdf":"http:\/\/proceedings.mlr.press\/v139\/kim21d\/kim21d.pdf","publisher":"PMLR","month":"18--24 Jul","series":"Proceedings of Machine Learning Research","volume":"139","editor":"Meila, Marina and Zhang, Tong","year":"2021","pages":"5506--5518","booktitle":"Proceedings of the 38th International Conference on Machine Learning"}}
{"bib_id":"prato_fully_2020","title":"Fully Quantized Transformer for Machine Translation","author":"Prato, Gabriele  and\nCharlaix, Ella  and\nRezagholizadeh, Mehdi","meta_info":{"pages":"1--14","doi":"10.18653\/v1\/2020.findings-emnlp.1","url":"https:\/\/aclanthology.org\/2020.findings-emnlp.1","publisher":"Association for Computational Linguistics","address":"Online","year":"2020","month":"November","booktitle":"Findings of the Association for Computational Linguistics: EMNLP 2020"}}
{"bib_id":"beltagy_longformer_2020","title":"Longformer: The Long-Document Transformer","author":"Beltagy, Iz and Peters, Matthew E. and Cohan, Arman","meta_info":{"keywords":"Computer Science - Computation and Language","year":"2020","month":"April","journal":"arXiv preprint arXiv:2004.05150v2","urldate":"2020-11-13","url":"http:\/\/arxiv.org\/abs\/2004.05150v2","shorttitle":"Longformer"}}
{"bib_id":"child_generating_2019","title":"Generating Long Sequences with Sparse Transformers","author":"Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya","meta_info":{"keywords":"Computer Science - Machine Learning, Statistics - Machine Learning","year":"2019","month":"April","journal":"arXiv preprint arXiv:1904.10509v1","urldate":"2020-09-17","url":"http:\/\/arxiv.org\/abs\/1904.10509v1"}}
{"bib_id":"ji_distribution_2021","title":"On the Distribution, Sparsity, and Inference-time Quantization of Attention Values in Transformers","author":"Ji, Tianchu and Jain, Shraddhan and Ferdman, Michael and Milder, Peter and Schwartz, H. Andrew and Balasubramanian, Niranjan","meta_info":{"pages":"4147--4157","year":"2021","month":"August","publisher":"Association for Computational Linguistics","booktitle":"Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021","urldate":"2021-08-04","doi":"10.18653\/v1\/2021.findings-acl.363","url":"https:\/\/aclanthology.org\/2021.findings-acl.363","address":"Online"}}
