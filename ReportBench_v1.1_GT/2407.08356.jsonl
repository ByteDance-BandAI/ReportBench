{"bib_id":"Zhang_2022","title":"FPGA-Based Implementation of an Event-Driven Spiking Multi-Kernel Convolution Architecture","author":"Zhang, Jian and Feng, Lichen and Wang, Tengbo and Shi, Wei and Wang, Yuechao and Zhang, Guohe","meta_info":{"doi":"10.1109\/TCSII.2021.3126012","keywords":"Neurons;Convolution;Kernel;Program processors;Indexes;Frequency modulation;Field programmable gate arrays;Event-driven;spiking convolution neural network (SCNN);field programmable gate array (FPGA);parallel processing;hardware implementation","pages":"1682-1686","number":"3","volume":"69","year":"2022","journal":"IEEE Transactions on Circuits and Systems II: Express Briefs"}}
{"bib_id":"Liu_2022","title":"EDFLOW: Event Driven Optical Flow Camera With Keypoint Detection and Adaptive Block Matching","author":"Liu, Min and Delbruck, Tobi","meta_info":{"doi":"10.1109\/TCSVT.2022.3156653","keywords":"Optical sensors;Voltage control;Cameras;Optical saturation;Optical feedback;Hardware;Estimation;Dynamic vision sensor;FPGA;near-sensor processing","pages":"5776-5789","number":"9","volume":"32","year":"2022","journal":"IEEE Transactions on Circuits and Systems for Video Technology"}}
{"bib_id":"li_2022","title":"Eventor: an efficient event-based monocular multi-view stereo accelerator on FPGA platform","author":"Li, Mingjun and Yang, Jianlei and Qi, Yingjie and Dong, Meng and Yang, Yuhao and Liu, Runze and Pan, Weitao and Yu, Bei and Zhao, Weisheng","meta_info":{"series":"DAC '22","location":"San Francisco, California","keywords":"multi-view stereo, event-based vision, acceleration, FPGA","numpages":"6","pages":"331–336","booktitle":"Proceedings of the 59th ACM\/IEEE Design Automation Conference","abstract":"Event cameras are bio-inspired vision sensors that asynchronously represent pixel-level brightness changes as event streams. Event-based monocular multi-view stereo (EMVS) is a technique that exploits the event streams to estimate semi-dense 3D structure with known trajectory. It is a critical task for event-based monocular SLAM. However, the required intensive computation workloads make it challenging for real-time deployment on embedded platforms. In this paper, Eventor is proposed as a fast and efficient EMVS accelerator by realizing the most critical and time-consuming stages including event back-projection and volumetric ray-counting on FPGA. Highly paralleled and fully pipelined processing elements are specially designed via FPGA and integrated with the embedded ARM as a heterogeneous system to improve the throughput and reduce the memory footprint. Meanwhile, the EMVS algorithm is reformulated to a more hardware-friendly manner by rescheduling, approximate computing and hybrid data quantization. Evaluation results on DAVIS dataset show that Eventor achieves up to 24X improvement in energy efficiency compared with Intel i5 CPU platform.","doi":"10.1145\/3489517.3530452","url":"https:\/\/doi.org\/10.1145\/3489517.3530452","address":"New York, NY, USA","publisher":"Association for Computing Machinery","isbn":"9781450391429","year":"2022"}}
{"bib_id":"gao_2024","title":"A Composable Dynamic Sparse Dataflow Architecture for Efficient Event-based Vision Processing on FPGA","author":"Gao, Yizhao and Zhang, Baoheng and Ding, Yuhao and So, Hayden Kwok-Hay","meta_info":{"series":"FPGA '24","location":"<conf-loc>, <city>Monterey<\/city>, <state>CA<\/state>, <country>USA<\/country>, <\/conf-loc>","keywords":"event-based vision, event camera, sparse dnn accelerator, submanifold sparse convolution, dataflow accelerator, fpga","numpages":"12","pages":"246–257","booktitle":"Proceedings of the 2024 ACM\/SIGDA International Symposium on Field Programmable Gate Arrays","abstract":"Event-based vision represents a paradigm shift in how vision information is captured and processed. By only responding to dynamic intensity changes in the scene, event-based sensing produces far less data than conventional frame-based cameras, promising to springboard a new generation of high-speed, low-power machines for edge intelligence. However, processing such dynamically sparse input originated from event cameras efficiently in real time, particularly with complex deep neural networks (DNN), remains a formidable challenge. Existing solutions that employ GPUs and other frame-based DNN accelerators often struggle to efficiently process the dynamically sparse event data, missing the opportunities to improve processing efficiency with sparse data. To address this, we propose ESDA, a composable dynamic sparse dataflow architecture that allows customized DNN accelerators to be constructed rapidly on FPGAs for event-based vision tasks. ESDA is a modular system that is composed of a set of parametrizable modules for each network layer type. These modules share a uniform sparse token-feature interface and can be connected easily to compose an all-on-chip dataflow accelerator on FPGA for each network model. To fully exploit the intrinsic sparsity in event data, ESDA incorporates the use of submanifold sparse convolutions that largely enhance the activation sparsity throughout the layers while simplifying hardware implementation. Finally, a network architecture and hardware implementation co-optimizing framework that allows tradeoffs between accuracy and performance is also presented. Experimental results demonstrate that when compared with existing GPU and hardware-accelerated solutions, ESDA achieves substantial speedup and improvement in energy efficiency across different applications, and it allows much wider design space for real-world deployments.","doi":"10.1145\/3626202.3637558","url":"https:\/\/doi.org\/10.1145\/3626202.3637558","address":"New York, NY, USA","publisher":"Association for Computing Machinery","isbn":"9798400704185","year":"2024"}}
{"bib_id":"Pivezhandi_2020","title":"ParaHist: FPGA Implementation of Parallel Event-Based Histogram for Optical Flow Calculation","author":"Pivezhandi, Mohammad and Jones, Phillip H. and Zambreno, Joseph","meta_info":{"doi":"10.1109\/ASAP49362.2020.00038","keywords":"Histograms;Random access memory;Cameras;Computer architecture;Throughput;Field programmable gate arrays;Benchmark testing;Event-based camera sensors;FPGA-based architecture;Histogram generation","pages":"185-188","number":"","volume":"","year":"2020","booktitle":"2020 IEEE 31st International Conference on Application-specific Systems, Architectures and Processors (ASAP)"}}
{"bib_id":"Yu_2022","title":"An ultra-high-speed hardware accelerator for image reconstruction and stereo rectification on event-based camera","author":"Yu Zhang and Tao He and Lefeng Peng and Yi Chang and Kai Huang and Gang Chen","meta_info":{"abstract":"Event-based cameras are novel bio-inspired vision sensors, which sense brightness changes rather than the actual intensity level. In contrast to conventional cameras, such cameras capture new information about the scene in the form of sparse events at a very short latency. Recently, some researchers have studied on efficient event-based camera reconstruction approaches to obtain high-quality images. These efforts make performing stereo vision based on event-cameras being possible. However, traditional stereo architecture cannot process such high-speed image streams generated by the event-based camera in real-time (in the order of μs), which calls for new approaches. In this paper, we provide a set of practical solutions on visual image reconstruction and stereo rectification for spike camera, which serves two important procedures of stereo vision. We first provide FPGA-accelerated hardware architecture to achieve ultra-high-speed visual image reconstruction with full texture of natural scenes from spike data. Then, an FPGA-accelerated ultra-high-speed stereo rectification architecture is proposed to rectify the reconstructed images generated for stereo vision. In this architecture, a fully pipelined calculation module is designed to process the complex coordinate transformation operations, which puts in pipelined registers to maximize the clock frequency. To further improve the throughput, we propose a parallel processing architecture that uses multiple processing elements (PEs) to process multiple pixels per cycle. In addition, we design a memory management unit (MMU) to optimize the memory usage of the hardware resource. The whole architecture is effectively implemented on a Xilinx Zynq7100 FPGA chip. We evaluate the proposed architecture with different settings. The experiments show that our architecture can process the spike streams in real-time manner.","keywords":"Event-based camera, High-speed, Reconstruction, Stereo Rectification, FPGA","url":"https:\/\/www.sciencedirect.com\/science\/article\/pii\/S0026269221002949","doi":"https:\/\/doi.org\/10.1016\/j.mejo.2021.105312","issn":"0026-2692","year":"2022","pages":"105312","volume":"119","journal":"Microelectronics Journal"}}
{"bib_id":"Pantho_2022","title":"Event camera simulator design for modeling attention-based inference architectures","author":"Pantho, Md Jubaer Hossain\nand Mbongue, Joel Mandebi\nand Bhowmik, Pankaj\nand Bobda, Christophe","meta_info":{"url":"https:\/\/doi.org\/10.1007\/s11554-021-01191-y","doi":"10.1007\/s11554-021-01191-y","issn":"1861-8219","abstract":"In recent years, there has been a growing interest in realizing methodologies to integrate more and more computation at the level of the image sensor. The rising trend has seen an increased research interest in developing novel event cameras that can facilitate CNN computation directly in the sensor. However, event-based cameras ca be expensive, limiting performance exploration on high-level models and algorithms. This paper presents an event camera simulator that can be a potent tool for hardware design prototyping, parameter optimization, attention-based innovative algorithm development, and benchmarking. The proposed simulator implements a distributed computation model to identify relevant regions in an image frame. Our simulator's relevance computation model is realized as a collection of modules and performs computations in parallel. The distributed computation model is configurable, making it highly useful for design space exploration. The Rendering engine of the simulator samples frame-regions only when there is a new event. The simulator closely emulates an image processing pipeline similar to that of physical cameras. Our experimental results show that the simulator can effectively emulate event vision with low overheads","pages":"363-374","number":"2","volume":"19","day":"01","month":"Apr","year":"2022","journal":"Journal of Real-Time Image Processing"}}
{"bib_id":"Linares-Barranco_2021","title":"Dynamic Vision Sensor integration on FPGA-based CNN accelerators for high-speed visual classification","author":"Linares-Barranco, Alejandro and Rios-Navarro, Antonio and Canas-Moreno, Salvador and Piñero-Fuentes, Enrique and Tapiador-Morales, Ricardo and Delbruck, Tobi","meta_info":{"series":"ICONS 2021","location":"Knoxville, TN, to use fusion of event and vision data (but also from radar and lidar). This issue, although promising, has only been addressed in one paper i̧teLele_2022.\nıtem research into the implementation of modern AI methods, including GNNs and transofmeres, and further exploration of the topic of SNNs,\nıtem a greater focus on ‘straight-through’ data processing.\nıtem end-to-end systems in robotics: e.g. for cars, or autonomous drones,\nþe use of the latest generation of SoC FPGAs - e.g. the Versal\/ACAP series from Xilinx - which are equipped with resources dedicated to AI computing. USA","keywords":"Address-Event-Representation, FPGA, Neuromorphic Engineering, convolutional neural networks","numpages":"7","articleno":"21","booktitle":"International Conference on Neuromorphic Systems 2021","abstract":"Deep-learning is a cutting edge theory that is being applied to many fields. For vision applications the Convolutional Neural Networks (CNN) are demanding significant accuracy for classification tasks. Numerous hardware accelerators have populated during the last years to improve CPU or GPU based solutions. This technology is commonly prototyped and tested over FPGAs before being considered for ASIC fabrication for mass production. The use of commercial typical cameras (30fps) limits the capabilities of these systems for high speed applications. The use of dynamic vision sensors (DVS) that emulate the behaviour of a biological retina is taking an incremental importance to improve this applications due to its nature, where the information is represented by a continuous stream of spikes and the frames to be processed by the CNN are constructed collecting a fixed number of these spikes (called events). The faster an object is, the more events are produced by DVS, so the higher is the equivalent frame rate. Therefore, these DVS utilization allows to compute a frame at the maximum speed a CNN accelerator can offer. In this paper we present a VHDL\/HLS description of a pipelined design for FPGA able to collect events from an Address-Event-Representation (AER) DVS retina to obtain a normalized histogram to be used by a particular CNN accelerator, called NullHop. VHDL is used to describe the circuit, and HLS for computation blocks, which are used to perform the normalization of a frame needed for the CNN. Results outperform previous implementations of frames collection and normalization using ARM processors running at 800MHz on a Zynq7100 in both latency and power consumption. A measured 67% speed-up factor is presented for a Roshambo CNN real-time experiment running at &nbsp;160fps peak rate.","doi":"10.1145\/3477145.3477167","url":"https:\/\/doi.org\/10.1145\/3477145.3477167","address":"New York, NY, USA","publisher":"Association for Computing Machinery","isbn":"9781450386913","year":"2021"}}
{"bib_id":"Sengupta_2021","title":"Architecture and Algorithm Co-Design Framework for Embedded Processors in Event-Based Cameras","author":"Sengupta, Jonah P. and Villemur, Martin and Mendat, Daniel R. and Tognetti, Gaspar and Andreou, Andreas G.","meta_info":{"doi":"10.1109\/ISCAS51556.2021.9401246","keywords":"Visualization;Program processors;Neuromorphics;Heuristic algorithms;Computer architecture;Cameras;Object tracking;event-based vision processing;embedded computing;RISC-V;FPGA SoC","pages":"1-5","number":"","volume":"","year":"2021","booktitle":"2021 IEEE International Symposium on Circuits and Systems (ISCAS)"}}
{"bib_id":"Cladera_2020","title":"On-Device Event Filtering with Binary Neural Networks for Pedestrian Detection Using Neuromorphic Vision Sensors","author":"Cladera, Fernando and Bisulco, Anthony and Kepple, Daniel and Isler, Volkan and Lee, Daniel D.","meta_info":{"doi":"10.1109\/ICIP40778.2020.9191148","keywords":"Image sensors;Pedestrians;Power demand;Neuromorphics;Filtering;Neural networks;Vision sensors;Streaming media;Cameras;Field programmable gate arrays;dynamic vision sensors;binary neural networks;pedestrian detection;FPGA;embedded systems","pages":"3084-3088","number":"","volume":"","year":"2020","booktitle":"2020 IEEE International Conference on Image Processing (ICIP)"}}
{"bib_id":"Kim_2022","title":"Spiking Cooperative Network Implemented on FPGA for Real-Time Event-Based Stereo System","author":"Kim, Jung-Gyun and Seo, Donghwan and Lee, Byung-Geun","meta_info":{"doi":"10.1109\/ACCESS.2022.3228041","keywords":"Neurons;Field programmable gate arrays;Sensors;Cooperative systems;Hardware;Voltage control;Sensor systems;Event detection;Real-time system;Event-based vision;stereo;FPGA;event-driven computation;cooperative algorithm;spiking neuron;real-time processing","pages":"130806-130815","number":"","volume":"10","year":"2022","journal":"IEEE Access"}}
{"bib_id":"Xu_2023","title":"Taming Event Cameras with Bio-Inspired Architecture and Algorithm: A Case for Drone Obstacle Avoidance","author":"Xu, Jingao and Li, Danyang and Yang, Zheng and Zhao, Yishujie and Cao, Hao and Liu, Yunhao and Shangguan, Longfei","meta_info":{"numpages":"16","articleno":"55","booktitle":"Proceedings of the 29th Annual International Conference on Mobile Computing and Networking","abstract":"Fast and accurate obstacle avoidance is crucial to drone safety. Yet existing on-board sensor modules such as frame cameras and radars are ill-suited for doing so due to their low temporal resolution or limited field of view. This paper presents BioDrone, a new design paradigm for drone obstacle avoidance using stereo event cameras. At the heart of BioDrone is two simple yet effective system design inspired by the mammalian visual system, namely, a chiasm-inspired signal processing pipeline for fast event filtering and obstacle detection, and a lateral geniculate nucleus (LGN)-inspired event matching algorithm for accurate obstacle localization. To make BioDrone a practical solution, we further take significant engineering efforts to deploy the software stack on FPGA through software and hardware co-design. The performance comparison with two state-of-the-art event-based obstacle avoidance systems shows BioDrone achieves a consistently high obstacle detection rate of 96.1%. The average localization error of BioDrone is 6.8cm with a 4.7ms latency, outperforming both baselines by over 40%.","url":"https:\/\/doi.org\/10.1145\/3570361.3613269","address":"New York, NY, USA","publisher":"Association for Computing Machinery","isbn":"9781450399906","year":"2023"}}
{"bib_id":"Gao_2022","title":"REMOT: A Hardware-Software Architecture for Attention-Guided Multi-Object Tracking with Dynamic Vision Sensors on FPGAs","author":"Gao, Yizhao and Wang, Song and So, Hayden Kwok-Hay","meta_info":{"series":"FPGA '22","location":"Virtual Event, USA","keywords":"multi-object tracking, hota, hardware\/software co-design, fpga, event sensors, event camera, dynamic vision sensors, attention unit","numpages":"11","pages":"158–168","booktitle":"Proceedings of the 2022 ACM\/SIGDA International Symposium on Field-Programmable Gate Arrays","abstract":"In contrast to conventional vision sensors that produce images of the entire field-of-view at a fixed frame rate, dynamic vision sensors (DVS) are neuromorphic devices that only produce sparse events in response to changes in light intensity local to each pixel, making them promising technologies for use in demanding edge scenarios where energy-efficient intelligent computations are needed. While several early research have demonstrated promising results in performing high-level machine vision tasks using vision events only, these algorithms are often too complex for real-time deployments in edge systems with limited processing and storage capabilities. In this work, a novel hardware-software architecture, called REMOT, is proposed to leverage the unique properties of DVS to perform real-time multi-object tracking (MOT) on FPGAs. REMOT incorporates a parallel set of reconfigurable hardware attention units (AUs) that work in tandem with a modular attention-guided software framework running in the attached processor. Each hardware AU autonomously adjusts its region of attention by processing each vision event as they are produced by the DVS. Using information aggregated by the AUs, high-level analyses are performed in software. To demonstrate the flexibility and modularity of REMOT, a family of MOT algorithms with different hardware-software configurations and tradeoffs have been implemented on 2 different edge reconfigurable systems. Experimental results show that REMOT is capable of processing 0.43-2.22 million events per second at 1.75-5.68 watts, making them suitable for real-time operations while maintaining good MOT accuracy in our target datasets. When compared with a software-only implementation using the same edge platforms, our HW-SW implementation results in up to 33.6 times higher event processing throughput and 25.9 times higher power efficiency.","doi":"10.1145\/3490422.3502365","url":"https:\/\/doi.org\/10.1145\/3490422.3502365","address":"New York, NY, USA","publisher":"Association for Computing Machinery","isbn":"9781450391498","year":"2022"}}
{"bib_id":"Zhu_2022","title":"An FPGA Accelerator for High-Speed Moving Objects Detection and Tracking With a Spike Camera","author":"Zhu, Yaoyu and Zhang, Yu and Xie, Xiaodong and Huang, Tiejun","meta_info":{"eprint":"https:\/\/direct.mit.edu\/neco\/article-pdf\/34\/8\/1812\/2034912\/neco_a_01507.pdf","url":"https:\/\/doi.org\/10.1162\/neco_a_01507","doi":"10.1162\/neco_a_01507","issn":"0899-7667","abstract":"Ultra-high-speed object detection and tracking are crucial in fields such as fault detection and scientific observation. Existing solutions to this task have deficiencies in processing speeds. To deal with this difficulty, we propose a neural-inspired ultra-high-speed moving object filtering, detection, and tracking scheme, as well as a corresponding accelerator based on a high-speed spike camera. We parallelize the filtering module and divide the detection module to accelerate the algorithm and balance latency among modules for the benefit of the task-level pipeline. To be specific, a block-based parallel computation model is proposed to accelerate the filtering module, and the detection module is accelerated by a parallel connected component labeling algorithm modeling spike sparsity and spatial connectivity of moving objects with a searching tree. The hardware optimizations include processing the LIF layer with a group of multiplexers to reduce ADD operations and replacing expensive exponential operations with multiplications of preprocessed fixed-point values to increase processing speed and minimize resource consumption. We design an accelerator with the above techniques, achieving 19 times acceleration over the serial version after 25-way parallelization. A processing system for the accelerator is also implemented on the Xilinx ZCU-102 board to validate its functionality and performance. Our accelerator can process more than 20,000 spike images with 250 × 400 resolution per second with 1.618 W dynamic power consumption.","month":"07","year":"2022","pages":"1812-1839","number":"8","volume":"34","journal":"Neural Computation"}}
{"bib_id":"Gao_2023","title":"A Reconfigurable Architecture for Real-time Event-based Multi-Object Tracking","author":"Gao, Yizhao and Wang, Song and So, Hayden Kwok-Hay","meta_info":{"keywords":"REMOT, Dynamic Vision Sensors, multi-object tracking, event sensors, event camera, hardware\/software co-design, attention unit, FPGA, HOTA","numpages":"26","articleno":"58","month":"sep","journal":"ACM Trans. Reconfigurable Technol. Syst.","abstract":"Although advances in event-based machine vision algorithms have demonstrated unparalleled capabilities in performing some of the most demanding tasks, their implementations under stringent real-time and power constraints in edge systems remain a major challenge. In this work, a reconfigurable hardware-software architecture called REMOT, which performs real-time event-based multi-object tracking on FPGAs, is presented. REMOT performs vision tasks by defining a set of actions over attention units (AUs). These actions allow AUs to track an object candidate autonomously by adjusting its region of attention and allow information gathered by each AU to be used for making algorithmic-level decisions. Taking advantage of this modular structure, algorithm-architecture codesign can be performed by implementing different parts of the algorithm in either hardware or software for different tradeoffs. Results show that REMOT can process 0.43–2.91 million events per second at 1.75–5.45 W. Compared with the software baseline, our implementation achieves up to 44 times higher throughput and 35.4 times higher power efficiency. Migrating the Merge operation to hardware further reduces the worst-case latency to be 95 times shorter than the software baseline. By varying the AU configuration and operation, a reduction of 0.59–0.77 mW per AU on the programmable logic has also been demonstrated.","doi":"10.1145\/3593587","url":"https:\/\/doi.org\/10.1145\/3593587","issn":"1936-7406","number":"4","volume":"16","address":"New York, NY, USA","publisher":"Association for Computing Machinery","issue_date":"December 2023","year":"2023"}}
{"bib_id":"Lele_2022","title":"Fusing Frame and Event Vision for High-speed Optical Flow for Edge Application","author":"Lele, Ashwin Sanjay and Raychowdhury, Arijit","meta_info":{"doi":"10.1109\/ISCAS48785.2022.9937763","keywords":"Image motion analysis;Limiting;Tracking;Machine vision;Image edge detection;Pipelines;Cameras;Computer Vision;Dynamic Vision Sensors;Drone Tracking;Accuracy-speed trade-off","pages":"804-808","number":"","volume":"","year":"2022","booktitle":"2022 IEEE International Symposium on Circuits and Systems (ISCAS)"}}
{"bib_id":"Shi_2022","title":"A High-speed Event-based Object Tracking with Heterogeneous Computing Hardware","author":"Shi, Chenyang and Jin, Jing and Wei, Boyi and Li, Yuzhen and Li, Wenzhuo and Liu, Hanxiao and Zhang, Yibo and Luo, Shaobo","meta_info":{"doi":"10.1109\/APCCAS55924.2022.10090299","keywords":"Training;Power demand;Neuromorphic engineering;Computer architecture;Vision sensors;Cameras;Hardware;event-based vision;object tracking;heterogeneous computing;hardware implementation","pages":"394-399","number":"","volume":"","year":"2022","booktitle":"2022 IEEE Asia Pacific Conference on Circuits and Systems (APCCAS)"}}
{"bib_id":"Humenberger_2012","title":"Embedded fall detection with a neural network and bio-inspired stereo vision","author":"Humenberger, Martin and Schraml, Stephan and Sulzbachner, Christoph and Belbachir, Ahmed Nabil and Srp, Agoston and Vajda, Ferenc","meta_info":{"doi":"10.1109\/CVPRW.2012.6238896","keywords":"Training;Neural networks;Optical sensors;Vectors;Monitoring;Feature extraction","pages":"60-67","number":"","volume":"","year":"2012","booktitle":"2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops"}}
{"bib_id":"Belbachir_2012","title":"CARE: A dynamic stereo vision sensor system for fall detection","author":"Belbachir, A.N. and Litzenberger, M. and Schraml, S. and Hofstätter, M. and Bauer, D. and Schön, P. and Humenberger, M. and Sulzbachner, C. and Lunden, T. and Merne, M.","meta_info":{"doi":"10.1109\/ISCAS.2012.6272141","keywords":"Stereo vision;Field programmable gate arrays;Real time systems;Sensor systems;Digital signal processing;Voltage control;Arrays","pages":"731-734","number":"","volume":"","year":"2012","booktitle":"2012 IEEE International Symposium on Circuits and Systems (ISCAS)"}}
{"bib_id":"Perez-Pena_2012","title":"Neuro-Inspired Spike-Based Motion: From Dynamic Vision Sensor to Robot Motor Open-Loop Control through Spike-VITE","author":"Perez-Peña, Fernando and Morgado-Estevez, Arturo and Linares-Barranco, Alejandro and Jimenez-Fernandez, Angel and Gomez-Rodriguez, Francisco and Jimenez-Moreno, Gabriel and Lopez-Coronado, Juan","meta_info":{"doi":"10.3390\/s131115805","abstract":"In this paper we present a complete spike-based architecture: from a Dynamic Vision Sensor (retina) to a stereo head robotic platform. The aim of this research is to reproduce intended movements performed by humans taking into account as many features as possible from the biological point of view. This paper fills the gap between current spike silicon sensors and robotic actuators by applying a spike processing strategy to the data flows in real time. The architecture is divided into layers: the retina, visual information processing, the trajectory generator layer which uses a neuroinspired algorithm (SVITE) that can be replicated into as many times as DoF the robot has; and finally the actuation layer to supply the spikes to the robot (using PFM). All the layers do their tasks in a spike-processing mode, and they communicate each other through the neuro-inspired AER protocol. The open-loop controller is implemented on FPGA using AER interfaces developed by RTC Lab. Experimental results reveal the viability of this spike-based controller. Two main advantages are: low hardware resources (2% of a Xilinx Spartan 6) and power requirements (3.4 W) to control a robot with a high number of DoF (up to 100 for a Xilinx Spartan 6). It also evidences the suitable use of AER as a communication protocol between processing and actuation.","issn":"1424-8220","pubmedid":"24264330","url":"https:\/\/www.mdpi.com\/1424-8220\/13\/11\/15805","pages":"15805--15832","number":"11","year":"2013","volume":"13","journal":"Sensors"}}
{"bib_id":"Eibensteiner_2014","title":"A High-Performance Hardware Architecture for a Frameless Stereo Vision Algorithm Implemented on a FPGA Platform","author":"Eibensteiner, Florian and Kogler, Juergen and Scharinger, Josef","meta_info":{"doi":"10.1109\/CVPRW.2014.97","keywords":"Retina;Silicon;Image segmentation;Stereo vision;Hardware;History;Cameras","pages":"637-644","number":"","volume":"","year":"2014","booktitle":"2014 IEEE Conference on Computer Vision and Pattern Recognition Workshops"}}
{"bib_id":"Perez-Pena_2014","title":"Spike-based VITE control with Dynamic Vision Sensor applied to an Arm Robot","author":"Perez-Peña, Fernando and Morgado-Estevez, Arturo and Serrano-Gotarredona, Teresa and Gómez-Rodríguez, F. and Ferrer-García, V. and Jimenez-Fernandez, A. and Linares-Barranco, A.","meta_info":{"doi":"10.1109\/ISCAS.2014.6865171","keywords":"Robot sensing systems;Neurons;Computer architecture;Visualization;Joints;Field programmable gate arrays","pages":"463-466","number":"","volume":"","year":"2014","booktitle":"2014 IEEE International Symposium on Circuits and Systems (ISCAS)"}}
{"bib_id":"Camunas-Mesa_2014","title":"Event-driven sensing and processing for high-speed robotic vision","author":"Camuñas-Mesa, L. A. and Serrano-Gotarredona, T. and Linares-Barranco, B.","meta_info":{"doi":"10.1109\/BioCAS.2014.6981776","keywords":"Voltage control;Convolution;Gabor filters;Retina;Robot sensing systems;Field programmable gate arrays;Address Event Representation (AER);Event-driven vision;Event-driven processing;Event-driven convolutions;Gabor filters;High-speed vision;Bio-inspired vision","pages":"516-519","number":"","volume":"","year":"2014","booktitle":"2014 IEEE Biomedical Circuits and Systems Conference (BioCAS) Proceedings"}}
{"bib_id":"Linares-Barranco_2015","title":"A USB3.0 FPGA event-based filtering and tracking framework for dynamic vision sensors","author":"Linares-Barranco, A. and Gómez-Rodríguez, F. and Villanueva, V. and Longinotti, L. and Delbrück, T.","meta_info":{"doi":"10.1109\/ISCAS.2015.7169172","keywords":"Sensors;Voltage control;Field programmable gate arrays;Visualization;Computers;Retina;Hardware","pages":"2417-2420","number":"","volume":"","year":"2015","booktitle":"2015 IEEE International Symposium on Circuits and Systems (ISCAS)"}}
{"bib_id":"Rios-Navarro_2015","title":"Real-time motor rotation frequency detection with event-based visual and spike-based auditory AER sensory integration for FPGA","author":"Rios-Navarro, A. and Cerezuela-Escudero, E. and Dominguez-Morales, M. and Jimenez-Fernandez, A. and Jimenez-Moreno, G. and Linares-Barranco, A.","meta_info":{"doi":"10.1109\/EBCCSP.2015.7300696","keywords":"Sensors;Optical filters;Retina;Field programmable gate arrays;Hardware;Voltage control;Monitoring;Address-Event-Representation;spikebased filters;neuromorphic enginnering;event-based vision;DVS;silicon retina;sensor integration","pages":"1-6","number":"","volume":"","year":"2015","booktitle":"2015 International Conference on Event-based Control, Communication, and Signal Processing (EBCCSP)"}}
{"bib_id":"Moeys_2016","title":"Retinal ganglion cell software and FPGA model implementation for object detection and tracking","author":"Moeys, Diederik Paul and Delbrück, Tobias and Rios-Navarro, Antonio and Linares-Barranco, Alejandro","meta_info":{"doi":"10.1109\/ISCAS.2016.7527520","keywords":"Tracking;Field programmable gate arrays;Robot sensing systems;Retina;Cameras;Voltage control;Fires","pages":"1434-1437","number":"","volume":"","year":"2016","booktitle":"2016 IEEE International Symposium on Circuits and Systems (ISCAS)"}}
{"bib_id":"Liu_2017","title":"Block-matching optical flow for dynamic vision sensors: Algorithm and FPGA implementation","author":"Liu, Min and Delbruck, Tobi","meta_info":{"doi":"10.1109\/ISCAS.2017.8050295","keywords":"Voltage control;Field programmable gate arrays;Cameras;Random access memory;High definition video;Algorithm design and analysis;Software","pages":"1-4","number":"","volume":"","year":"2017","booktitle":"2017 IEEE International Symposium on Circuits and Systems (ISCAS)"}}
{"bib_id":"Liu_2_2017","title":"Neuromorphic Approach Sensitivity Cell Modeling and FPGA Implementation","author":"Hongjie Liu and Antonio Rios-Navarro and Diederik Paul Moeys and Tobi Delbrück and Alejandro Linares-Barranco","meta_info":{"url":"https:\/\/api.semanticscholar.org\/CorpusID:36714269","year":"2017","booktitle":"International Conference on Artificial Neural Networks"}}
{"bib_id":"Tapiador-Morales_2018","title":"Event-based Row-by-Row Multi-convolution engine for Dynamic-Vision Feature Extraction on FPGA","author":"Tapiador-Morales, Ricardo and Rios-Navarro, Antonio and Dominguez-Morales, Juan P. and Gutierrez-Galan, D. and Dominguez-Morales, M. and Jimenez-Fernandez, A. and Linares-Barranco, Alejandro","meta_info":{"doi":"10.1109\/IJCNN.2018.8489449","keywords":"Convolution;Engines;Kernel;Random access memory;Field programmable gate arrays;Convolutional Neural Networks;FPGA;computer vision;artificial intelligence;deep learning","pages":"1-7","number":"","volume":"","year":"2018","booktitle":"2018 International Joint Conference on Neural Networks (IJCNN)"}}
{"bib_id":"Linares_Barranco_2019","title":"Low Latency Event-Based Filtering and Feature Extraction for Dynamic Vision Sensors in Real-Time FPGA Applications","author":"Linares-Barranco, Alejandro and Perez-Peña, Fernando and Moeys, Diederik Paul and Gomez-Rodriguez, Francisco and Jimenez-Moreno, Gabriel and Liu, Shih-Chii and Delbruck, Tobi","meta_info":{"doi":"10.1109\/ACCESS.2019.2941282","keywords":"Feature extraction;Field programmable gate arrays;Sensors;Visualization;Voltage control;Real-time systems;Retina;Neuromorphic engineering;address-event-representation (AER);dynamic vision;frame-free vision;event-based processing;event-based filters;field programmable gate arrays (FPGA);VHDL","pages":"134926-134942","number":"","volume":"7","year":"2019","journal":"IEEE Access"}}
{"bib_id":"Aimar_2019","title":"NullHop: A Flexible Convolutional Neural Network Accelerator Based on Sparse Representations of Feature Maps","author":"Aimar, Alessandro and Mostafa, Hesham and Calabrese, Enrico and Rios-Navarro, Antonio and Tapiador-Morales, Ricardo and Lungu, Iulia-Alexandra and Milde, Moritz B. and Corradi, Federico and Linares-Barranco, Alejandro and Liu, Shih-Chii and Delbruck, Tobi","meta_info":{"doi":"10.1109\/TNNLS.2018.2852335","keywords":"Kernel;Training;Hardware;Feature extraction;Computer architecture;Graphics processing units;Pipelines;Artificial intelligence;computer vision;convolutional neural networks (CNNs);field-programmable gate array (FPGA);VLSI","pages":"644-656","number":"3","volume":"30","year":"2019","journal":"IEEE Transactions on Neural Networks and Learning Systems"}}
{"bib_id":"Aung_2018","title":"Event-based Plane-fitting Optical Flow for Dynamic Vision Sensors in FPGA","author":"Aung, Myo Tun and Teo, Rodney and Orchard, Garrick","meta_info":{"doi":"10.1109\/ISCAS.2018.8351588","keywords":"Optical sensors;Field programmable gate arrays;Optical imaging;Adaptive optics;Image edge detection;Biomedical optical imaging","pages":"1-5","number":"","volume":"","year":"2018","booktitle":"2018 IEEE International Symposium on Circuits and Systems (ISCAS)"}}
{"bib_id":"Hoseini_2018","title":"Real-Time Temporal Frequency Detection in FPGA Using Event-Based Vision Sensor","author":"Hoseini, Sahar and Linares-Barranco, Bernabe","meta_info":{"doi":"10.1109\/ICCP.2018.8516629","keywords":"Voltage control;Field programmable gate arrays;Feature extraction;Cameras;Hardware;Vision sensors;Protocols","pages":"271-278","number":"","volume":"","year":"2018","booktitle":"2018 IEEE 14th International Conference on Intelligent Computer Communication and Processing (ICCP)"}}
{"bib_id":"Domínguez_Morales_2019","title":"Stereo Matching in Address-Event-Representation (AER) Bio-Inspired Binocular Systems in a Field-Programmable Gate Array (FPGA)","author":"Domínguez-Morales, Manuel and Dominguez-Morales, Juan Pedro and Jiménez-Fernandez, Angel and Linares-Barranco, Alejandro and Jimenez, Gabriel","meta_info":{"doi":"10.3390\/electronics8040410","journal":"Electronics","volume":"8","pages":"410","month":"04","year":"2019"}}
{"bib_id":"Domínguez_Morales_2_2019","title":"Bio-Inspired Stereo Vision Calibration for Dynamic Vision Sensors","author":"Domínguez-Morales, Manuel J. and Jiménez-Fernández, Angel and Jiménez-Moreno, Gabriel and Conde, Cristina and Cabello, Enrique and Linares-Barranco, Alejandro","meta_info":{"doi":"10.1109\/ACCESS.2019.2943160","keywords":"Calibration;Retina;Field programmable gate arrays;Cameras;Hardware;Three-dimensional displays;Stereo vision;Neuromorphic engineering;dynamic vision sensor;bio-inspired systems;stereo vision;calibration","pages":"138415-138425","number":"","volume":"7","year":"2019","journal":"IEEE Access"}}
{"bib_id":"Rios-Navarro_2019","title":"Efficient DMA transfers management on embedded Linux PSoC for Deep-Learning gestures recognition: Using Dynamic Vision Sensor and NullHop one-layer CNN accelerator to play RoShamBo","author":"Rios-Navarro, Antonio and Tapiador-Morales, Ricardo and Jimenez-Moreno, Gabriel and Linares-Barranco, Alejandro","meta_info":{"series":"Interacción '19","location":"Donostia, Gipuzkoa, Spain","keywords":"hardware accelerator, Linux, FPGA, Deep-Learning, DMA, CNN","numpages":"2","articleno":"59","booktitle":"Proceedings of the XX International Conference on Human Computer Interaction","abstract":"This demonstration shows a Dynamic Vision Sensor able to capture visual motion at a speed equivalent to a high-speed camera (20k fps). The collected visual information is presented as normalized histogram to a CNN accelerator hardware, called NullHop, that is able to process a pre-trained CNN to play Roshambo against a human. The CNN designed for this purpose consist of 5 convolutional layers and a fully connected layer. The latency for processing one histogram is 8ms. NullHop is deployed on the FPGA fabric of a PSoC from Xilinx, the Zynq 7100, which is based on a dual-core ARM computer and a Kintex-7 with 444K logic cells, integrated in the same chip. ARM computer is running Linux and a specific C++ controller is running the whole demo. This controller runs at user space in order to extract the maximum throughput thanks to an efficient use of the AXIStream, based of DMA transfers. This short delay needed to process one visual histogram, allows us to average several consecutive classification outputs. Therefore, it provides the best estimation of the symbol that the user presents to the visual sensor. This output is then mapped to present the winner symbol within the 60ms latency that the brain considers acceptable before thinking that there is a trick.","doi":"10.1145\/3335595.3335597","url":"https:\/\/doi.org\/10.1145\/3335595.3335597","address":"New York, NY, USA","publisher":"Association for Computing Machinery","isbn":"9781450371766","year":"2019"}}
{"bib_id":"Ramesh_2020","title":"Low-Power Dynamic Object Detection and Classification With Freely Moving Event Cameras","author":"Ramesh, Bharath  and Ussa, Andrés  and Della Vedova, Luca  and Yang, Hong  and Orchard, Garrick ","meta_info":{"abstract":"<p>We present the first purely event-based, energy-efficient approach for dynamic object detection and categorization with a freely moving event camera. Compared to traditional cameras, event-based object recognition systems are considerably behind in terms of accuracy and algorithmic maturity. To this end, this paper presents an event-based feature extraction method devised by accumulating local activity across the image frame and then applying principal component analysis (PCA) to the normalized neighborhood region. Subsequently, we propose a backtracking-free <italic>k<\/italic>-d tree mechanism for efficient feature matching by taking advantage of the low-dimensionality of the feature representation. Additionally, the proposed <italic>k<\/italic>-d tree mechanism allows for feature selection to obtain a lower-dimensional object representation when hardware resources are limited to implement PCA. Consequently, the proposed system can be realized on a field-programmable gate array (FPGA) device leading to high performance over resource ratio. The proposed system is tested on real-world event-based datasets for object categorization, showing superior classification performance compared to state-of-the-art algorithms. Additionally, we verified the real-time FPGA performance of the proposed object detection method, trained with limited data as opposed to deep learning methods, under a closed-loop aerial vehicle flight mode. We also compare the proposed object categorization framework to pre-trained convolutional neural networks using transfer learning and highlight the drawbacks of using frame-based sensors under dynamic camera motion. Finally, we provide critical insights about the feature extraction method and the classification parameters on the system performance, which aids in understanding the framework to suit various low-power (less than a few watts) application scenarios.<\/p>","issn":"1662-453X","doi":"10.3389\/fnins.2020.00135","url":"https:\/\/www.frontiersin.org\/journals\/neuroscience\/articles\/10.3389\/fnins.2020.00135","year":"2020","volume":"14","journal":"Frontiers in Neuroscience"}}
{"bib_id":"He_2020","title":"A High-Speed Low-Cost VLSI System Capable of On-Chip Online Learning for Dynamic Vision Sensor Data Classification","author":"He, Wei and Huang, Jinguo and Wang, Tengxiao and Lin, Yingcheng and He, Junxian and Zhou, Xichuan and Li, Ping and Wang, Ying and Wu, Nanjian and Shi, Cong","meta_info":{"doi":"10.3390\/s20174715","abstract":"This paper proposes a high-speed low-cost VLSI system capable of on-chip online learning for classifying address-event representation (AER) streams from dynamic vision sensor (DVS) retina chips. The proposed system executes a lightweight statistic algorithm based on simple binary features extracted from AER streams and a Random Ferns classifier to classify these features. The proposed system’s characteristics of multi-level pipelines and parallel processing circuits achieves a high throughput up to 1 spike event per clock cycle for AER data processing. Thanks to the nature of the lightweight algorithm, our hardware system is realized in a low-cost memory-centric paradigm. In addition, the system is capable of on-chip online learning to flexibly adapt to different in-situ application scenarios. The extra overheads for on-chip learning in terms of time and resource consumption are quite low, as the training procedure of the Random Ferns is quite simple, requiring few auxiliary learning circuits. An FPGA prototype of the proposed VLSI system was implemented with 9.5~96.7% memory consumption and <11% computational and logic resources on a Xilinx Zynq-7045 chip platform. It was running at a clock frequency of 100 MHz and achieved a peak processing throughput up to 100 Meps (Mega events per second), with an estimated power consumption of 690 mW leading to a high energy efficiency of 145 Meps\/W or 145 event\/μJ. We tested the prototype system on MNIST-DVS, Poker-DVS, and Posture-DVS datasets, and obtained classification accuracies of 77.9%, 99.4% and 99.3%, respectively. Compared to prior works, our VLSI system achieves higher processing speeds, higher computing efficiency, comparable accuracy, and lower resource costs.","issn":"1424-8220","pubmedid":"32825560","url":"https:\/\/www.mdpi.com\/1424-8220\/20\/17\/4715","article-number":"4715","number":"17","year":"2020","volume":"20","journal":"Sensors"}}
{"bib_id":"Guo_2020","title":"HashHeat: An O(C) Complexity Hashing-based Filter for Dynamic Vision Sensor","author":"Guo, Shasha and Kang, Ziyang and Wang, Lei and Li, Shiming and Xu, Weixia","meta_info":{"doi":"10.1109\/ASP-DAC47756.2020.9045268","keywords":"Neuromorphics;Memory management;Vision sensors;Complexity theory;Spatiotemporal phenomena;Voltage control;Signal to noise ratio","pages":"452-457","number":"","volume":"","year":"2020","booktitle":"2020 25th Asia and South Pacific Design Automation Conference (ASP-DAC)"}}
{"bib_id":"DiMauro_2021","title":"FlyDVS: An Event-Driven Wireless Ultra-Low Power Visual Sensor Node","author":"Di Mauro, Alfio and Scherer, Moritz and Mas, Jordi Fornt and Bougenot, Basile and Magno, Michele and Benini, Luca","meta_info":{"doi":"10.23919\/DATE51398.2021.9474260","keywords":"Wireless communication;Wireless sensor networks;Visualization;Power demand;Data acquisition;Lattices;Cameras;Brain-Inspired Sensor;Event-based camera;Bluetooth Low Energy;Low power Design;FPGA;ULP;edge device","pages":"1851-1854","number":"","volume":"","year":"2021","booktitle":"2021 Design, Automation & Test in Europe Conference & Exhibition (DATE)"}}
{"bib_id":"Zhang_2021","title":"An FPGA-Accelerated Ultra-High-Speed Stereo Rectification for Event-Based Camera","author":"Zhang, Yu and Chen, Gang","meta_info":{"doi":"10.1109\/ICET51757.2021.9450900","keywords":"Memory management;Vision sensors;Streaming media;Throughput;Cameras;Real-time systems;Stereo vision;Event-camera;High-speed;Calibration;Stereo rectification;FPGA","pages":"158-162","number":"","volume":"","year":"2021","booktitle":"2021 IEEE 4th International Conference on Electronics Technology (ICET)"}}
{"bib_id":"Khodamoradi_2021","title":"$O(N)$O(N)-Space Spatiotemporal Filter for Reducing Noise in Neuromorphic Vision Sensors","author":"Khodamoradi, Alireza and Kastner, Ryan","meta_info":{"doi":"10.1109\/TETC.2017.2788865","keywords":"Spatiotemporal phenomena;Memory management;Correlation;Hardware;Neuromorphics;Complexity theory;Neuromorphic sensors;event based;noise;spatiotemporal filter;background activity","pages":"15-23","number":"1","volume":"9","year":"2021","journal":"IEEE Transactions on Emerging Topics in Computing"}}
{"bib_id":"Rios_Navarro_2023","title":"Within-Camera Multilayer Perceptron DVS Denoising","author":"A. Rios-Navarro and S. Guo and G. Abarajithan and K. Vijayakumar and A. Linares-Barranco and T. Aarrestad and R. Kastner and T. Delbruck","meta_info":{"month":"jun","address":"Los Alamitos, CA, USA","publisher":"IEEE Computer Society","url":"https:\/\/doi.ieeecomputersociety.org\/10.1109\/CVPRW59228.2023.00409","doi":"10.1109\/CVPRW59228.2023.00409","keywords":"power demand;costs;noise reduction;random access memory;multilayer perceptrons;cameras;hardware","abstract":"In-camera event denoising reduces the data rate of event cameras by filtering out noise at the source. A lightweight multilayer perceptron denoising filter (MLPF) provides state-of-the-art low-cost denoising accuracy. It processes a small neighborhood of pixels from the timestamp image around each event to discriminate signal and noise events. This paper proposes two digital logic implementations of the MLPF denoiser and quantifies their resource cost, power, and latency. The hardware MLPF quantizes the weights and hidden unit activations to 4 bits and has about 1k weights with about 40% sparsity. The Area-Under-Curve Receiver Operating Characteristic accuracy is nearly indistinguishable from that of the floating point network. The FPGA MLPF processes each event in 10 clock cycles. In FPGA, it uses 3.5k flip flops and 11.5k LUTs. Our ASIC implementation in 65nm digital technology for a 346 × 260 pixel camera occupies an area of 4.3mm2 and consumes 4nJ of energy per event at event rates up to 25MHz. The MLPF can be easily integrated into an event camera using an FPGA or as an ASIC directly on the camera chip or in the same package. This denoising could dramatically reduce the energy consumed by the communication and host processor and open new areas of always-on event camera application under scavenged and battery power.Code: https:\/\/github.com\/SensorsINI\/dnd_hls","pages":"3933-3942","issn":"","volume":"","year":"2023","booktitle":"2023 IEEE\/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"}}
{"bib_id":"Wu_2023","title":"Towards Asynchronously Triggered Spiking Neural Network on FPGA for Event-based Vision","author":"Wu, Zhenyu and Song, Mo and So, Hayden Kwok-Hay","meta_info":{"doi":"10.1109\/ICFPT59805.2023.00051","keywords":"Energy consumption;Protocols;Neurons;Vision sensors;Synchronization;Field programmable gate arrays;Biological neural networks;Spiking neural network;asynchronous circuit;FPGA;dynamic vision sensor;neuromorphic computing","pages":"292-293","number":"","volume":"","year":"2023","booktitle":"2023 International Conference on Field Programmable Technology (ICFPT)"}}
{"bib_id":"gallego2020event","title":"Event-based vision: A survey","author":"Gallego, Guillermo and Delbrück, Tobi and Orchard, Garrick and Bartolozzi, Chiara and Taba, Brian and Censi, Andrea and Leutenegger, Stefan and Davison, Andrew J and Conradt, Jörg and Daniilidis, Kostas and others","meta_info":{"publisher":"IEEE","year":"2020","pages":"154--180","number":"1","volume":"44","journal":"IEEE transactions on pattern analysis and machine intelligence"}}
{"bib_id":"Shariff_2024","title":"Event Cameras in Automotive Sensing: A Review","author":"Shariff, Waseem and Dilmaghani, Mehdi Sefidgar and Kielty, Paul and Moustafa, Mohamed and Lemley, Joe and Corcoran, Peter","meta_info":{"doi":"10.1109\/ACCESS.2024.3386032","keywords":"Cameras;Automotive engineering;Sensors;Hardware;Industries;Brightness;Vehicle dynamics;Event detection;Automotive engineering;Neuromorphic event camera;event based vision;driver monitoring system;out-of-cabin monitoring;in-cabin monitoring","pages":"51275-51306","number":"","volume":"12","year":"2024","journal":"IEEE Access"}}
{"bib_id":"Gandhare2019Survey","title":"Survey on FPGA Architecture and Recent Applications","author":"Gandhare, Shubham and Karthikeyan, B.","meta_info":{"doi":"10.1109\/ViTECoN.2019.8899550","keywords":"Field programmable gate arrays;Computer architecture;Routing;Application specific integrated circuits;Integrated circuit interconnections;Programming;Market research;Field Programmable Gate Array;Application specific integrated circuit;Non-recurrent engineering cost","pages":"1-4","number":"","volume":"","year":"2019","booktitle":"2019 International Conference on Vision Towards Emerging Trends in Communication and Networking (ViTECoN)"}}
{"bib_id":"Kowalczyk_2022","title":"Hardware architecture for high throughput event visual data filtering with matrix of IIR filters algorithm","author":"Kowalczyk, Marcin and Kryjak, Tomasz","meta_info":{"doi":"10.1109\/DSD57027.2022.00046","keywords":"Visualization;Filtering;Computer architecture;IIR filters;Vision sensors;Throughput;Hardware;FPGA;Zynq UltraScale+ MPSoC;dynamic vision sensor;DVS;event camera;filtering","pages":"284-291","number":"","volume":"","year":"2022","booktitle":"2022 25th Euromicro Conference on Digital System Design (DSD)"}}
{"bib_id":"Blachut_2023","title":"High-definition event frame generation using SoC FPGA devices","author":"Blachut, Krzysztof and Kryjak, Tomasz","meta_info":{"doi":"10.23919\/SPA59660.2023.10274447","keywords":"Time-frequency analysis;Heuristic algorithms;Machine vision;Random access memory;Signal processing algorithms;Artificial neural networks;Streaming media","pages":"106-111","number":"","volume":"","year":"2023","booktitle":"2023 Signal Processing: Algorithms, Architectures, Arrangements, and Applications (SPA)"}}
{"bib_id":"Jeziorek_2024","title":"Optimising Graph Representation for Hardware Implementation of Graph Convolutional Networks for Event-based Vision","author":"Kamil Jeziorek and Piotr Wzorek and Krzysztof Blachut and Andrea Pinna and Tomasz Kryjak","meta_info":{"primaryclass":"cs.CV","archiveprefix":"arXiv","eprint":"2401.04988","year":"2024 (Accepted for DASIP 2024)"}}
{"bib_id":"Bisulco_2020","title":"Near-Chip Dynamic Vision Filtering for Low-Bandwidth Pedestrian Detection","author":"Bisulco, Anthony and Cladera, Fernando and Isler, Volkan and Lee, Daniel Dongyuel","meta_info":{"doi":"10.1109\/ISVLSI49217.2020.00050","keywords":"Performance evaluation;Pedestrians;Microcontrollers;Neural networks;Systems architecture;Vision sensors;Very large scale integration;Internet of Things;Voltage control;Testing;dynamic vision sensors;binary neural networks;pedestrian detection;FPGA","pages":"234-239","number":"","volume":"","year":"2020","booktitle":"2020 IEEE Computer Society Annual Symposium on VLSI (ISVLSI)"}}
{"bib_id":"Barrios_Aviles_2018","title":"Less Data Same Information for Event-Based Sensors: A Bioinspired Filtering and Data Reduction Algorithm","author":"Barrios-Avilés, Juan and Rosado-Muñoz, Alfredo and Medus, Leandro D. and Bataller-Mompeán, Manuel and Guerrero-Martínez, Juan F.","meta_info":{"doi":"10.3390\/s18124122","abstract":"Sensors provide data which need to be processed after acquisition to remove noise and extract relevant information. When the sensor is a network node and acquired data are to be transmitted to other nodes (e.g., through Ethernet), the amount of generated data from multiple nodes can overload the communication channel. The reduction of generated data implies the possibility of lower hardware requirements and less power consumption for the hardware devices. This work proposes a filtering algorithm (LDSI—Less Data Same Information) which reduces the generated data from event-based sensors without loss of relevant information. It is a bioinspired filter, i.e., event data are processed using a structure resembling biological neuronal information processing. The filter is fully configurable, from a “transparent mode” to a very restrictive mode. Based on an analysis of configuration parameters, three main configurations are given: weak, medium and restrictive. Using data from a DVS event camera, results for a similarity detection algorithm show that event data can be reduced up to 30% while maintaining the same similarity index when compared to unfiltered data. Data reduction can reach 85% with a penalty of 15% in similarity index compared to the original data. An object tracking algorithm was also used to compare results of the proposed filter with other existing filter. The LDSI filter provides less error (4.86 ± 1.87) when compared to the background activity filter (5.01 ± 1.93). The algorithm was tested under a PC using pre-recorded datasets, and its FPGA implementation was also carried out. A Xilinx Virtex6 FPGA received data from a 128 × 128 DVS camera, applied the LDSI algorithm, created a AER dataflow and sent the data to the PC for data analysis and visualization. The FPGA could run at 177 MHz clock speed with a low resource usage (671 LUT and 40 Block RAM for the whole system), showing real time operation capabilities and very low resource usage. The results show that, using an adequate filter parameter tuning, the relevant information from the scene is kept while fewer events are generated (i.e., fewer generated data).","issn":"1424-8220","pubmedid":"30477237","url":"https:\/\/www.mdpi.com\/1424-8220\/18\/12\/4122","article-number":"4122","number":"12","year":"2018","volume":"18","journal":"Sensors"}}
{"bib_id":"Linares_Barranco_2018","title":"Approaching Retinal Ganglion Cell Modeling and FPGA Implementation for Robotics","author":"Linares-Barranco, Alejandro and Liu, Hongjie and Rios-Navarro, Antonio and Gomez-Rodriguez, Francisco and Moeys, Diederik P. and Delbruck, Tobi","meta_info":{"doi":"10.3390\/e20060475","abstract":"Taking inspiration from biology to solve engineering problems using the organizing principles of biological neural computation is the aim of the field of neuromorphic engineering. This field has demonstrated success in sensor based applications (vision and audition) as well as in cognition and actuators. This paper is focused on mimicking the approaching detection functionality of the retina that is computed by one type of Retinal Ganglion Cell (RGC) and its application to robotics. These RGCs transmit action potentials when an expanding object is detected. In this work we compare the software and hardware logic FPGA implementations of this approaching function and the hardware latency when applied to robots, as an attention\/reaction mechanism. The visual input for these cells comes from an asynchronous event-driven Dynamic Vision Sensor, which leads to an end-to-end event based processing system. The software model has been developed in Java, and computed with an average processing time per event of 370 ns on a NUC embedded computer. The output firing rate for an approaching object depends on the cell parameters that represent the needed number of input events to reach the firing threshold. For the hardware implementation, on a Spartan 6 FPGA, the processing time is reduced to 160 ns\/event with the clock running at 50 MHz. The entropy has been calculated to demonstrate that the system is not totally deterministic in response to approaching objects because of several bioinspired characteristics. It has been measured that a Summit XL mobile robot can react to an approaching object in 90 ms, which can be used as an attentional mechanism. This is faster than similar event-based approaches in robotics and equivalent to human reaction latencies to visual stimulus.","issn":"1099-4300","pubmedid":"33265565","url":"https:\/\/www.mdpi.com\/1099-4300\/20\/6\/475","article-number":"475","number":"6","year":"2018","volume":"20","journal":"Entropy"}}
{"bib_id":"Liu_2019","title":"Live Demonstration: A Real-Time Event-Based Fast Corner Detection Demo Based on FPGA","author":"Liu, Min and Kao, Wei-Tse and Delbruck, Tobi","meta_info":{"doi":"10.1109\/CVPRW.2019.00212","keywords":"Field programmable gate arrays;Corner detection;Portable computers;Cameras;Power demand;Optical sensors;Meters","pages":"1678-1679","number":"","volume":"","year":"2019","booktitle":"2019 IEEE\/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"}}
{"bib_id":"Tapiador_Morales_2019","title":"Neuromorphic LIF Row-by-Row Multiconvolution Processor for FPGA","author":"Tapiador-Morales, Ricardo and Linares-Barranco, Alejandro and Jimenez-Fernandez, Angel and Jimenez-Moreno, Gabriel","meta_info":{"doi":"10.1109\/TBCAS.2018.2880012","keywords":"Kernel;Engines;Convolutional codes;Neuromorphics;Convolution;Address-event-representation;artificial intelligence;computer vision;convolutional neural networks;deep learning;DVS;FPGA;neuromorphic engineering","pages":"159-169","number":"1","volume":"13","year":"2019","journal":"IEEE Transactions on Biomedical Circuits and Systems"}}
{"bib_id":"Tapiador_Morales_2_2019","title":"Spiking Row-by-Row FPGA Multi-Kernel and Multi-Layer Convolution Processor","author":"Tapiador Morales, Ricardo and Rios Navarro, Antonio and Dominguez Morales, Juan Pedro and Gutierrez Galan, Daniel and Linares-Barranco, Alejandro","meta_info":{"doi":"10.1109\/FPL.2019.00046","keywords":"Convolution;Field programmable gate arrays;Kernel;Engines;Neurons;Neuromorphics;Computer architecture;Spiking Convolutional Neural Networks, FPGA, Computer vision, Neuromorphic engineering","pages":"248-249","number":"","volume":"","year":"2019","booktitle":"2019 29th International Conference on Field Programmable Logic and Applications (FPL)"}}
{"bib_id":"Camunas_Mesa_2019","title":"Low-power hardware implementation of SNN with decision block for recognition tasks","author":"Camuñas-Mesa, Luis A. and Linares-Barranco, Bernabé and Serrano-Gotarredona, Teresa","meta_info":{"doi":"10.1109\/ICECS46596.2019.8964964","keywords":"Convolutional Neural Networks;Spiking Neural Networks;event-driven processing;neuromorphic vision","pages":"73-76","number":"","volume":"","year":"2019","booktitle":"2019 26th IEEE International Conference on Electronics, Circuits and Systems (ICECS)"}}
{"bib_id":"Asgari_2022","title":"FPGA Implementation of An Event-driven Saliency-based Selective Attention Model","author":"Asgari, Hajar and Risi, Nicoletta and Indiveri, Giacomo","meta_info":{"doi":"10.1109\/BioCAS54905.2022.9948687","keywords":"Visualization;Neuromorphics;Machine vision;Vision sensors;Feature extraction;Cameras;Hardware;Neuromorphic engineering;selective visual attention;event-driven;foveal vision;FPGA","pages":"307-311","number":"","volume":"","year":"2022","booktitle":"2022 IEEE Biomedical Circuits and Systems Conference (BioCAS)"}}
{"bib_id":"Ramesh_2019","title":"PCA-RECT: An Energy-Efficient Object Detection Approach for Event Cameras","author":"Ramesh, Bharath\nand Ussa, Andrés\nand Vedova, Luca Della\nand Yang, Hong\nand Orchard, Garrick","meta_info":{"isbn":"978-3-030-21074-8","abstract":"We present the first purely event-based, energy-efficient approach for object detection and categorization using an event camera. Compared to traditional frame-based cameras, choosing event cameras results in high temporal resolution (order of microseconds), low power consumption (few hundred mW) and wide dynamic range (120 dB) as attractive properties. However, event-based object recognition systems are far behind their frame-based counterparts in terms of accuracy. To this end, this paper presents an event-based feature extraction method devised by accumulating local activity across the image frame and then applying principal component analysis (PCA) to the normalized neighborhood region. Subsequently, we propose a backtracking-free k-d tree mechanism for efficient feature matching by taking advantage of the low-dimensionality of the feature representation. Additionally, the proposed k-d tree mechanism allows for feature selection to obtain a lower-dimensional dictionary representation when hardware resources are limited to implement dimensionality reduction. Consequently, the proposed system can be realized on a field-programmable gate array (FPGA) device leading to high performance over resource ratio. The proposed system is tested on real-world event-based datasets for object categorization, showing superior classification performance and relevance to state-of-the-art algorithms. Additionally, we verified the object detection method and real-time FPGA performance in lab settings under non-controlled illumination conditions with limited training data and ground truth annotations.","pages":"434--449","address":"Cham","publisher":"Springer International Publishing","year":"2019","booktitle":"Computer Vision -- ACCV 2018 Workshops","editor":"Carneiro, Gustavo\nand You, Shaodi"}}
{"bib_id":"Tapiador-Morales_2020","title":"Event-Based Gesture Recognition through a Hierarchy of Time-Surfaces for FPGA","author":"Tapiador-Morales, Ricardo and Maro, Jean-Matthieu and Jimenez-Fernandez, Angel and Jimenez-Moreno, Gabriel and Benosman, Ryad and Linares-Barranco, Alejandro","meta_info":{"doi":"10.3390\/s20123404","abstract":"Neuromorphic vision sensors detect changes in luminosity taking inspiration from mammalian retina and providing a stream of events with high temporal resolution, also known as Dynamic Vision Sensors (DVS). This continuous stream of events can be used to extract spatio-temporal patterns from a scene. A time-surface represents a spatio-temporal context for a given spatial radius around an incoming event from a sensor at a specific time history. Time-surfaces can be organized in a hierarchical way to extract features from input events using the Hierarchy Of Time-Surfaces algorithm, hereinafter HOTS. HOTS can be organized in consecutive layers to extract combination of features in a similar way as some deep-learning algorithms do. This work introduces a novel FPGA architecture for accelerating HOTS network. This architecture is mainly based on block-RAM memory and the non-restoring square root algorithm, requiring basic components and enabling it for low-power low-latency embedded applications. The presented architecture has been tested on a Zynq 7100 platform at 100 MHz. The results show that the latencies are in the range of 1 μ s to 6.7 μ s, requiring a maximum dynamic power consumption of 77 mW. This system was tested with a gesture recognition dataset, obtaining an accuracy loss for 16-bit precision of only 1.2% with respect to the original software HOTS.","issn":"1424-8220","pubmedid":"32560238","url":"https:\/\/www.mdpi.com\/1424-8220\/20\/12\/3404","article-number":"3404","number":"12","year":"2020","volume":"20","journal":"Sensors"}}
{"bib_id":"Ussa_2023","title":"A Hybrid Neuromorphic Object Tracking and Classification Framework for Real-Time Systems","author":"Ussa, Andrés and Rajen, Chockalingam Senthil and Pulluri, Tarun and Singla, Deepak and Acharya, Jyotibdha and Chuanrong, Gideon Fu and Basu, Arindam and Ramesh, Bharath","meta_info":{"doi":"10.1109\/TNNLS.2023.3243679","keywords":"Neuromorphics;Proposals;Cameras;Object tracking;Real-time systems;Field programmable gate arrays;Vision sensors;Event-based vision;FPGA implementation;IBM TrueNorth (TN);neuromorphic vision;object classification;object tracking","pages":"1-10","number":"","volume":"","year":"2023","journal":"IEEE Transactions on Neural Networks and Learning Systems"}}
{"bib_id":"Camunas_Mesa_2018","title":"A Configurable Event-Driven Convolutional Node with Rate Saturation Mechanism for Modular ConvNet Systems Implementation","author":"Camuñas-Mesa, Luis A.  and Domínguez-Cordero, Yaisel L.  and Linares-Barranco, Alejandro  and Serrano-Gotarredona, Teresa  and Linares-Barranco, Bernabé ","meta_info":{"abstract":"<p>Convolutional Neural Networks (ConvNets) are a particular type of neural network often used for many applications like image recognition, video analysis or natural language processing. They are inspired by the human brain, following a specific organization of the connectivity pattern between layers of neurons known as receptive field. These networks have been traditionally implemented in software, but they are becoming more computationally expensive as they scale up, having limitations for real-time processing of high-speed stimuli. On the other hand, hardware implementations show difficulties to be used for different applications, due to their reduced flexibility. In this paper, we propose a fully configurable event-driven convolutional node with rate saturation mechanism that can be used to implement arbitrary ConvNets on FPGAs. This node includes a convolutional processing unit and a routing element which allows to build large 2D arrays where any multilayer structure can be implemented. The rate saturation mechanism emulates the refractory behavior in biological neurons, guaranteeing a minimum separation in time between consecutive events. A 4-layer ConvNet with 22 convolutional nodes trained for poker card symbol recognition has been implemented in a Spartan6 FPGA. This network has been tested with a stimulus where 40 poker cards were observed by a Dynamic Vision Sensor (DVS) in 1 s time. Different slow-down factors were applied to characterize the behavior of the system for high speed processing. For slow stimulus play-back, a 96% recognition rate is obtained with a power consumption of 0.85 mW. At maximum play-back speed, a traffic control mechanism downsamples the input stimulus, obtaining a recognition rate above 63% when less than 20% of the input events are processed, demonstrating the robustness of the network.<\/p>","issn":"1662-453X","doi":"10.3389\/fnins.2018.00063","url":"https:\/\/www.frontiersin.org\/journals\/neuroscience\/articles\/10.3389\/fnins.2018.00063","year":"2018","volume":"12","journal":"Frontiers in Neuroscience"}}
{"bib_id":"Yousefzadeh_2015","title":"Fast Pipeline 128×128 pixel spiking convolution core for event-driven vision processing in FPGAs","author":"Yousefzadeh, A. and Serrano-Gotarredona, T. and Linares-Barranco, B.","meta_info":{"doi":"10.1109\/EBCCSP.2015.7300698","keywords":"Random access memory;Cameras;Field programmable gate arrays;Pipelines;Frequency modulation;Spiking Convolutional Neural Networks;DVS;Artificial Retina;FPGA;Parallel Processing","pages":"1-8","number":"","volume":"","year":"2015","booktitle":"2015 International Conference on Event-based Control, Communication, and Signal Processing (EBCCSP)"}}
{"bib_id":"Shawahna_2019","title":"FPGA-Based Accelerators of Deep Learning Networks for Learning and Classification: A Review","author":"Shawahna, Ahmad and Sait, Sadiq M. and El-Maleh, Aiman","meta_info":{"doi":"10.1109\/ACCESS.2018.2890150","keywords":"Deep learning;Field programmable gate arrays;Neural networks;Hardware;Acceleration;Convolution;Throughput;Adaptable architectures;convolutional neural networks (CNNs);deep learning;dynamic reconfiguration;energy-efficient architecture;field programmable gate arrays (FPGAs);hardware accelerator;machine learning;neural networks;optimization;parallel computer architecture;reconfigurable computing","pages":"7823-7859","number":"","volume":"7","year":"2019","journal":"IEEE Access"}}
{"bib_id":"Amir_2017","title":"A Low Power, Fully Event-Based Gesture Recognition System","author":"Amir, Arnon and Taba, Brian and Berg, David and Melano, Timothy and McKinstry, Jeffrey and Di Nolfo, Carmelo and Nayak, Tapan and Andreopoulos, Alexander and Garreau, Guillaume and Mendoza, Marcela and Kusnitz, Jeff and Debole, Michael and Esser, Steve and Delbruck, Tobi and Flickner, Myron and Modha, Dharmendra","meta_info":{"doi":"10.1109\/CVPR.2017.781","keywords":"Cameras;Neurons;Gesture recognition;Voltage control;Real-time systems;Sensors;Feature extraction","pages":"7388-7397","number":"","volume":"","year":"2017","booktitle":"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"}}
{"bib_id":"orchard2015converting","title":"Converting static image datasets to spiking neuromorphic datasets using saccades","author":"Orchard, Garrick and Jayawant, Ajinkya and Cohen, Gregory K and Thakor, Nitish","meta_info":{"publisher":"Frontiers","year":"2015","pages":"159859","volume":"9","journal":"Frontiers in neuroscience"}}
{"bib_id":"Zhai_2021","title":"Optical flow and scene flow estimation: A survey","author":"Mingliang Zhai and Xuezhi Xiang and Ning Lv and Xiangdong Kong","meta_info":{"abstract":"Motion analysis is one of the most fundamental and challenging problems in the field of computer vision, which can be widely applied in many areas, such as autonomous driving, action recognition, scene understanding, and robotics. In general, the displacement field between subsequent frames can be divided into two types: optical flow and scene flow. The optical flow represents the pixel motion of adjacent frames. In contrast, the scene flow is a 3D motion field of the dynamic scene between two frames. Traditional approaches for the estimation of optical flow and scene flow usually leverage the variational technique, which can be solved as an energy minimization process. In recent years, deep learning has emerged as a powerful technique for learning feature representations directly from data. It has led to remarkable progress in the field of optical flow and scene flow estimation. In this paper, we provide a comprehensive survey of optical flow and scene flow estimation. First, we briefly review the pioneering approaches that use variational technique and then we delve in detail into the deep learning-based approaches. Furthermore, we present insightful observations on evaluation issues, specifically benchmark datasets, evaluation metrics, and state-of-the-art performance. Finally, we give the promising directions for future research. To the best of our knowledge, we are the first to review both optical flow and scene flow estimation, and the first to cover both traditional and deep learning-based approaches.","keywords":"Motion analysis, Optical flow, Scene flow, Variational model, Deep learning, Convolutional neural networks (CNNs)","url":"https:\/\/www.sciencedirect.com\/science\/article\/pii\/S0031320321000480","doi":"https:\/\/doi.org\/10.1016\/j.patcog.2021.107861","issn":"0031-3203","year":"2021","pages":"107861","volume":"114","journal":"Pattern Recognition"}}
{"bib_id":"Pérez_Carrasco_2013","title":"Mapping from Frame-Driven to Frame-Free Event-Driven Vision Systems by Low-Rate Rate Coding and Coincidence Processing--Application to Feedforward ConvNets","author":"Pérez-Carrasco, José Antonio and Zhao, Bo and Serrano, Carmen and Acha, Begoña and Serrano-Gotarredona, Teresa and Chen, Shouchun and Linares-Barranco, Bernabé","meta_info":{"doi":"10.1109\/TPAMI.2013.71","keywords":"Neurons;Sensors;Voltage control;Visualization;Feature extraction;Neural networks;Dynamic range;Feature extraction;convolutional neural networks;object recognition;spiking neural networks;event-driven neural networks;bioinspired vision;high speed vision","pages":"2706-2719","number":"11","volume":"35","year":"2013","journal":"IEEE Transactions on Pattern Analysis and Machine Intelligence"}}
