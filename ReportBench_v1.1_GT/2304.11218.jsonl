{"bib_id":"stanford","title":"The 2022 AI Index Report - \nMeasuring trends in Artificial Intelligence","author":"Stanford Institute for Human-Centered Artificial Intelligence (HAI)","meta_info":{"url":"https:\/\/aiindex.stanford.edu\/report\/","year":"2022"}}
{"bib_id":"LisbonStrategy","title":"EU Lisbon Strategy","author":"European Council","meta_info":{"url":"https:\/\/www.europarl.europa.eu\/summits\/lis1_en.htm","year":"2000"}}
{"bib_id":"European_Data_Economy","title":"Communication from the Commission to the European Parliament, the Council, the European Economic and Social Committee and the Committee of the Regions: Building a European Data Economy","author":"European Commission","meta_info":{"year":"2017","url":"https:\/\/digital-strategy.ec.europa.eu\/en\/library\/communication-building-european-data-economy"}}
{"bib_id":"european2021fosterinapproachtoai","title":"Communication on Fostering a European approach to Artificial Intelligence","author":"European Commission","meta_info":{"url":"https:\/\/digital-strategy.ec.europa.eu\/en\/library\/communication-fostering-european-approach-artificial-intelligence","year":"2021"}}
{"bib_id":"european2021coordinatedplanonai2021review","title":"Coordinated Plan on Artificial Intelligence 2021 Review","author":"European Commission","meta_info":{"url":"https:\/\/digital-strategy.ec.europa.eu\/en\/library\/coordinated-plan-artificial-intelligence-2021-review","year":"2021"}}
{"bib_id":"europeanrollingplanforictstandard2022","title":"Rolling Plan for ICT Standardisation 2022","author":"European Commission","meta_info":{"url":"https:\/\/joinup.ec.europa.eu\/collection\/rolling-plan-ict-standardisation\/rolling-plan-2022","year":"2022"}}
{"bib_id":"gdpr","title":"Regulation (EU) 2016\/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95\/46\/EC (General Data Protection Regulation) (Text with EEA relevance)","author":"European Commission","meta_info":{"year":"2016","url":"https:\/\/eur-lex.europa.eu\/eli\/reg\/2016\/679\/oj","timestamp":"2020-08-20T11:33:21.000+0200","publisher":"European Commission"}}
{"bib_id":"aiact_original","title":"Proposal for a Regulation of the European Parliament and of the Council laying down harmonised rules on Artificial Intelligence (Artificial Intelligence Act) and amending certain Union legislative acts","author":"European Parliament and Council","meta_info":{"year":"2021-04-21"}}
{"bib_id":"aiact_generalapproach","title":"Proposal for a Regulation of the European Parliament and of the Council\nlaying down harmonised rules on artificial intelligence (Artificial Intelligence\nAct) and amending certain Union legislative acts - General Approach","author":"European Parliament and Council","meta_info":{"year":"2022-11-25"}}
{"bib_id":"ai_liability_direct","title":"Proposal for a Directive of the European Parliament and of the Council on adapting non-contractual civil liability rules to artificial intelligence\n(AI Liability Directive)","author":"European Parliament and Council","meta_info":{"url":"https:\/\/commission.europa.eu\/business-economy-euro\/doing-business-eu\/contract-rules\/digital-contracts\/liability-rules-artificial-intelligence_en","year":"2022-09-28"}}
{"bib_id":"digital_services_act","title":"Proposal for a regulation of the European Parliament and of the Council\non a Single Market For Digital Services (Digital Services Act) and amending Directive\n2000\/31\/EC","author":"European Parliament and Council","meta_info":{"year":"2020-12-15"}}
{"bib_id":"aiact_IMCO-LIBE","title":"Draft Report\non the proposal for a regulation of the European Parliament and of the Council\non harmonised rules on Artificial Intelligence (Artificial Intelligence Act) and\namending certain Union Legislative Acts","author":"IMCO-LIBE Committees of European Parliament","meta_info":{"year":"2022-04-20"}}
{"bib_id":"aiact_JURI","title":"Opinion\nof the Committee on Legal Affairs\nfor the Committee on the Internal Market and Consumer Protection and the\nCommittee on Civil Liberties, Justice and Home Affairs\non the proposal for a regulation of the European Parliament and of the Council\nlaying down harmonised rules on artificial intelligence (Artificial Intelligence\nAct) and amending certain Union Legislative Acts","author":"JURI Committee of European Parliament","meta_info":{"year":"2022-09-12"}}
{"bib_id":"EUJRCAIWatchStandLadnscape2021","title":"AI Watch, AI standardisation landscape state of play and link to the EC proposal for an AI regulatory framework","author":"European Commission and Joint Research Centre and Nativi, S and De Nigris, S","meta_info":{"doi":"doi\/10.2760\/376602","year":"2021","publisher":"Publications Office"}}
{"bib_id":"XAIdarpa","title":"Federal Contract Opportunity for Explainable Artificial Intelligence (XAI) DARPA-BAA-16-53","author":"Defense Advanced Research Projects Agency of United States of America","meta_info":{"url":"https:\/\/www.darpa.mil\/attachments\/DARPA-BAA-16-53.pdf","year":"2016-08-050"}}
{"bib_id":"blueprintforanaibillofrights","title":"Blueprint for an AI Bill of Rights: Making Automated Systems Work for the American People","author":"Office of Science and Technology Policy of the White House of United States of America","meta_info":{"url":"https:\/\/www.whitehouse.gov\/ostp\/ai-bill-of-rights\/","year":"2022"}}
{"bib_id":"abdul2018trends","title":"Trends and Trajectories for Explainable, Accountable and Intelligible\nSystems: An HCI Research Agenda","author":"Ashraf M. Abdul and\nJo Vermeulen and\nDanding Wang and\nBrian Y. Lim and\nMohan S. Kankanhalli","meta_info":{"bibsource":"dblp computer science bibliography, https:\/\/dblp.org","biburl":"https:\/\/dblp.org\/rec\/conf\/chi\/AbdulVWLK18.bib","timestamp":"Fri, 12 Mar 2021 15:28:42 +0100","doi":"10.1145\/3173574.3174156","url":"https:\/\/doi.org\/10.1145\/3173574.3174156","year":"2018","publisher":"ACM","pages":"582","booktitle":"Proceedings of the 2018 CHI Conference on Human Factors in Computing\nSystems, CHI 2018, Montreal, QC, Canada, April 21-26, 2018","editor":"Regan L. Mandryk and\nMark Hancock and\nMark Perry and\nAnna L. Cox"}}
{"bib_id":"holzinger2018current","title":"Current advances, trends and challenges of machine learning and knowledge extraction: from machine learning to explainable AI","author":"Holzinger, Andreas and Kieseberg, Peter and Weippl, Edgar and Tjoa, A Min","meta_info":{"organization":"Springer","year":"2018","pages":"1--8","booktitle":"Machine Learning and Knowledge Extraction: Second IFIP TC 5, TC 8\/WG 8.4, 8.9, TC 12\/WG 12.9 International Cross-Domain Conference, CD-MAKE 2018, Hamburg, Germany, August 27--30, 2018, Proceedings 2"}}
{"bib_id":"fromprinciplestopracticeaibillofrights","title":"From Principles to Practice: a Technical Companion to the Blueprint for an AI Bill of Rights","author":"Office of Science and Technology Policy of the White House of United States of America","meta_info":{"url":"https:\/\/www.whitehouse.gov\/ostp\/ai-bill-of-rights\/from-principles-to-practice\/","year":"2022"}}
{"bib_id":"MaintainingAmericanLeadershipinArtificialIntelligence","title":"Maintaining American Leadership in Artificial Intelligence - Executive Order 13859 of February 11, 2019","author":"Executive Office of the President of United States of America","meta_info":{"url":"https:\/\/www.federalregister.gov\/d\/2019-02544","year":"2019"}}
{"bib_id":"GuidanceforRegulationofAIapplications","title":"Guidance for Regulation of Artificial Intelligence Applications","author":"Executive Office of the President of United States of America - Office of Management and Budget","meta_info":{"url":"https:\/\/www.whitehouse.gov\/wp-content\/uploads\/2020\/11\/M-21-06.pdf","year":"2020"}}
{"bib_id":"NSCAI2021FinalReport","title":"Final Report","author":"National Security Commission on Artificial Intelligence of United States of America","meta_info":{"url":"https:\/\/www.nscai.gov\/2021-final-report\/","year":"2021"}}
{"bib_id":"NIST-AIRMF1.0","title":"Risk Management Framework v1.0","author":"National Institute of Standards and Technology (NIST) of United States of America","meta_info":{"language":"en","doi":"https:\/\/doi.org\/10.6028\/NIST.AI.100-1","url":"https:\/\/www.nist.gov\/news-events\/news\/2023\/01\/nist-risk-management-framework-aims-improve-trustworthiness-artificial","publisher":"National Institute of Standards and Technology (NIST), Gaithersburg, MD","year":"2023"}}
{"bib_id":"USAnationalAIiniatitiveAct","title":"National Artificial Intelligence Initiative Act of 2020","author":"116th Congress of the United States of America","meta_info":{"url":"https:\/\/www.congress.gov\/116\/crpt\/hrpt617\/CRPT-116hrpt617.pdf#page=1210","year":"2020"}}
{"bib_id":"USAAlgoAccountAct2022","title":"Algorithmic Accountability Act of 2022, H.R.6580","author":"117th Congress of the United States of America","meta_info":{"url":"https:\/\/www.congress.gov\/bill\/117th-congress\/house-bill\/6580\/text","year":"2022"}}
{"bib_id":"UkNatDataStrategy2019","title":"National Data Strategy","author":"Department for Digital and Culture and Media & Sport of the United Kingdom","meta_info":{"url":"https:\/\/www.gov.uk\/guidance\/national-data-strategy","year":"2019"}}
{"bib_id":"UkAISectorDeal2019","title":"AI Sector Deal - Policy paper","author":"Department for Digital and Culture and Media & Sport and Department for Business and Energy & Industrial Strategy of the United Kingdom","meta_info":{"url":"https:\/\/www.gov.uk\/government\/publications\/artificial-intelligence-sector-deal\/ai-sector-deal","year":"2019"}}
{"bib_id":"UK2021EthicsTransparency","title":"Ethics, Transparency and Accountability Framework for Automated Decision-Making - Guidance","author":"Cabinet Office & Central Digital & Data Office & Office for Artificial Intelligence","meta_info":{"url":"https:\/\/www.gov.uk\/government\/publications\/ethics-transparency-and-accountability-framework-for-automated-decision-making\/ethics-transparency-and-accountability-framework-for-automated-decision-making","year":"2021"}}
{"bib_id":"UK2021algotransaprencyrecordingstandard","title":"Algorithmic Transparency Recording Standard - Guidance","author":"Central Digital & Data Office & Centre for Data Ethics & Innovation","meta_info":{"url":"https:\/\/www.gov.uk\/government\/publications\/algorithmic-transparency-template","year":"2021"}}
{"bib_id":"UkEstablishingaproinnovationregAI2022","title":"Establishing a pro-innovation approach to regulating AI - Policy paper presented to UK Parliament","author":"Department for Digital and Culture and Media & Sport and Department for Business and Energy & Industrial Strategy and Office for Artificial Intelligence of the United Kingdom","meta_info":{"url":"https:\/\/www.gov.uk\/government\/publications\/establishing-a-pro-innovation-approach-to-regulating-ai\/establishing-a-pro-innovation-approach-to-regulating-ai-policy-statement#fnref:35","year":"2022"}}
{"bib_id":"UkNAtionalAIStrategyActionPlan2022","title":"National AI Strategy - AI Action Plan","author":"Department for Digital and Culture and Media & Sport and Department for Business and Energy & Industrial Strategy and Office for Artificial Intelligence of the United Kingdom","meta_info":{"url":"https:\/\/www.gov.uk\/government\/publications\/national-ai-strategy-ai-action-plan\/national-ai-strategy-ai-action-plan","year":"2022"}}
{"bib_id":"projectexplain","title":"Project ExplAIn - Interim Report","author":"Information Commissioner's Office (ICO) of the United Kingdom and The Alan Turing Institute","meta_info":{"url":"https:\/\/ico.org.uk\/media\/about-the-ico\/documents\/2615039\/project-explain-20190603.pdf","year":"2019"}}
{"bib_id":"ICOGuidanceontheAIauditingframe2020","title":"Guidance on the AI auditing framework: Draft guidance for consultation","author":"Information Commissioner's Office (ICO) of the United Kingdom","meta_info":{"url":"https:\/\/ico.org.uk\/media\/2617219\/guidance-on-the-ai-auditing-framework-draft-for-consultation.pdf","year":"2020"}}
{"bib_id":"ICOGuidanceontheAIanddataprotection2020","title":"Guidance on AI and data protection","author":"Information Commissioner's Office (ICO) of the United Kingdom","meta_info":{"url":"https:\/\/ico.org.uk\/for-organisations\/guide-to-data-protection\/key-dp-themes\/guidance-on-artificial-intelligence-and-data-protection\/","year":"2020"}}
{"bib_id":"ICOExplainingdecisionsmadewithAI2020","title":"Explaining decisions made with AI","author":"Information Commissioner's Office (ICO) of the United Kingdom and The Alan Turing Institute","meta_info":{"url":"https:\/\/ico.org.uk\/for-organisations\/guide-to-data-protection\/key-dp-themes\/explaining-decisions-made-with-ai\/","year":"2020"}}
{"bib_id":"AlanTuringCommonRegCapAI","title":"Common Regulatory Capacity for AI","author":"Aitken, Mhairi and Leslie, David and Ostmann, Florian and Pratt, Jacob and Margetts, Helen and Dorobantu, Cosmina","meta_info":{"doi":"https:\/\/doi.org\/10.5281\/zenodo.6838946","year":"2022","publisher":"The Alan Turing Institute"}}
{"bib_id":"Ukdataprotectionact2020","title":"Data Protection Act 2018","author":"Parliament of the United Kingdom","meta_info":{"url":"https:\/\/www.legislation.gov.uk\/ukpga\/2018\/12\/contents\/enacted","year":"2018"}}
{"bib_id":"etsi007","title":"DGR\/SAI-007' Work Item: Explicability and transparency of AI processing\n","author":"European Telecommunications Standards Institute (ETSI), Securing Artificial Intelligence (SAI)","meta_info":{"year":"2023","howpublished":"r̆lhttps:\/\/portal.etsi.org\/webapp\/WorkProgram\/Report_WorkItem.asp?WKI_ID=63078"}}
{"bib_id":"etsieni005","title":"ETSI GS ENI 005 V2.1.1 - Experiential Networked Intelligence (ENI), System Architecture\n","author":"European Telecommunications Standards Institute (ETSI), Experiential Networked Intelligence (ENI)","meta_info":{"year":"2021","howpublished":"r̆lhttps:\/\/www.etsi.org\/deliver\/etsi_gs\/ENI\/001_099\/005\/02.01.01_60\/gs_ENI005v020101p.pdf"}}
{"bib_id":"ETSIGRSAI","title":"ETSI Securing Artificial Intelligence (SAI) Committee","author":"European Telecommunications Standards Institute (ETSI)","meta_info":{"year":"2019","howpublished":"https:\/\/www.etsi.org\/technologies\/securing-artificial-intelligence"}}
{"bib_id":"protocolalgo","title":"This Senate bill would force companies to audit AI used for housing and loans","author":"Kate Kaye","meta_info":{"year":"2022","howpublished":"https:\/\/www.protocol.com\/enterprise\/revised-algorithmic-accountability-bill-ai"}}
{"bib_id":"FTCtrade","title":"Trade Regulation Rule on Commercial Surveillance and Data Security (Billing Code: 6750-01-P)","author":"Federal Trade Commission of the United States of America","meta_info":{"year":"2022","howpublished":"https:\/\/www.federalregister.gov\/documents\/2022\/08\/22\/2022-17752\/trade-regulation-rule-on-commercial-surveillance-and-data-security"}}
{"bib_id":"etsiactivitiesaia","title":"ETSI Activities in the field of Artificial Intelligence - Preparing the implementation of the European AI Act","author":"Mueck, Markus and Forbes, Raymond and Cadzow, Scott and Wood, Suno and Gazis, Evangelos, European Telecommunications Standards Institute (ETSI)","meta_info":{"year":"2022","howpublished":"r̆lhttps:\/\/www.etsi.org\/images\/files\/ETSIWhitePapers\/ETSI-WP52-ETSI-activities-in-the-field-of-AI.pdf"}}
{"bib_id":"ecdraftstandardisationrequest","title":"Draft standardisation request to the European Standardisation Organisations in support of safe and trustworthy artificial intelligence","author":"European Commission -  Directorate-General for Internal Market, Industry, Entrepreneurship and SMEs","meta_info":{"year":"2022","url":"https:\/\/ec.europa.eu\/docsroom\/documents\/52376"}}
{"bib_id":"cencenelec-ai","title":"Joint Technical Committee 21 (JTC 21) ‘Artificial Intelligence’","author":"CEN-CENELEC","meta_info":{"note":"Accessed on: 2023-01-30","year":"2021","url":"https:\/\/www.cencenelec.eu\/areas-of-work\/cen-cenelec-topics\/artificial-intelligence\/"}}
{"bib_id":"cencenelecroadmap","title":"Focus Group Report - Road Map on Artificial Intelligence (AI)","author":"CEN-CENELEC","meta_info":{"note":"Accessed on: 2023-01-30","year":"2020","url":"https:\/\/www.standict.eu\/node\/4854"}}
{"bib_id":"NIST-8312","title":"Four Principles of Explainable Artificial Intelligence","author":"P. Jonathon Phillips and Carina Hahn and Peter Fontana and Amy Yates and Kristen K. Greene and David Broniatowski and Mark A. Przybocki","meta_info":{"language":"en","doi":"https:\/\/doi.org\/10.6028\/NIST.IR.8312","url":"https:\/\/tsapps.nist.gov\/publication\/get_pdf.cfm?pub_id=933399","publisher":"NIST Interagency\/Internal Report (NISTIR), National Institute of Standards and Technology, Gaithersburg, MD","month":"2021-09-29 04:09:00","year":"2021"}}
{"bib_id":"NIST-8367","title":"Psychological Foundations of Explainability and Interpretability in Artificial Intelligence","author":"David Broniatowski","meta_info":{"language":"en","doi":"https:\/\/doi.org\/10.6028\/NIST.IR.8367","url":"https:\/\/tsapps.nist.gov\/publication\/get_pdf.cfm?pub_id=931426","publisher":"NIST Interagency\/Internal Report (NISTIR), National Institute of Standards and Technology, Gaithersburg, MD","month":"2021-04-12 04:04:00","year":"2021"}}
{"bib_id":"NISTFedEng","title":"U.S. Leadership in AI: A Plan for Federal Engagement in Developing Technical Standards and Related Tools - Prepared in response to Executive Order 13859 Submitted on August 9, 2019","author":"National Institute of Standards and Technology (NIST) of United States of America","meta_info":{"language":"en","url":"https:\/\/www.nist.gov\/news-events\/news\/2019\/08\/plan-outlines-priorities-federal-agency-engagement-ai-standards-development","publisher":"National Institute of Standards and Technology, Gaithersburg, MD","year":"2021"}}
{"bib_id":"NISTxaiWG","title":"Artificial Intelligence: AI Fundamental Research - Explainability","author":"National Institute of Standards and Technology (NIST) of United States of America","meta_info":{"language":"en","url":"https:\/\/www.nist.gov\/artificial-intelligence\/ai-fundamental-research-explainability","publisher":"National Institute of Standards and Technology, Gaithersburg, MD","year":"2021"}}
{"bib_id":"ISOIECAWITS6254","title":"ISO\/IEC AWI TS 6254 -Information technology — Artificial intelligence — Objectives and approaches for explainability of ML models and AI systems","author":"International Standards Association (ISO), Securing Artificial Intelligence (SAI)","meta_info":{"year":"2023","howpublished":"r̆lhttps:\/\/www.iso.org\/standard\/82148.html"}}
{"bib_id":"ISOIECTRR240282020","title":"ISO\/IEC TR 24028:2020 - \nInformation technology — Artificial intelligence — Overview of trustworthiness in artificial intelligence","author":"International Standards Association (ISO)","meta_info":{"year":"2020","howpublished":"r̆lhttps:\/\/www.iso.org\/standard\/77608.html"}}
{"bib_id":"ISOIECAWI12792","title":"ISO\/IEC AWI 12792 - \nInformation technology — Artificial intelligence — Transparency taxonomy of AI systems","author":"International Standards Association (ISO)","meta_info":{"year":"2025","howpublished":"r̆lhttps:\/\/www.iso.org\/standard\/84111.html"}}
{"bib_id":"ISOIECJTC1SC42","title":"ISO\/IEC JTC 1\/SC 42 - Artificial intelligence Committee\n","author":"International Standards Association (ISO), JTC 1 \/SC 42 Artificial Intelligence","meta_info":{"year":"2017","howpublished":"r̆lhttps:\/\/www.iso.org\/committee\/6794475.html"}}
{"bib_id":"IEEEP2976","title":"IEEE CIS\/SC\/XAI WG P2976 - Standard for XAI – eXplainable Artificial Intelligence - for Achieving Clarity and Interoperability of AI Systems Design","author":"Standard for XAI - eXplainable AI Working Group IEEE Computational Intelligence Society\/ Standards Committee (IEEE CIS\/SC\/XAI WG)","meta_info":{"year":"2024","howpublished":"r̆lhttps:\/\/standards.ieee.org\/ieee\/2976\/10522\/"}}
{"bib_id":"IEEE70012021","title":"IEEE 7001-2021 - Standard for Transparency of Autonomous Systems","author":"Intelligent Transporation Systems IEEE Vehicular Technology Society (IEEE VT\/ITS)","meta_info":{"year":"2022","howpublished":"r̆lhttps:\/\/standards.ieee.org\/ieee\/7001\/6929\/"}}
{"bib_id":"IEEEP2894","title":"IEEE P2894 - Guide for an Architectural Framework for Explainable Artificial Intelligence","author":"Artificial Intelligence Standards Committee, XAI - Explainable Artificial Intelligence IEEE Computer Society (IEEE C\/AISC\/XAI)","meta_info":{"year":"2020","howpublished":"r̆lhttps:\/\/standards.ieee.org\/ieee\/2894\/10284\/"}}
{"bib_id":"IEEEP7003","title":"IEEE P7003 - Algorithmic Bias Considerations","author":"Software & Systems Engineering Standards Committee, Algorithmic Bias Working Group IEEE Computer Society (IEEE C\/S2ESC\/ALGB-WG)","meta_info":{"year":"2017","howpublished":"r̆lhttps:\/\/standards.ieee.org\/ieee\/7003\/6980\/"}}
{"bib_id":"partnership","title":"The Partnership on AI","author":"Heer, Jeffrey","meta_info":{"numpages":"2","pages":"25–26","month":"oct","journal":"AI Matters","doi":"10.1145\/3284751.3284760","url":"https:\/\/doi-org.ezbusc.usc.gal\/10.1145\/3284751.3284760","number":"3","volume":"4","address":"New York, NY, USA","publisher":"Association for Computing Machinery","issue_date":"October 2018","year":"2018"}}
{"bib_id":"CAIDPIndex2022","title":"Artificial Intelligence and Democratic Values Index - February, 2022","author":"Center for AI and Digital Policy (CAIDP)","meta_info":{"isbn":"979-8-9857883-0-3","url":"https:\/\/caidp.org\/reports\/aidv-2021\/","year":"2022"}}
{"bib_id":"sokol2020explainability","title":"Explainability fact sheets: a framework for systematic assessment\nof explainable approaches","author":"Kacper Sokol and\nPeter A. Flach","meta_info":{"bibsource":"dblp computer science bibliography, https:\/\/dblp.org","biburl":"https:\/\/dblp.org\/rec\/conf\/fat\/SokolF20.bib","timestamp":"Thu, 26 Aug 2021 22:19:24 +0200","doi":"10.1145\/3351095.3372870","url":"https:\/\/doi.org\/10.1145\/3351095.3372870","year":"2020","publisher":"ACM","pages":"56--67","booktitle":"FAT* '20: Conference on Fairness, Accountability, and Transparency,\nBarcelona, Spain, January 27-30, 2020","editor":"Mireille Hildebrandt and\nCarlos Castillo and\nL. Elisa Celis and\nSalvatore Ruggieri and\nLinnet Taylor and\nGabriela Zanfir-Fortuna"}}
{"bib_id":"Elishanddanahboyd","title":"Situating methods in the magic of Big Data and AI","author":"M. C. Elish and danah boyd","meta_info":{"url":" \nhttps:\/\/doi.org\/10.1080\/03637751.2017.1375130\n","doi":"10.1080\/03637751.2017.1375130","publisher":"Routledge","year":"2018","pages":"57-80","number":"1","volume":"85","journal":"Communication Monographs"}}
{"bib_id":"kelleyetalexciting","title":"Exciting, Useful, Worrying, Futuristic: Public Perception of Artificial Intelligence in 8 Countries","author":"Kelley, Patrick Gage and Yang, Yongwei and Heldreth, Courtney and Moessner, Christopher and Sedley, Aaron and Kramm, Andreas and Newman, David T. and Woodruff, Allison","meta_info":{"series":"AIES '21","location":"Virtual Event, USA","keywords":"artificial intelligence, public perception","numpages":"11","pages":"627–637","booktitle":"Proceedings of the 2021 AAAI\/ACM Conference on AI, Ethics, and Society","doi":"10.1145\/3461702.3462605","url":"https:\/\/doi-org.ezbusc.usc.gal\/10.1145\/3461702.3462605","address":"New York, NY, USA","publisher":"Association for Computing Machinery","isbn":"9781450384735","year":"2021"}}
{"bib_id":"johnsonverdicchio","title":"Reframing AI Discourse","author":"Johnson, Deborah G. and Verdicchio, Mario","meta_info":{"keywords":"Autonomy, Robots, Future, Sociotechnical systems, Artificial intelligence","numpages":"16","pages":"575–590","month":"dec","journal":"Minds Mach.","abstract":"A critically important ethical issue facing the AI research community is how AI research and AI products can be responsibly conceptualised and presented to the public. A good deal of fear and concern about uncontrollable AI is now being displayed in public discourse. Public understanding of AI is being shaped in a way that may ultimately impede AI research. The public discourse as well as discourse among AI researchers leads to at least two problems: a confusion about the notion of `autonomy' that induces people to attribute to machines something comparable to human autonomy, and a `sociotechnical blindness' that hides the essential role played by humans at every stage of the design and deployment of an AI system. Here our purpose is to develop and use a language with the aim to reframe the discourse in AI and shed light on the real issues in the discipline.","doi":"10.1007\/s11023-017-9417-6","url":"https:\/\/doi-org.ezbusc.usc.gal\/10.1007\/s11023-017-9417-6","issn":"0924-6495","number":"4","volume":"27","address":"USA","publisher":"Kluwer Academic Publishers","issue_date":"December 2017","year":"2017"}}
{"bib_id":"krafftetaldefining","title":"Defining AI in Policy versus Practice","author":"Krafft, P. M. and Young, Meg and Katell, Michael and Huang, Karen and Bugingo, Ghislain","meta_info":{"series":"AIES '20","location":"New York, NY, USA","keywords":"definitions, policy, artificial intelligence, sociotechnical imaginaries","numpages":"7","pages":"72–78","booktitle":"Proceedings of the AAAI\/ACM Conference on AI, Ethics, and Society","doi":"10.1145\/3375627.3375835","url":"https:\/\/doi-org.ezbusc.usc.gal\/10.1145\/3375627.3375835","address":"New York, NY, USA","publisher":"Association for Computing Machinery","isbn":"9781450371100","year":"2020"}}
{"bib_id":"ibanez","title":"Operationalising AI Ethics: How Are Companies Bridging the Gap between Practice and Principles? An Exploratory Study","author":"Ibáñez, Javier Camacho and Olmeda, Mónica Villas","meta_info":{"keywords":"Explainability, Ethics, Principles, Privacy, Artificial intelligence, Fairness","numpages":"25","pages":"1663–1687","month":"dec","journal":"AI Soc.","doi":"10.1007\/s00146-021-01267-0","url":"https:\/\/doi-org.ezbusc.usc.gal\/10.1007\/s00146-021-01267-0","issn":"0951-5666","number":"4","volume":"37","address":"Berlin, Heidelberg","publisher":"Springer-Verlag","issue_date":"Dec 2022","year":"2022"}}
{"bib_id":"morley2021operationalising","title":"Operationalising AI ethics: barriers, enablers and next steps","author":"Morley, Jessica and Kinsey, Libby and Elhalal, Anat and Garcia, Francesca and Ziosi, Marta and Floridi, Luciano","meta_info":{"doi":"10.1007\/s00146-021-01308-8","publisher":"Springer","year":"2021","pages":"1--13","journal":"AI & SOCIETY"}}
{"bib_id":"hickok2021lessons","title":"Lessons learned from AI ethics principles for future actions","author":"Hickok, Merve","meta_info":{"doi":"10.1007\/s43681-020-00008-1","publisher":"Springer","year":"2021","pages":"41--47","number":"1","volume":"1","journal":"AI and Ethics"}}
{"bib_id":"ayling2022putting","title":"Putting AI ethics to work: are the tools fit for purpose?","author":"Ayling, Jacqui and Chapman, Adriane","meta_info":{"publisher":"Springer","year":"2022","pages":"405--429","number":"3","volume":"2","journal":"AI and Ethics"}}
{"bib_id":"Floridi2019","title":"Translating Principles Into Practices of Digital Ethics: Five Risks of Being Unethical","author":"Luciano Floridi","meta_info":{"number":"2","publisher":"Springer Verlag","journal":"Philosophy and Technology","doi":"10.1007\/s13347-019-00354-x","volume":"32","pages":"185--193","year":"2019"}}
{"bib_id":"mittelstadt2019principles","title":"Principles alone cannot guarantee ethical AI","author":"Mittelstadt, Brent","meta_info":{"publisher":"Nature Publishing Group UK London","doi":"10.1038\/s42256-019-0114-4","year":"2019","pages":"501--507","number":"11","volume":"1","journal":"Nature machine intelligence"}}
{"bib_id":"chromikexplanatorydepth","title":"I Think I Get Your Point, AI! The Illusion of Explanatory Depth in Explainable AI","author":"Chromik, Michael and Eiband, Malin and Buchner, Felicitas and Krüger, Adrian and Butz, Andreas","meta_info":{"series":"IUI '21","location":"College Station, TX, USA","keywords":"Shapley explanation, understanding, explainable AI, cognitive bias","numpages":"11","pages":"307–317","booktitle":"26th International Conference on Intelligent User Interfaces","doi":"10.1145\/3397481.3450644","url":"https:\/\/doi-org.ezbusc.usc.gal\/10.1145\/3397481.3450644","address":"New York, NY, USA","publisher":"Association for Computing Machinery","isbn":"9781450380171","year":"2021"}}
{"bib_id":"robbins2019misdirected","title":"A misdirected principle with a catch: explicability for AI","author":"Robbins, Scott","meta_info":{"publisher":"Springer","doi":"10.1007\/s11023-019-09509-3","year":"2019","pages":"495--514","number":"4","volume":"29","journal":"Minds and Machines"}}
{"bib_id":"russel2013artificial","title":"Artificial intelligence: a modern approach","author":"Russel, Stuart and Norvig, Peter and others","meta_info":{"publisher":"Pearson Education Limited London","year":"2013","volume":"256"}}
{"bib_id":"balagopalan2022road","title":"The road to explainability is paved with bias: Measuring the fairness of explanations","author":"Balagopalan, Aparna and Zhang, Haoran and Hamidieh, Kimia and Hartvigsen, Thomas and Rudzicz, Frank and Ghassemi, Marzyeh","meta_info":{"year":"2022","pages":"1194--1206","booktitle":"2022 ACM Conference on Fairness, Accountability, and Transparency"}}
{"bib_id":"yang2019bim","title":"BIM: Towards quantitative evaluation of interpretability methods with ground truth","author":"Yang, Mengjiao and Kim, Been","meta_info":{"year":"2019","journal":"arXiv preprint arXiv:1907.09701"}}
{"bib_id":"adebayo2022post","title":"Post hoc Explanations may be Ineffective for Detecting Unknown Spurious\nCorrelation","author":"Julius Adebayo and\nMichael Muelly and\nHarold Abelson and\nBeen Kim","meta_info":{"bibsource":"dblp computer science bibliography, https:\/\/dblp.org","biburl":"https:\/\/dblp.org\/rec\/conf\/iclr\/AdebayoMAK22.bib","timestamp":"Sat, 20 Aug 2022 01:15:42 +0200","url":"https:\/\/openreview.net\/forum?id=xNOVfCCvDpM","year":"2022","publisher":"OpenReview.net","booktitle":"The Tenth International Conference on Learning Representations, ICLR\n2022, Virtual Event, April 25-29, 2022"}}
{"bib_id":"mohseni2021multidisciplinary","title":"A multidisciplinary survey and framework for design and evaluation of explainable AI systems","author":"Mohseni, Sina and Zarei, Niloofar and Ragan, Eric D","meta_info":{"publisher":"ACM New York, NY","year":"2021","pages":"1--45","number":"3-4","volume":"11","journal":"ACM Transactions on Interactive Intelligent Systems (TiiS)"}}
{"bib_id":"jin2022evaluating","title":"Evaluating explainable AI on a multi-modal medical imaging task: can existing algorithms fulfill clinical requirements?","author":"Jin, Weina and Li, Xiaoxiao and Hamarneh, Ghassan","meta_info":{"year":"2022","pages":"11945--11953","number":"11","volume":"36","booktitle":"Proceedings of the AAAI Conference on Artificial Intelligence"}}
{"bib_id":"ehsan2021explainable","title":"The who in explainable ai: How ai background shapes perceptions of ai explanations","author":"Ehsan, Upol and Passi, Samir and Liao, Q Vera and Chan, Larry and Lee, I and Muller, Michael and Riedl, Mark O and others","meta_info":{"year":"2021","journal":"arXiv preprint arXiv:2107.13509"}}
{"bib_id":"liao2020questioning","title":"Questioning the AI: informing design practices for explainable AI user experiences","author":"Liao, Q Vera and Gruen, Daniel and Miller, Sarah","meta_info":{"year":"2020","pages":"1--15","booktitle":"Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems"}}
{"bib_id":"alqaraawi2020evaluating","title":"Evaluating saliency map explanations for convolutional neural networks: a user study","author":"Alqaraawi, Ahmed and Schuessler, Martin and Weiß, Philipp and Costanza, Enrico and Berthouze, Nadia","meta_info":{"year":"2020","pages":"275--285","booktitle":"Proceedings of the 25th International Conference on Intelligent User Interfaces"}}
{"bib_id":"szymanski2021visual","title":"Visual, textual or hybrid: the effect of user expertise on different explanations","author":"Szymanski, Maxwell and Millecamp, Martijn and Verbert, Katrien","meta_info":{"year":"2021","pages":"109--119","booktitle":"26th International Conference on Intelligent User Interfaces"}}
{"bib_id":"hadash2022improving","title":"Improving understandability of feature contributions in model-agnostic explainable AI tools","author":"Hadash, Sophia and Willemsen, Martijn C and Snijders, Chris and IJsselsteijn, Wijnand A","meta_info":{"year":"2022","pages":"1--9","booktitle":"Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems"}}
{"bib_id":"ehsan2020human","title":"Human-centered explainable ai: Towards a reflective sociotechnical approach","author":"Ehsan, Upol and Riedl, Mark O","meta_info":{"organization":"Springer","year":"2020","pages":"449--466","booktitle":"HCI International 2020-Late Breaking Papers: Multimodality and Intelligence: 22nd HCI International Conference, HCII 2020, Copenhagen, Denmark, July 19--24, 2020, Proceedings 22"}}
{"bib_id":"balayn2022can","title":"How can Explainability Methods be Used to Support Bug Identification in Computer Vision Models?","author":"Balayn, Agathe and Rikalo, Natasa and Lofi, Christoph and Yang, Jie and Bozzon, Alessandro","meta_info":{"year":"2022","pages":"1--16","booktitle":"Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems"}}
{"bib_id":"lage2019human","title":"Human evaluation of models built for interpretability","author":"Lage, Isaac and Chen, Emily and He, Jeffrey and Narayanan, Menaka and Kim, Been and Gershman, Samuel J and Doshi-Velez, Finale","meta_info":{"year":"2019","pages":"59--67","volume":"7","booktitle":"Proceedings of the AAAI Conference on Human Computation and Crowdsourcing"}}
{"bib_id":"sixt2020explanations","title":"When explanations lie: Why many modified bp attributions fail","author":"Sixt, Leon and Granz, Maximilian and Landgraf, Tim","meta_info":{"organization":"PMLR","year":"2020","pages":"9046--9057","booktitle":"International Conference on Machine Learning"}}
{"bib_id":"bordtPostHocExplanationsFail2022","title":"Post-Hoc Explanations Fail to Achieve Their Purpose in Adversarial Contexts","author":"Bordt, Sebastian and Finck, Michèle and Raidl, Eric and von Luxburg, Ulrike","meta_info":{"file":"C\\:\\\\Users\\łucan\\\\Zotero\\\\storage\\\\NTDFQH38\\\\Bordt et al. - 2022 - Post-Hoc Explanations Fail to Achieve their Purpos.pdf;C\\:\\\\Users\\łucan\\\\Zotero\\\\storage\\ŁRP85WZ9\\\\2201.html","keywords":"Computer Science - Artificial Intelligence,Computer Science - Machine Learning","archiveprefix":"arXiv","abstract":"Existing and planned legislation stipulates various obligations to provide information about machine learning algorithms and their functioning, often interpreted as obligations to \"explain\". Many researchers suggest using post-hoc explanation algorithms for this purpose. In this paper, we combine legal, philosophical and technical arguments to show that post-hoc explanation algorithms are unsuitable to achieve the law's objectives. Indeed, most situations where explanations are requested are adversarial, meaning that the explanation provider and receiver have opposing interests and incentives, so that the provider might manipulate the explanation for her own ends. We show that this fundamental conflict cannot be resolved because of the high degree of ambiguity of post-hoc explanations in realistic application scenarios. As a consequence, post-hoc explanation algorithms are unsuitable to achieve the transparency objectives inherent to the legal norms. Instead, there is a need to more explicitly discuss the objectives underlying \"explainability\" obligations as these can often be better achieved through other mechanisms. There is an urgent need for a more open and honest discussion regarding the potential and limitations of post-hoc explanations in adversarial contexts, in particular in light of the current negotiations of the European Union's draft Artificial Intelligence Act.","doi":"10.1145\/3531146.3533153","pages":"891--905","primaryclass":"cs","month":"June","year":"2022","booktitle":"2022 ACM Conference on Fairness, Accountability, and Transparency"}}
{"bib_id":"brkanLegalTechnicalFeasibility2020","title":"Legal and Technical Feasibility of the GDPR's Quest for Explanation of Algorithmic Decisions: Of Black Boxes, White Boxes and Fata Morganas","author":"Brkan, Maja and Bonnet, Grégory","meta_info":{"file":"C\\:\\\\Users\\łucan\\\\Zotero\\\\storage\\T̋8MW2JI\\\\Brkan and Bonnet - 2020 - Legal and Technical Feasibility of the GDPR’s Ques.pdf","langid":"english","abstract":"Understanding of the causes and correlations for algorithmic decisions is currently one of the major challenges of computer science, addressed under an umbrella term ``explainable AI (XAI)''. Being able to explain an AI-based system may help to make algorithmic decisions more satisfying and acceptable, to better control and update AI-based systems in case of failure, to build more accurate models, and to discover new knowledge directly or indirectly. On the legal side, the question whether the General Data Protection Regulation (GDPR) provides data subjects with the right to explanation in case of automated decision-making has equally been the subject of a heated doctrinal debate. While arguing that the right to explanation in the GDPR should be a result of interpretative analysis of several GDPR provisions jointly, the authors move this debate forward by discussing the technical and legal feasibility of the explanation of algorithmic decisions. Legal limits, in particular the secrecy of algorithms, as well as technical obstacles could potentially obstruct the practical implementation of this right. By adopting an interdisciplinary approach, the authors explore not only whether it is possible to translate the EU legal requirements for an explanation into the actual machine learning decision-making, but also whether those limitations can shape the way the legal right is used in practice.","doi":"10.1017\/err.2020.10","issn":"1867-299X, 2190-8249","pages":"18--50","number":"1","volume":"11","journal":"European Journal of Risk Regulation","month":"March","year":"2020","shorttitle":"Legal and Technical Feasibility of the GDPR's Quest for Explanation of Algorithmic Decisions"}}
{"bib_id":"CommunicationArtificialIntelligence2018","title":"Communication Artificial Intelligence for Europe: Shaping Europe's Digital Future","author":"European Commission","meta_info":{"file":"C\\:\\\\Users\\łucan\\\\Zotero\\\\storage\\\\WVGKPY2E\\o̧mmunication-artificial-intelligence-europe.html","langid":"english","url":"https:\/\/digital-strategy.ec.europa.eu\/en\/library\/communication-artificial-intelligence-europe","abstract":"Communication from the Commission to the European Parliament, the European Council, the Council, the European Economic and Social Committee and the Committee of the Regions on Artificial Intelligence for Europe.","month":"April","year":"2018"}}
{"bib_id":"ebersExplainableAIEuropean2022","title":"Explainable AI in the European Union: An Overview of the Current Legal Framework(s)","author":"Ebers, Martin","meta_info":{"file":"C\\:\\\\Users\\łucan\\\\Zotero\\\\storage\\\\BC7U9TMA\\\\Ebers - 2022 - Explainable AI in the European Union An Overview .pdf","langid":"english","doi":"10.53292\/208f5901.ff492fb3","pages":"103--132","journal":"The Swedish Law and Informatics Research Institute","month":"March","year":"2022","shorttitle":"Explainable AI in the European Union"}}
{"bib_id":"liao2022connecting","title":"Connecting Algorithmic Research and Usage Contexts: A Perspective of Contextualized Evaluation for Explainable AI","author":"Liao, Q Vera and Zhang, Yunfeng and Luss, Ronny and Doshi-Velez, Finale and Dhurandhar, Amit","meta_info":{"year":"2022","pages":"147--159","number":"1","volume":"10","booktitle":"Proceedings of the AAAI Conference on Human Computation and Crowdsourcing"}}
{"bib_id":"danezis2014privacy","title":"Privacy and Data Protection by Design-from policy to engineering","author":"Danezis, G and Domingo-Ferrer, J and Hansen, M and Hoepman, JH and Metayer, D Le and Tirtea, R and Schiffner, S","meta_info":{"publisher":"Heraklion: European Union Agency for Network and Information Security","year":"2014","journal":"TP-05-14-111-EN-N; TP-05-14-111-EN-N"}}
{"bib_id":"gurses2011engineering","title":"Engineering privacy by design","author":"Gürses, Seda and Troncoso, Carmela and Diaz, Claudia","meta_info":{"year":"2011","pages":"25","number":"3","volume":"14","journal":"Computers, Privacy & Data Protection"}}
{"bib_id":"karmaker2021automl","title":"Automl to date and beyond: Challenges and opportunities","author":"Karmaker, Shubhra Kanti and Hassan, Md Mahadi and Smith, Micah J and Xu, Lei and Zhai, Chengxiang and Veeramachaneni, Kalyan","meta_info":{"publisher":"ACM New York, NY","year":"2021","pages":"1--36","number":"8","volume":"54","journal":"ACM Computing Surveys (CSUR)"}}
{"bib_id":"mulligan2019thing","title":"This thing called fairness: Disciplinary confusion realizing a value in technology","author":"Mulligan, Deirdre K and Kroll, Joshua A and Kohli, Nitin and Wong, Richmond Y","meta_info":{"publisher":"ACM New York, NY, USA","year":"2019","pages":"1--36","number":"CSCW","volume":"3","journal":"Proceedings of the ACM on Human-Computer Interaction"}}
{"bib_id":"ExplainingDecisionsMade2020","title":"Explaining Decisions Made with AI","author":"Information Commissioner's Office","meta_info":{"file":"C\\:\\\\Users\\łucan\\\\Zotero\\\\storage\\\\ZVAIQWA8\\\\explaining-decisions-made-with-ai.html","langid":"english","howpublished":"https:\/\/ico.org.uk\/for-organisations\/guide-to-data-protection\/key-dp-themes\/explaining-decisions-made-with-artificial-intelligence\/","publisher":"ICO","year":"2020"}}
{"bib_id":"hackerVarietiesAIExplanations2022","title":"Varieties of~AI Explanations Under the~Law. From~the~GDPR to~the~AIA, and~Beyond","author":"Hacker, Philipp and Passoth, Jan-Hendrik","meta_info":{"file":"C\\:\\\\Users\\łucan\\\\Zotero\\\\storage\\\\FL6CQXNN\\a̋cker and Passoth - 2022 - Varieties of AI Explanations Under the Law. From t.pdf","keywords":"Artificial intelligence,Explainability,Regulation","langid":"english","isbn":"978-3-031-04083-2","abstract":"The quest to explain the output of artificial intelligence systems has clearly moved from a mere technical to a highly legally and politically relevant endeavor. In this paper, we provide an overview of legal obligations to explain AI and evaluate current policy proposals. In this, we distinguish between different functional varieties of AI explanations - such as multiple forms of enabling, technical and protective transparency - and show how different legal areas engage with and mandate such different types of explanations to varying degrees. Starting with the rights-enabling framework of the GDPR, we proceed to uncover technical and protective forms of explanations owed under contract, tort and banking law. Moreover, we discuss what the recent EU proposal for an Artificial Intelligence Act means for explainable AI, and review the proposal's strengths and limitations in this respect. Finally, from a policy perspective, we advocate for moving beyond mere explainability towards a more encompassing framework for trustworthy and responsible AI that includes actionable explanations, values-in-design and co-design methodologies, interactions with algorithmic fairness, and quality benchmarking.","doi":"10.1007\/978-3-031-04083-2_17","address":"Cham","publisher":"Springer International Publishing","pages":"343--373","series":"Lecture Notes in Computer Science","year":"2022","editor":"Holzinger, Andreas and Goebel, Randy and Fong, Ruth and Moon, Taesup and Müller, Klaus-Robert and Samek, Wojciech","booktitle":"xxAI - Beyond Explainable AI: International Workshop, Held in Conjunction with ICML 2020, July 18, 2020, Vienna, Austria, Revised and Extended Papers"}}
{"bib_id":"HLEGALTAI2020","title":"The Assessment List for Trustworthy Artificial Intelligence (ALTAI)","author":"High-Level Expert Group on Artificial Intelligence","meta_info":{"langid":"english","url":"https:\/\/ec.europa.eu\/futurium\/en\/ai-alliance-consultation","abstract":"Ethics Guidelines for Trustworthy AI","journal":"Futurium - European Commission","year":"2020","type":"Text"}}
{"bib_id":"EthicsGuidelinesTrustworthy2018","title":"Ethics Guidelines for Trustworthy AI","author":"High-Level Expert Group on Artificial Intelligence","meta_info":{"file":"C\\:\\\\Users\\łucan\\\\Zotero\\\\storage\\\\PC4PK79Y\\\\ai-alliance-consultation.1.html","langid":"english","url":"https:\/\/digital-strategy.ec.europa.eu\/en\/library\/ethics-guidelines-trustworthy-ai","abstract":"Ethics Guidelines for Trustworthy AI","year":"2019","type":"Text"}}
{"bib_id":"european2020white","title":"White paper on artificial intelligence: A European approach to excellence and trust","author":"European Commission","meta_info":{"url":"https:\/\/commission.europa.eu\/publications\/white-paper-artificial-intelligence-european-approach-excellence-and-trust_en","year":"2020","journal":"Brussels, 1st edn. European Commission, Brussels"}}
{"bib_id":"malgieriAutomatedDecisionmakingEU2019","title":"Automated Decision-Making in the EU Member States: The Right to Explanation and Other ``Suitable Safeguards'' in the National Legislations","author":"Malgieri, Gianclaudio","meta_info":{"file":"C\\:\\\\Users\\łucan\\\\Zotero\\\\storage\\\\V477ESC3\\\\Malgieri - 2019 - Automated decision-making in the EU Member States.pdf;C\\:\\\\Users\\łucan\\\\Zotero\\\\storage\\\\MKVL2EFV\\\\S0267364918303753.html","keywords":"AI,Algorithmic impact assessment,Article 22,Automated decision-making,Data Protection,GDPR,Legibility,Right to contest,Right to explanation,Suitable safeguards","langid":"english","abstract":"The aim of this paper is to analyse the very recently approved national Member States' laws that have implemented the GDPR in the field of automated decision-making (prohibition, exceptions, safeguards): all national legislations have been analysed and in particular 9 Member States Law address the case of automated decision making providing specific exemptions and relevant safeguards, as requested by Article 22(2)(b) of the GDPR (Belgium, The Netherlands, France, Germany, Hungary, Slovenia, Austria, the United Kingdom, Ireland). The approaches are very diverse: the scope of the provision can be narrow (just automated decisions producing legal or similarly detrimental effects) or wide (any decision with a significant impact) and even specific safeguards proposed are very diverse. After this overview, this article will also address the following questions: are Member States free to broaden the scope of automated decision-making regulation? Are `positive decisions' allowed under Article 22, GDPR, as some Member States seem to affirm? Which safeguards can better guarantee rights and freedoms of the data subject? In particular, while most Member States refers just to the three safeguards mentioned at Article 22(3) (i.e. subject's right to express one's point of view; right to obtain human intervention; right to contest the decision), three approaches seem very innovative: a) some States guarantee a right to legibility\/explanation about the algorithmic decisions (France and Hungary); b) other States (Ireland and United Kingdom) regulate human intervention on algorithmic decisions through an effective accountability mechanism (e.g. notification, explanation of why such contestation has not been accepted, etc.); c) another State (Slovenia) require an innovative form of human rights impact assessments on automated decision-making.","doi":"10.1016\/j.clsr.2019.05.002","issn":"0267-3649","pages":"105327","number":"5","volume":"35","journal":"Computer Law & Security Review","month":"October","year":"2019","shorttitle":"Automated Decision-Making in the EU Member States"}}
{"bib_id":"CABITZA2023118888","title":"Quod erat demonstrandum? - Towards a typology of the concept of explanation for the design of explainable AI","author":"Federico Cabitza and Andrea Campagner and Gianclaudio Malgieri and Chiara Natali and David Schneeberger and Karl Stoeger and Andreas Holzinger","meta_info":{"keywords":"Explainable AI, XAI, Explanations, Taxonomy, Artificial intelligence, Machine learning","url":"https:\/\/www.sciencedirect.com\/science\/article\/pii\/S0957417422019066","doi":"https:\/\/doi.org\/10.1016\/j.eswa.2022.118888","issn":"0957-4174","year":"2023","pages":"118888","volume":"213","journal":"Expert Systems with Applications"}}
{"bib_id":"Fenwick2018","title":"Business and Regulatory Responses to Artificial Intelligence: Dynamic Regulation, Innovation Ecosystems and the Strategic Management of Disruptive Technology","author":"Fenwick, Mark\nand Vermeulen, Erik P. M.\nand Corrales, Marcelo","meta_info":{"url":"https:\/\/doi.org\/10.1007\/978-981-13-2874-9_4","doi":"10.1007\/978-981-13-2874-9_4","isbn":"978-981-13-2874-9","pages":"81--103","address":"Singapore","publisher":"Springer Singapore","year":"2018","booktitle":"Robotics, AI and the Future of Law","editor":"Corrales, Marcelo\nand Fenwick, Mark\nand Forgó, Nikolaus"}}
{"bib_id":"sovranoMetricsExplainabilityEuropean2022","title":"Metrics, Explainability and the European AI Act Proposal","author":"Sovrano, Francesco and Sapienza, Salvatore and Palmirani, Monica and Vitali, Fabio","meta_info":{"file":"C\\:\\\\Users\\łucan\\\\Zotero\\\\storage\\\\JY2K8T5B\\\\Sovrano et al. - 2022 - Metrics, Explainability and the European AI Act Pr.pdf","abstract":"On 21 April 2021, the European Commission proposed the first legal framework on Artificial Intelligence (AI) to address the risks posed by this emerging method of computation. The Commission proposed a Regulation known as the AI Act. The proposed AI Act considers not only machine learning, but expert systems and statistical models long in place. Under the proposed AI Act, new obligations are set to ensure transparency, lawfulness, and fairness. Their goal is to establish mechanisms to ensure quality at launch and throughout the whole life cycle of AI-based systems, thus ensuring legal certainty that encourages innovation and investments on AI systems while preserving fundamental rights and values. A standardisation process is ongoing: several entities (e.g., ISO) and scholars are discussing how to design systems that are compliant with the forthcoming Act, and explainability metrics play a significant role. Specifically, the AI Act sets some new minimum requirements of explicability (transparency and explainability) for a list of AI systems labelled as ``high-risk'' listed in Annex III. These requirements include a plethora of technical explanations capable of covering the right amount of information, in a meaningful way. This paper aims to investigate how such technical explanations can be deemed to meet the minimum requirements set by the law and expected by society. To answer this question, with this paper we propose an analysis of the AI Act, aiming to understand (1) what specific explicability obligations are set and who shall comply with them and (2) whether any metric for measuring the degree of compliance of such explanatory documentation could be designed. Moreover, by envisaging the legal (or ethical) requirements that such a metric should possess, we discuss how to implement them in a practical way. More precisely, drawing inspiration from recent advancements in the theory of explanations, our analysis proposes that metrics to measure the kind of explainability endorsed by the proposed AI Act shall be risk-focused, model-agnostic, goal-aware, intelligible, and accessible. Therefore, we discuss the extent to which these requirements are met by the metrics currently under discussion.","doi":"10.3390\/j5010010","pages":"126--138","volume":"5","journal":"J","month":"February","year":"2022"}}
{"bib_id":"wachterWhyRightExplanation2017","title":"Why a Right to Explanation of Automated Decision-Making Does Not Exist in the General Data Protection Regulation","author":"Wachter, Sandra and Mittelstadt, Brent and Floridi, Luciano","meta_info":{"file":"C\\:\\\\Users\\łucan\\\\Zotero\\\\storage\\\\2NNRFZ6D\\\\Wachter et al. - 2017 - Why a Right to Explanation of Automated Decision-M.pdf;C\\:\\\\Users\\łucan\\\\Zotero\\\\storage\\\\37QCFARD\\\\3860948.html","abstract":"Key PointsSince approval of the European Union General Data Protection Regulation (GDPR) in 2016, it has been widely and repeatedly claimed that a `right to explanation' of all decisions made by automated or artificially intelligent algorithmic systems will be legally mandated by the GDPR once it is in force, in 2018.However, there are several reasons to doubt both the legal existence and the feasibility of such a right. In contrast to the right to explanation of specific automated decisions claimed elsewhere, the GDPR only mandates that data subjects receive meaningful, but properly limited, information (Articles 13– 15) about the logic involved, as well as the significance and the envisaged consequences of automated decision-making systems, what we term a `right to be informed'.The ambiguity and limited scope of the `right not to be subject to automated decision-making' contained in Article 22 (from which the alleged `right to explanation' stems) raises questions over the protection actually afforded to data subjects.These problems show that the GDPR lacks precise language as well as explicit and well-defined rights and safeguards against automated decision-making, and therefore runs the risk of being toothless.We propose a number of legislative steps that, if implemented, may improve the transparency and accountability of automated decision-making when the GDPR comes into force in 2018.","doi":"10.1093\/idpl\/ipx005","issn":"2044-3994","pages":"76--99","number":"2","volume":"7","journal":"International Data Privacy Law","month":"May","year":"2017"}}
{"bib_id":"edwardsveale2018","title":"Enslaving the Algorithm: From a “Right to an Explanation” to a “Right to Better Decisions”?","author":"Edwards, Lilian and Veale, Michael","meta_info":{"doi":"10.1109\/MSP.2018.2701152","pages":"46-54","number":"3","volume":"16","year":"2018","journal":"IEEE Security & Privacy"}}
{"bib_id":"rudin_2019_stop","title":"Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead","author":"Rudin, Cynthia","meta_info":{"publisher":"Nature Publishing Group UK London","doi":"10.1038\/s42256-019-0048-x","year":"2019","pages":"206--215","number":"5","volume":"1","journal":"Nature machine intelligence"}}
{"bib_id":"chakraborty_interpretability_2017","title":"Interpretability of deep learning models: A survey of results","author":"Chakraborty, Supriyo and Tomsett, Richard and Raghavendra, Ramya and Harborne, Daniel and Alzantot, Moustafa and Cerutti, Federico and Srivastava, Mani and Preece, Alun and Julier, Simon and Rao, Raghuveer M. and Kelley, Troy D. and Braines, Dave and Sensoy, Murat and Willis, Christopher J. and Gurram, Prudhvi","meta_info":{"file":"Chakraborty et al. - 2017 - Interpretability of deep learning models A survey.pdf:\/home\/adam\/Zotero\/storage\/Q8PNM4AT\/Chakraborty et al. - 2017 - Interpretability of deep learning models A survey.pdf:application\/pdf","pages":"1--6","year":"2017","month":"August","publisher":"IEEE","booktitle":"2017 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computed, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld\/SCALCOM\/UIC\/ATC\/CBDCom\/IOP\/SCI)","urldate":"2023-02-02","language":"en","abstract":"Deep neural networks have achieved near-human accuracy levels in various types of classiﬁcation and prediction tasks including images, text, speech, and video data. However, the networks continue to be treated mostly as black-box function approximators, mapping a given input to a classiﬁcation output. The next step in this human-machine evolutionary process –incorporating these networks into mission critical processes such as medical diagnosis, planning and control – requires a level of trust association with the machine output.","doi":"10.1109\/UIC-ATC.2017.8397411","url":"https:\/\/ieeexplore.ieee.org\/document\/8397411\/","shorttitle":"Interpretability of deep learning models","isbn":"978-1-5386-0435-9","address":"San Francisco, CA"}}
{"bib_id":"chenshapley","title":"L-Shapley and C-Shapley: Efficient Model Interpretation for Structured Data","author":"Chen, Jianbo and Song, Le and Wainwright, Martin J and Jordan, Michael I","meta_info":{"year":"2019","booktitle":"International Conference on Learning Representations (ICML)"}}
{"bib_id":"dhanorkar2021needs","title":"Who needs to know what, when?: Broadening the Explainable AI (XAI) Design Space by Looking at Explanations Across the AI Lifecycle","author":"Dhanorkar, Shipi and Wolf, Christine T and Qian, Kun and Xu, Anbang and Popa, Lucian and Li, Yunyao","meta_info":{"year":"2021","pages":"1591--1602","booktitle":"Designing Interactive Systems Conference 2021"}}
{"bib_id":"das2020opportunities","title":"Opportunities and challenges in explainable artificial intelligence (xai): A survey","author":"Das, Arun and Rad, Paul","meta_info":{"year":"2020","journal":"arXiv preprint arXiv:2006.11371"}}
{"bib_id":"wagner2018ethics","title":"Ethics as an escape from regulation. From “ethics-washing” to ethics-shopping?","author":"Wagner, Ben","meta_info":{"publisher":"Amsterdam University Press","year":"2018"}}
{"bib_id":"bietti2020ethics","title":"From ethics washing to ethics bashing: a view on tech ethics from within moral philosophy","author":"Bietti, Elettra","meta_info":{"year":"2020","pages":"210--219","booktitle":"Proceedings of the 2020 conference on fairness, accountability, and transparency"}}
{"bib_id":"green2021contestation","title":"The contestation of tech ethics: A sociotechnical approach to technology ethics in practice","author":"Green, Ben","meta_info":{"publisher":"TUP","year":"2021","pages":"209--225","number":"3","volume":"2","journal":"Journal of Social Computing"}}
{"bib_id":"sambasivan2021everyone","title":"“Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI","author":"Sambasivan, Nithya and Kapania, Shivani and Highfill, Hannah and Akrong, Diana and Paritosh, Praveen and Aroyo, Lora M","meta_info":{"year":"2021","pages":"1--15","booktitle":"proceedings of the 2021 CHI Conference on Human Factors in Computing Systems"}}
{"bib_id":"balayn2022can","title":"How can Explainability Methods be Used to Support Bug Identification in Computer Vision Models?","author":"Balayn, Agathe and Rikalo, Natasa and Lofi, Christoph and Yang, Jie and Bozzon, Alessandro","meta_info":{"year":"2022","pages":"1--16","booktitle":"Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems"}}
{"bib_id":"krishna2022disagreement","title":"The disagreement problem in explainable machine learning: A practitioner's perspective","author":"Krishna, Satyapriya and Han, Tessa and Gu, Alex and Pombra, Javin and Jabbari, Shahin and Wu, Steven and Lakkaraju, Himabindu","meta_info":{"year":"2022","journal":"arXiv preprint arXiv:2202.01602"}}
{"bib_id":"hase2020evaluating","title":"Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior?","author":"Hase, Peter and Bansal, Mohit","meta_info":{"year":"2020","pages":"5540--5552","booktitle":"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"}}
{"bib_id":"bhatt2020explainable","title":"Explainable machine learning in deployment","author":"Bhatt, Umang and Xiang, Alice and Sharma, Shubham and Weller, Adrian and Taly, Ankur and Jia, Yunhan and Ghosh, Joydeep and Puri, Ruchir and Moura, José MF and Eckersley, Peter","meta_info":{"year":"2020","pages":"648--657","booktitle":"Proceedings of the 2020 conference on fairness, accountability, and transparency"}}
{"bib_id":"belle2021principles","title":"Principles and practice of explainable machine learning","author":"Belle, Vaishak and Papantonis, Ioannis","meta_info":{"publisher":"Frontiers","year":"2021","pages":"39","journal":"Frontiers in big Data"}}
{"bib_id":"arrieta_explainable_2019","title":"Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI","author":"Alejandro Barredo Arrieta and Natalia Díaz-Rodríguez and Javier Del Ser and Adrien Bennetot and Siham Tabik and Alberto Barbado and Salvador Garcia and Sergio Gil-Lopez and Daniel Molina and Richard Benjamins and Raja Chatila and Francisco Herrera","meta_info":{"keywords":"Explainable Artificial Intelligence, Machine Learning, Deep Learning, Data Fusion, Interpretability, Comprehensibility, Transparency, Privacy, Fairness, Accountability, Responsible Artificial Intelligence","url":"https:\/\/www.sciencedirect.com\/science\/article\/pii\/S1566253519308103","doi":"https:\/\/doi.org\/10.1016\/j.inffus.2019.12.012","issn":"1566-2535","year":"2020","pages":"82-115","volume":"58","journal":"Information Fusion"}}
{"bib_id":"adebayo2018sanity","title":"Sanity checks for saliency maps","author":"Adebayo, Julius and Gilmer, Justin and Muelly, Michael and Goodfellow, Ian and Hardt, Moritz and Kim, Been","meta_info":{"year":"2018","volume":"31","journal":"Advances in neural information processing systems"}}
{"bib_id":"ghorbani2019interpretation","title":"Interpretation of neural networks is fragile","author":"Ghorbani, Amirata and Abid, Abubakar and Zou, James","meta_info":{"year":"2019","pages":"3681--3688","number":"01","volume":"33","booktitle":"Proceedings of the AAAI conference on artificial intelligence"}}
{"bib_id":"jesus2021can","title":"How can I choose an explainer? An application-grounded evaluation of post-hoc explanations","author":"Jesus, Sérgio and Belém, Catarina and Balayan, Vladimir and Bento, João and Saleiro, Pedro and Bizarro, Pedro and Gama, João","meta_info":{"year":"2021","pages":"805--815","booktitle":"Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency"}}
{"bib_id":"lee2019developing","title":"Developing the sensitivity of LIME for better machine learning explanation","author":"Lee, Eunjin and Braines, David and Stiffler, Mitchell and Hudler, Adam and Harborne, Daniel","meta_info":{"organization":"SPIE","year":"2019","pages":"349--356","volume":"11006","booktitle":"Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications"}}
{"bib_id":"ribeiro_model-agnostic_2016","title":"Model-Agnostic Interpretability of Machine Learning","author":"Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos","meta_info":{"file":"arXiv Fulltext PDF:\/home\/adam\/Zotero\/storage\/J2P7U64D\/Ribeiro et al. - 2016 - Model-Agnostic Interpretability of Machine Learnin.pdf:application\/pdf;arXiv.org Snapshot:\/home\/adam\/Zotero\/storage\/KP67X8RD\/1606.html:text\/html","annote":"Comment: presented at 2016 ICML Workshop on Human Interpretability in Machine Learning (WHI 2016), New York, NY","keywords":"Computer Science - Machine Learning, Statistics - Machine Learning","note":"arXiv:1606.05386 [cs, stat]","year":"2016","month":"June","publisher":"arXiv","urldate":"2023-02-02","abstract":"Understanding why machine learning models behave the way they do empowers both system designers and end-users in many ways: in model selection, feature engineering, in order to trust and act upon the predictions, and in more intuitive user interfaces. Thus, interpretability has become a vital concern in machine learning, and work in the area of interpretable models has found renewed interest. In some applications, such models are as accurate as non-interpretable ones, and thus are preferred for their transparency. Even when they are not accurate, they may still be preferred when interpretability is of paramount importance. However, restricting machine learning to interpretable models is often a severe limitation. In this paper we argue for explaining machine learning predictions using model-agnostic approaches. By treating the machine learning models as black-box functions, these approaches provide crucial flexibility in the choice of models, explanations, and representations, improving debugging, comparison, and interfaces for a variety of users and models. We also outline the main challenges for such methods, and review a recently-introduced model-agnostic explanation approach (LIME) that addresses these challenges.","url":"http:\/\/arxiv.org\/abs\/1606.05386"}}
{"bib_id":"burkart2021survey","title":"A survey on the explainability of supervised machine learning","author":"Burkart, Nadia and Huber, Marco F","meta_info":{"year":"2021","pages":"245--317","volume":"70","journal":"Journal of Artificial Intelligence Research"}}
{"bib_id":"slack2020fooling","title":"Fooling lime and shap: Adversarial attacks on post hoc explanation methods","author":"Slack, Dylan and Hilgard, Sophie and Jia, Emily and Singh, Sameer and Lakkaraju, Himabindu","meta_info":{"year":"2020","pages":"180--186","booktitle":"Proceedings of the AAAI\/ACM Conference on AI, Ethics, and Society"}}
{"bib_id":"mohseni2021quantitative","title":"Quantitative evaluation of machine learning explanations: A human-grounded benchmark","author":"Mohseni, Sina and Block, Jeremy E and Ragan, Eric","meta_info":{"year":"2021","pages":"22--31","booktitle":"26th International Conference on Intelligent User Interfaces"}}
{"bib_id":"confalonieri2021historical","title":"A historical perspective of explainable Artificial Intelligence","author":"Confalonieri, Roberto and Coba, Ludovik and Wagner, Benedikt and Besold, Tarek R","meta_info":{"publisher":"Wiley Online Library","year":"2021","pages":"e1391","number":"1","volume":"11","journal":"Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery"}}
{"bib_id":"zablocki2022explainability","title":"Explainability of deep vision-based autonomous driving systems: Review and challenges","author":"Zablocki, Éloi and Ben-Younes, Hédi and Pérez, Patrick and Cord, Matthieu","meta_info":{"publisher":"Springer","year":"2022","pages":"2425--2452","number":"10","volume":"130","journal":"International Journal of Computer Vision"}}
{"bib_id":"danilevsky2020survey","title":"A Survey of the State of Explainable AI for Natural Language Processing","author":"Danilevsky, Marina  and\nQian, Kun  and\nAharonov, Ranit  and\nKatsis, Yannis  and\nKawas, Ban  and\nSen, Prithviraj","meta_info":{"pages":"447--459","url":"https:\/\/aclanthology.org\/2020.aacl-main.46","publisher":"Association for Computational Linguistics","address":"Suzhou, China","year":"2020","month":"December","journal":"Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing"}}
{"bib_id":"lage2019evaluation","title":"An evaluation of the human-interpretability of explanation","author":"Lage, Isaac and Chen, Emily and He, Jeffrey and Narayanan, Menaka and Kim, Been and Gershman, Sam and Doshi-Velez, Finale","meta_info":{"year":"2019","journal":"arXiv preprint arXiv:1902.00006"}}
{"bib_id":"zhang2020explainable","title":"Explainable recommendation: A survey and new perspectives","author":"Zhang, Yongfeng and Chen, Xu and others","meta_info":{"publisher":"Now Publishers, Inc.","year":"2020","pages":"1--101","number":"1","volume":"14","journal":"Foundations and Trends® in Information Retrieval"}}
{"bib_id":"adebayo2022post","title":"Post hoc Explanations may be Ineffective for Detecting Unknown Spurious\nCorrelation","author":"Julius Adebayo and\nMichael Muelly and\nHarold Abelson and\nBeen Kim","meta_info":{"bibsource":"dblp computer science bibliography, https:\/\/dblp.org","biburl":"https:\/\/dblp.org\/rec\/conf\/iclr\/AdebayoMAK22.bib","timestamp":"Sat, 20 Aug 2022 01:15:42 +0200","url":"https:\/\/openreview.net\/forum?id=xNOVfCCvDpM","year":"2022","publisher":"OpenReview.net","booktitle":"The Tenth International Conference on Learning Representations, ICLR\n2022, Virtual Event, April 25-29, 2022"}}
{"bib_id":"letzgus2022toward","title":"Toward explainable artificial intelligence for regression models: A methodological perspective","author":"Letzgus, Simon and Wagner, Patrick and Lederer, Jonas and Samek, Wojciech and Müller, Klaus-Robert and Montavon, Gregoire","meta_info":{"publisher":"IEEE","year":"2022","pages":"40--58","number":"4","volume":"39","journal":"IEEE Signal Processing Magazine"}}
{"bib_id":"samek2019towards","title":"Towards explainable artificial intelligence","author":"Samek, Wojciech and Müller, Klaus-Robert","meta_info":{"publisher":"Springer","year":"2019","pages":"5--22","journal":"Explainable AI: interpreting, explaining and visualizing deep learning"}}
{"bib_id":"jesus2021can","title":"How can I choose an explainer? An application-grounded evaluation of post-hoc explanations","author":"Jesus, Sérgio and Belém, Catarina and Balayan, Vladimir and Bento, João and Saleiro, Pedro and Bizarro, Pedro and Gama, João","meta_info":{"year":"2021","pages":"805--815","booktitle":"Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency"}}
{"bib_id":"zhang2018interpretable","title":"Interpretable convolutional neural networks","author":"Zhang, Quanshi and Wu, Ying Nian and Zhu, Song-Chun","meta_info":{"year":"2018","pages":"8827--8836","booktitle":"Proceedings of the IEEE conference on computer vision and pattern recognition"}}
{"bib_id":"BoE","title":"Machine learning in UK financial services","author":"Bank of England","meta_info":{"note":"Accessed on February 2, 2023","year":"2022","url":"https:\/\/www.bankofengland.co.uk\/report\/2022\/machine-learning-in-uk-financial-services"}}
{"bib_id":"bell2022s","title":"It’s just not that simple: an empirical study of the accuracy-explainability trade-off in machine learning for public policy","author":"Bell, Andrew and Solano-Kamaiko, Ian and Nov, Oded and Stoyanovich, Julia","meta_info":{"year":"2022","pages":"248--266","booktitle":"2022 ACM Conference on Fairness, Accountability, and Transparency"}}
{"bib_id":"madaio2020co","title":"Co-designing checklists to understand organizational challenges and opportunities around fairness in AI","author":"Madaio, Michael A and Stark, Luke and Wortman Vaughan, Jennifer and Wallach, Hanna","meta_info":{"year":"2020","pages":"1--14","booktitle":"Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems"}}
{"bib_id":"rakova2021responsible","title":"Where responsible AI meets reality: Practitioner perspectives on enablers for shifting organizational practices","author":"Rakova, Bogdana and Yang, Jingying and Cramer, Henriette and Chowdhury, Rumman","meta_info":{"publisher":"ACM New York, NY, USA","year":"2021","pages":"1--23","number":"CSCW1","volume":"5","journal":"Proceedings of the ACM on Human-Computer Interaction"}}
{"bib_id":"budig2020trade","title":"Trade-offs between privacy-preserving and explainable machine learning in healthcare","author":"Budig, Tobias and Herrmann, Selina and Dietz, Alexander","meta_info":{"year":"2020","booktitle":"Seminar Paper, Inst. Appl. Informat. Formal Description Methods (AIFB), KIT Dept. Econom. Manage., Karlsruhe, Germany"}}
{"bib_id":"balayn2021beyond","title":"Beyond Debiasing: Regulating AI and its inequalities","author":"Balayn, Agathe and Gürses, Seda","meta_info":{"year":"2021","url":"https:\/\/edri.org\/wp-content\/uploads\/2021\/09\/EDRi_Beyond-Debiasing-Report_Online.pdf","journal":"EDRi Report"}}
{"bib_id":"bendershahsituatingsearch","title":"Situating Search","author":"Shah, Chirag and Bender, Emily M.","meta_info":{"series":"CHIIR '22","location":"Regensburg, Germany","keywords":"Search models, Language models, Information Seeking Strategies","numpages":"12","pages":"221–232","booktitle":"ACM SIGIR Conference on Human Information Interaction and Retrieval","doi":"10.1145\/3498366.3505816","url":"https:\/\/doi-org.ezbusc.usc.gal\/10.1145\/3498366.3505816","address":"New York, NY, USA","publisher":"Association for Computing Machinery","isbn":"9781450391863","year":"2022"}}
{"bib_id":"smuha2021beyond","title":"Beyond the individual: governing AI’s societal harm","author":"Smuha, Nathalie A","meta_info":{"year":"2021","url":"https:\/\/ssrn.com\/abstract=3941956","number":"3","volume":"10","journal":"Internet Policy Review"}}
{"bib_id":"foundationmodels","title":"On the Opportunities and Risks of Foundation Models","author":"Bommasani, Rishi and Hudson, Drew A. and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S. and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and Brynjolfsson, Erik and Buch, Shyamal and Card, Dallas and Castellon, Rodrigo and Chatterji, Niladri and Chen, Annie and Creel, Kathleen and Davis, Jared Quincy and Demszky, Dora and Donahue, Chris and Doumbouya, Moussa and Durmus, Esin and Ermon, Stefano and Etchemendy, John and Ethayarajh, Kawin and Fei-Fei, Li and Finn, Chelsea and Gale, Trevor and Gillespie, Lauren and Goel, Karan and Goodman, Noah and Grossman, Shelby and Guha, Neel and Hashimoto, Tatsunori and Henderson, Peter and Hewitt, John and Ho, Daniel E. and Hong, Jenny and Hsu, Kyle and Huang, Jing and Icard, Thomas and Jain, Saahil and Jurafsky, Dan and Kalluri, Pratyusha and Karamcheti, Siddharth and Keeling, Geoff and Khani, Fereshte and Khattab, Omar and Koh, Pang Wei and Krass, Mark and Krishna, Ranjay and Kuditipudi, Rohith and Kumar, Ananya and Ladhak, Faisal and Lee, Mina and Lee, Tony and Leskovec, Jure and Levent, Isabelle and Li, Xiang Lisa and Li, Xuechen and Ma, Tengyu and Malik, Ali and Manning, Christopher D. and Mirchandani, Suvir and Mitchell, Eric and Munyikwa, Zanele and Nair, Suraj and Narayan, Avanika and Narayanan, Deepak and Newman, Ben and Nie, Allen and Niebles, Juan Carlos and Nilforoshan, Hamed and Nyarko, Julian and Ogut, Giray and Orr, Laurel and Papadimitriou, Isabel and Park, Joon Sung and Piech, Chris and Portelance, Eva and Potts, Christopher and Raghunathan, Aditi and Reich, Rob and Ren, Hongyu and Rong, Frieda and Roohani, Yusuf and Ruiz, Camilo and Ryan, Jack and Ré, Christopher and Sadigh, Dorsa and Sagawa, Shiori and Santhanam, Keshav and Shih, Andy and Srinivasan, Krishnan and Tamkin, Alex and Taori, Rohan and Thomas, Armin W. and Tramèr, Florian and Wang, Rose E. and Wang, William and Wu, Bohan and Wu, Jiajun and Wu, Yuhuai and Xie, Sang Michael and Yasunaga, Michihiro and You, Jiaxuan and Zaharia, Matei and Zhang, Michael and Zhang, Tianyi and Zhang, Xikun and Zhang, Yuhui and Zheng, Lucia and Zhou, Kaitlyn and Liang, Percy","meta_info":{"copyright":"Creative Commons Attribution 4.0 International","year":"2021","publisher":"arXiv","keywords":"Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computers and Society (cs.CY), FOS: Computer and information sciences, FOS: Computer and information sciences","url":"https:\/\/arxiv.org\/abs\/2108.07258","doi":"10.48550\/ARXIV.2108.07258"}}
{"bib_id":"png2022globalsouth","title":"At the Tensions of South and North: Critical Roles of Global South Stakeholders in AI Governance","author":"Png, Marie-Therese","meta_info":{"series":"FAccT '22","location":"Seoul, Republic of Korea","keywords":"Coloniality of AI, Global South AI, Inclusive AI governance, AI Policy","numpages":"12","pages":"1434–1445","booktitle":"2022 ACM Conference on Fairness, Accountability, and Transparency","abstract":"This paper aims to present a landscape of AI governance for and from the Global South, advanced by critical and decolonial-informed practitioners and scholars, and contrast this with the Inclusive AI Governance discourse led out of Global North institutions. By doing so, it identifies gaps in the dominant AI governance discourse, and bridges these gaps with relevant discourses of technology and power, localisation, and historical-geopolitical analyses of inequality led by Global South aligned actors. Specific areas of concern addressed by this paper include infrastructural and regulatory monopolies, harms associated with the labour and material supply chains of AI infrastructure, and commercial exploitation. By contrasting Global South and Global North discourses surrounding AI risks, this paper proposes a systemic restructuring of AI governance processes beyond current frameworks of Inclusive AI governance, offering three roles for Global South actors to substantively engage in AI governance processes.","doi":"10.1145\/3531146.3533200","url":"https:\/\/doi.org\/10.1145\/3531146.3533200","address":"New York, NY, USA","publisher":"Association for Computing Machinery","isbn":"9781450393522","year":"2022"}}
{"bib_id":"mohamed2020decolonial","title":"Decolonial AI: Decolonial theory as sociotechnical foresight in artificial intelligence","author":"Mohamed, Shakir and Png, Marie-Therese and Isaac, William","meta_info":{"publisher":"Springer","doi":"10.1007\/s13347-020-00405-8","year":"2020","pages":"659--684","volume":"33","journal":"Philosophy & Technology"}}
