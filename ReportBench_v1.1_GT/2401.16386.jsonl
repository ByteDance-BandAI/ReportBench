{"bib_id":"WahCUB2002011","title":"The Caltech-UCSD Birds-200-2011 Dataset","author":"Wah, C. and Branson, S. and Welinder, P. and Perona, P. and Belongie, S.","meta_info":{"number":"CNS-TR-2011-001","institution":"California Institute of Technology","year":"2011"}}
{"bib_id":"radford2021learning","title":"Learning transferable visual models from natural language supervision","author":"Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others","meta_info":{"organization":"PMLR","year":"2021","pages":"8748--8763","booktitle":"ICML"}}
{"bib_id":"he2015residual","title":"Deep Residual Learning for Image Recognition","author":"Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun","meta_info":{"pages":"770-778","year":"2016","booktitle":"CVPR"}}
{"bib_id":"deng2009imagenet","title":"Imagenet: A large-scale hierarchical image database","author":"Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li","meta_info":{"year":"2009","pages":"248--255","booktitle":"CVPR"}}
{"bib_id":"mccloskey1989catastrophic","title":"Catastrophic interference in connectionist networks: The sequential learning problem","author":"McCloskey, Michael and Cohen, Neal J","meta_info":{"publisher":"Elsevier","year":"1989","pages":"109--165","volume":"24","booktitle":"Psychology of learning and motivation"}}
{"bib_id":"van2022three","title":"Three types of incremental learning","author":"van de Ven, Gido M and Tuytelaars, Tinne and Tolias, Andreas S","meta_info":{"publisher":"Nature Publishing Group","year":"2022","pages":"1--13","journal":"Nature Machine Intelligence"}}
{"bib_id":"zhu2021prototype","title":"Prototype Augmentation and Self-Supervision for Incremental Learning","author":"Zhu, Fei and Zhang, Xu-Yao and Wang, Chuang and Yin, Fei and Liu, Cheng-Lin","meta_info":{"year":"2021","pages":"5871--5880","booktitle":"CVPR"}}
{"bib_id":"de2021survey","title":"A continual learning survey: Defying forgetting in classification tasks","author":"De Lange, Matthias and Aljundi, Rahaf and Masana, Marc and Parisot, Sarah and Jia, Xu and Leonardis, Aleš and Slabaugh, Gregory and Tuytelaars, Tinne","meta_info":{"year":"2021","pages":"3366--3385","number":"7","volume":"44","journal":"TPAMI"}}
{"bib_id":"floridi2020gpt","title":"GPT-3: Its nature, scope, limits, and consequences","author":"Floridi, Luciano and Chiriatti, Massimo","meta_info":{"publisher":"Springer","year":"2020","pages":"681--694","number":"4","volume":"30","journal":"Minds and Machines"}}
{"bib_id":"masana2022class","title":"Class-Incremental Learning: Survey and Performance Evaluation on Image Classification","author":"Masana, Marc and Liu, Xialei and Twardowski, Bartlomiej and Menta, Mikel and Bagdanov, Andrew D and van de Weijer, Joost","meta_info":{"year":"2023","pages":"5513--5533","number":"05","volume":"45","journal":"TPAMI"}}
{"bib_id":"krizhevsky2009learning","title":"Learning multiple layers of features from tiny images","author":"Krizhevsky, Alex and Hinton, Geoffrey and others","meta_info":{"year":"2009"}}
{"bib_id":"zhou2023class","title":"Deep Class-Incremental Learning: A Survey","author":"Zhou, Da-Wei and Wang, Qi-Wei and Qi, Zhi-Hong and Ye, Han-Jia and Zhan, De-Chuan and Liu, Ziwei","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2302.03648"}}
{"bib_id":"vaswani2017attention","title":"Attention is all you need","author":"Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia","meta_info":{"year":"2017","pages":"5998--6008","booktitle":"NIPS"}}
{"bib_id":"zhang2024comprehensive","title":"A Comprehensive Study of Knowledge Editing for Large Language Models","author":"Zhang, Ningyu and Yao, Yunzhi and Tian, Bozhong and Wang, Peng and Deng, Shumin and Wang, Mengru and Xi, Zekun and Mao, Shengyu and Zhang, Jintian and Ni, Yuansheng and others","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2401.01286"}}
{"bib_id":"dosovitskiy2020image","title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale","author":"Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others","meta_info":{"year":"2020","booktitle":"ICLR"}}
{"bib_id":"jia2022visual","title":"Visual Prompt Tuning","author":"Menglin Jia and\nLuming Tang and\nBor-Chun Chen and\nClaire Cardie and\nSerge J. Belongie and\nBharath Hariharan and\nSer-Nam Lim","meta_info":{"year":"2022","publisher":"Springer","pages":"709--727","booktitle":"ECCV"}}
{"bib_id":"smith2023coda","title":"CODA-Prompt: COntinual Decomposed Attention-based Prompting for Rehearsal-Free Continual Learning","author":"Smith, James Seale and Karlinsky, Leonid and Gutta, Vyshnavi and Cascante-Bonilla, Paola and Kim, Donghyun and Arbelle, Assaf and Panda, Rameswar and Feris, Rogerio and Kira, Zsolt","meta_info":{"year":"2023","pages":"11909--11919","booktitle":"CVPR"}}
{"bib_id":"khattak2023self","title":"Self-regulating prompts: Foundational model adaptation without forgetting","author":"Khattak, Muhammad Uzair and Wasim, Syed Talal and Naseer, Muzammal and Khan, Salman and Yang, Ming-Hsuan and Khan, Fahad Shahbaz","meta_info":{"year":"2023","pages":"15190--15200","booktitle":"ICCV"}}
{"bib_id":"janson2022simple","title":"A simple baseline that questions the use of pretrained-models in continual learning","author":"Janson, Paul and Zhang, Wenxuan and Aljundi, Rahaf and Elhoseiny, Mohamed","meta_info":{"year":"2022","journal":"arXiv preprint arXiv:2210.04428"}}
{"bib_id":"ahrens2023read","title":"Read Between the Layers: Leveraging Intra-Layer Representations for Rehearsal-Free Continual Learning with Pre-Trained Models","author":"Ahrens, Kyra and Lehmann, Hans Hergen and Lee, Jae Hee and Wermter, Stefan","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2312.08888"}}
{"bib_id":"wang2022learning","title":"Learning to prompt for continual learning","author":"Wang, Zifeng and Zhang, Zizhao and Lee, Chen-Yu and Zhang, Han and Sun, Ruoxi and Ren, Xiaoqi and Su, Guolong and Perot, Vincent and Dy, Jennifer and Pfister, Tomas","meta_info":{"year":"2022","pages":"139--149","booktitle":"CVPR"}}
{"bib_id":"wang2022dualprompt","title":"DualPrompt: Complementary Prompting for Rehearsal-free Continual Learning","author":"Wang, Zifeng and Zhang, Zizhao and Ebrahimi, Sayna and Sun, Ruoxi and Zhang, Han and Lee, Chen-Yu and Ren, Xiaoqi and Su, Guolong and Perot, Vincent and Dy, Jennifer and others","meta_info":{"year":"2022","journal":"arXiv preprint arXiv:2204.04799"}}
{"bib_id":"villa2023pivot","title":"Pivot: Prompting for video continual learning","author":"Villa, Andrés and Alcázar, Juan León and Alfarra, Motasem and Alhamoud, Kumail and Hurtado, Julio and Heilbron, Fabian Caba and Soto, Alvaro and Ghanem, Bernard","meta_info":{"year":"2023","pages":"24214--24223","booktitle":"CVPR"}}
{"bib_id":"Zheng_2023_ICCV","title":"Preventing Zero-Shot Transfer Degradation in Continual Learning of Vision-Language Models","author":"Zheng, Zangwei and Ma, Mingyuan and Wang, Kai and Qin, Ziheng and Yue, Xiangyu and You, Yang","meta_info":{"pages":"19125-19136","year":"2023","month":"October","booktitle":"ICCV"}}
{"bib_id":"zhou2023learning","title":"Learning without Forgetting for Vision-Language Models","author":"Zhou, Da-Wei and Zhang, Yuanhan and Ning, Jingyi and Ye, Han-Jia and Zhan, De-Chuan and Liu, Ziwei","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2305.19270"}}
{"bib_id":"steiner2021train","title":"How to train your vit? data, augmentation, and regularization in vision transformers","author":"Steiner, Andreas and Kolesnikov, Alexander and Zhai, Xiaohua and Wightman, Ross and Uszkoreit, Jakob and Beyer, Lucas","meta_info":{"year":"2021","journal":"arXiv preprint arXiv:2106.10270"}}
{"bib_id":"zhou2023revisiting","title":"Revisiting class-incremental learning with pre-trained models: Generalizability and adaptivity are all you need","author":"Zhou, Da-Wei and Ye, Han-Jia and Zhan, De-Chuan and Liu, Ziwei","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2303.07338"}}
{"bib_id":"moon1996expectation","title":"The expectation-maximization algorithm","author":"Moon, Todd K","meta_info":{"publisher":"IEEE","year":"1996","pages":"47--60","number":"6","volume":"13","journal":"IEEE Signal processing magazine"}}
{"bib_id":"chen2022adaptformer","title":"Adaptformer: Adapting vision transformers for scalable visual recognition","author":"Chen, Shoufa and Ge, Chongjian and Tong, Zhan and Wang, Jiangliu and Song, Yibing and Wang, Jue and Luo, Ping","meta_info":{"year":"2022","pages":"16664--16678","volume":"35","journal":"NeurIPS"}}
{"bib_id":"mcdonnell2023ranpac","title":"RanPAC: Random Projections and Pre-trained Models for Continual Learning","author":"McDonnell, Mark D and Gong, Dong and Parveneh, Amin and Abbasnejad, Ehsan and Hengel, Anton van den","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2307.02251"}}
{"bib_id":"prabhu2023computationally","title":"Computationally Budgeted Continual Learning: What Does Matter?","author":"Prabhu, Ameya and Al Kader Hammoud, Hasan Abed and Dokania, Puneet K and Torr, Philip HS and Lim, Ser-Nam and Ghanem, Bernard and Bibi, Adel","meta_info":{"year":"2023","pages":"3698--3707","booktitle":"CVPR"}}
{"bib_id":"gan2023decorate","title":"Decorate the newcomers: Visual domain prompt for continual test time adaptation","author":"Gan, Yulu and Bai, Yan and Lou, Yihang and Ma, Xianzheng and Zhang, Renrui and Shi, Nian and Luo, Lin","meta_info":{"year":"2023","pages":"7595--7603","number":"6","volume":"37","booktitle":"AAAI"}}
{"bib_id":"jung2023generating","title":"Generating instance-level prompts for rehearsal-free continual learning","author":"Jung, Dahuin and Han, Dongyoon and Bang, Jihwan and Song, Hwanjun","meta_info":{"year":"2023","pages":"11847--11857","booktitle":"ICCV"}}
{"bib_id":"khan2023introducing","title":"Introducing language guidance in prompt-based continual learning","author":"Khan, Muhammad Gul Zain Ali and Naeem, Muhammad Ferjad and Van Gool, Luc and Stricker, Didier and Tombari, Federico and Afzal, Muhammad Zeshan","meta_info":{"year":"2023","pages":"11463--11473","booktitle":"ICCV"}}
{"bib_id":"razdaibiedina2022progressive","title":"Progressive Prompts: Continual Learning for Language Models","author":"Razdaibiedina, Anastasia and Mao, Yuning and Hou, Rui and Khabsa, Madian and Lewis, Mike and Almahairi, Amjad","meta_info":{"year":"2023","booktitle":"ICLR"}}
{"bib_id":"wang2023isolation","title":"Isolation and impartial aggregation: A paradigm of incremental learning without interference","author":"Wang, Yabin and Ma, Zhiheng and Huang, Zhiwu and Wang, Yaowei and Su, Zhou and Hong, Xiaopeng","meta_info":{"year":"2023","pages":"10209--10217","number":"8","volume":"37","booktitle":"AAAI"}}
{"bib_id":"liu2022incremental","title":"Incremental Prompting: Episodic Memory Prompt for Lifelong Event Detection","author":"Liu, Minqian and Chang, Shiyu and Huang, Lifu","meta_info":{"year":"2022","pages":"2157--2165","booktitle":"COLING"}}
{"bib_id":"wang2023orthogonal","title":"Orthogonal Subspace Learning for Language Model Continual Learning","author":"Wang, Xiao and Chen, Tianze and Ge, Qiming and Xia, Han and Bao, Rong and Zheng, Rui and Zhang, Qi and Gui, Tao and Huang, Xuan-Jing","meta_info":{"year":"2023","pages":"10658--10671","booktitle":"Findings of EMNLP"}}
{"bib_id":"wang2023attriclip","title":"AttriCLIP: A Non-Incremental Learner for Incremental Knowledge Learning","author":"Wang, Runqi and Duan, Xiaoyue and Kang, Guoliang and Liu, Jianzhuang and Lin, Shaohui and Xu, Songcen and Lü, Jinhu and Zhang, Baochang","meta_info":{"year":"2023","pages":"3654--3663","booktitle":"CVPR"}}
{"bib_id":"moon2023online","title":"Online Class Incremental Learning on Stochastic Blurry Task Boundary via Mask and Visual Prompt Tuning","author":"Moon, Jun-Yeong and Park, Keon-Hee and Kim, Jung Uk and Park, Gyeong-Moon","meta_info":{"year":"2023","pages":"11731--11741","booktitle":"ICCV"}}
{"bib_id":"tang2023prompt","title":"When Prompt-based Incremental Learning Does Not Meet Strong Pretraining","author":"Tang, Yu-Ming and Peng, Yi-Xing and Zheng, Wei-Shi","meta_info":{"year":"2023","pages":"1706--1716","booktitle":"ICCV"}}
{"bib_id":"wang2023hierarchical","title":"Hierarchical decomposition of prompt-based continual learning: Rethinking obscured sub-optimality","author":"Wang, Liyuan and Xie, Jingyi and Zhang, Xingxing and Huang, Mingyi and Su, Hang and Zhu, Jun","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2310.07234"}}
{"bib_id":"yadav2023exploring","title":"Exploring Continual Learning for Code Generation Models","author":"Yadav, Prateek and Sun, Qing and Ding, Hantian and Li, Xiaopeng and Zhang, Dejiao and Tan, Ming and Bhatia, Parminder and Ma, Xiaofei and Nallapati, Ramesh and Ramanathan, Murali Krishna and others","meta_info":{"year":"2023","pages":"782--792","booktitle":"ACL"}}
{"bib_id":"chen2023promptfusion","title":"PromptFusion: Decoupling Stability and Plasticity for Continual Learning","author":"Chen, Haoran and Wu, Zuxuan and Han, Xintong and Jia, Menglin and Jiang, Yu-Gang","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2303.07223"}}
{"bib_id":"guo2024federated","title":"Federated Class-Incremental Learning with Prototype Guided Transformer","author":"Guo, Haiyang and Zhu, Fei and Liu, Wenzhuo and Zhang, Xu-Yao and Liu, Cheng-Lin","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2401.02094"}}
{"bib_id":"zheng2023learn","title":"Learn or Recall? Revisiting Incremental Learning with Pre-trained Language Models","author":"Zheng, Junhao and Qiu, Shengjie and Ma, Qianli","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2312.07887"}}
{"bib_id":"nicolas2024mop","title":"MoP-CLIP: A Mixture of Prompt-Tuned CLIP Models for Domain Incremental Learning","author":"Nicolas, Julien and Chiaroni, Florent and Ziko, Imtiaz and Ahmad, Ola and Desrosiers, Christian and Dolz, Jose","meta_info":{"year":"2024","pages":"1762--1772","booktitle":"WACV"}}
{"bib_id":"Gao_2023_ICCV","title":"A Unified Continual Learning Framework with General Parameter-Efficient Tuning","author":"Gao, Qiankun and Zhao, Chen and Sun, Yifan and Xi, Teng and Zhang, Gang and Ghanem, Bernard and Zhang, Jian","meta_info":{"pages":"11483-11493","year":"2023","month":"October","booktitle":"ICCV"}}
{"bib_id":"liu2023dual","title":"Dual prompt learning for continual rain removal from single images","author":"Liu, Minghao and Yang, Wenhan and Hu, Yuzhang and Liu, Jiaying","meta_info":{"year":"2023","pages":"7215--7223","booktitle":"IJCAI"}}
{"bib_id":"marouf2023weighted","title":"Weighted Ensemble Models Are Strong Continual Learners","author":"Marouf, Imad Eddine and Roy, Subhankar and Tartaglione, Enzo and Lathuilière, Stéphane","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2312.08977"}}
{"bib_id":"Zhang_2023_ICCV","title":"SLCA: Slow Learner with Classifier Alignment for Continual Learning on a Pre-trained Model","author":"Zhang, Gengwei and Wang, Liyuan and Kang, Guoliang and Chen, Ling and Wei, Yunchao","meta_info":{"pages":"19148-19158","year":"2023","month":"October","booktitle":"ICCV"}}
{"bib_id":"wang2022s","title":"S-prompts learning with pre-trained transformers: An occam’s razor for domain incremental learning","author":"Wang, Yabin and Huang, Zhiwu and Hong, Xiaopeng","meta_info":{"year":"2022","pages":"5682--5695","volume":"35","journal":"NeurIPS"}}
{"bib_id":"cao2023retentive","title":"Retentive or Forgetful? Diving into the Knowledge Memorizing Mechanism of Language Models","author":"Cao, Boxi and Tang, Qiaoyu and Lin, Hongyu and Han, Xianpei and Chen, Jiawei and Wang, Tianshu and Sun, Le","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2305.09144"}}
{"bib_id":"gunasekara2023survey","title":"Survey on online streaming continual learning","author":"Gunasekara, Nuwan and Pfahringer, Bernhard and Gomes, Heitor Murilo and Bifet, Albert","meta_info":{"year":"2023","pages":"6628--6637","booktitle":"IJCAI"}}
{"bib_id":"zhang2022benchmarking","title":"Benchmarking omni-vision representation through the lens of visual realms","author":"Zhang, Yuanhan and Yin, Zhenfei and Shao, Jing and Liu, Ziwei","meta_info":{"organization":"Springer","year":"2022","pages":"594--611","booktitle":"ECCV"}}
{"bib_id":"hendrycks2021many","title":"The many faces of robustness: A critical analysis of out-of-distribution generalization","author":"Hendrycks, Dan and Basart, Steven and Mu, Norman and Kadavath, Saurav and Wang, Frank and Dorundo, Evan and Desai, Rahul and Zhu, Tyler and Parajuli, Samyak and Guo, Mike and others","meta_info":{"year":"2021","pages":"8340--8349","booktitle":"ICCV"}}
{"bib_id":"hendrycks2021natural","title":"Natural adversarial examples","author":"Hendrycks, Dan and Zhao, Kevin and Basart, Steven and Steinhardt, Jacob and Song, Dawn","meta_info":{"year":"2021","pages":"15262--15271","booktitle":"CVPR"}}
{"bib_id":"barbu2019objectnet","title":"Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models","author":"Barbu, Andrei and Mayo, David and Alverio, Julian and Luo, William and Wang, Christopher and Gutfreund, Dan and Tenenbaum, Josh and Katz, Boris","meta_info":{"year":"2019","journal":"NeurIPS"}}
{"bib_id":"zhai2019large","title":"A large-scale study of representation learning with the visual task adaptation benchmark","author":"Zhai, Xiaohua and Puigcerver, Joan and Kolesnikov, Alexander and Ruyssen, Pierre and Riquelme, Carlos and Lucic, Mario and Djolonga, Josip and Pinto, Andre Susano and Neumann, Maxim and Dosovitskiy, Alexey and others","meta_info":{"year":"2019","journal":"arXiv preprint arXiv:1910.04867"}}
{"bib_id":"FortinC22","title":"Continual Semantic Segmentation Leveraging Image-level Labels and\nRehearsal","author":"Mathieu Pagé Fortin and\nBrahim Chaib-draa","meta_info":{"year":"2022","pages":"1268--1275","booktitle":"IJCAI"}}
{"bib_id":"IIM21","title":"Learning with Selective Forgetting","author":"Takashi Shibata and\nGo Irie and\nDaiki Ikami and\nYu Mitsuzumi","meta_info":{"year":"2021","pages":"989--996","booktitle":"IJCAI"}}
{"bib_id":"zhou2024expandable","title":"Expandable Subspace Ensemble for Pre-Trained Model-Based Class-Incremental Learning","author":"Zhou, Da-Wei and Sun, Hai-Long and Ye, Han-Jia and Zhan, De-Chuan","meta_info":{"year":"2024","booktitle":"CVPR"}}
{"bib_id":"sun2023self","title":"Self-supervised continual graph learning in adaptive riemannian spaces","author":"Sun, Li and Ye, Junda and Peng, Hao and Wang, Feiyang and Philip, S Yu","meta_info":{"year":"2023","pages":"4633--4642","number":"4","volume":"37","booktitle":"AAAI"}}
{"bib_id":"wiwatcharakoses2019self","title":"Self-Organizing Incremental Neural Networks for Continual Learning.","author":"Wiwatcharakoses, Chayut and Berrar, Daniel","meta_info":{"year":"2019","pages":"6476--6477","booktitle":"IJCAI"}}
{"bib_id":"li2022learning","title":"Learning from students: Online contrastive distillation network for general continual learning","author":"Li, Jin and Ji, Zhong and Wang, Gang and Wang, Qiang and Gao, Feng","meta_info":{"year":"2022","pages":"3215--3221","booktitle":"IJCAI"}}
