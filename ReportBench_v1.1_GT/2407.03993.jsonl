{"bib_id":"vafa2024large","title":"Do Large Language Models Perform the Way People Expect? Measuring the Human Generalization Function","author":"Keyon Vafa and Ashesh Rambachan and Sendhil Mullainathan","meta_info":{"journal":"arXiv preprint arXiv:2406.01382","year":"2024","url":"https:\/\/arxiv.org\/pdf\/2406.01382"}}
{"bib_id":"yu2024large","title":"Large language model as attributed training data generator: A tale of diversity and bias","author":"Yu, Yue and Zhuang, Yuchen and Zhang, Jieyu and Meng, Yu and Ratner, Alexander J and Krishna, Ranjay and Shen, Jiaming and Zhang, Chao","meta_info":{"year":"2024","volume":"36","journal":"Advances in Neural Information Processing Systems","url":"https:\/\/proceedings.neurips.cc\/paper_files\/paper\/2023\/hash\/ae9500c4f5607caf2eff033c67daa9d7-Abstract-Datasets_and_Benchmarks.html"}}
{"bib_id":"meng2023tuning","title":"Tuning language models as training data generators for augmentation-enhanced few-shot learning","author":"Meng, Yu and Michalski, Martin and Huang, Jiaxin and Zhang, Yu and Abdelzaher, Tarek and Han, Jiawei","meta_info":{"organization":"PMLR","year":"2023","pages":"24457--24477","booktitle":"International Conference on Machine Learning","url":"https:\/\/proceedings.mlr.press\/v202\/meng23b\/meng23b.pdf"}}
{"bib_id":"meng2022generating","title":"Generating training data with language models: Towards zero-shot language understanding","author":"Meng, Yu and Huang, Jiaxin and Zhang, Yu and Han, Jiawei","meta_info":{"year":"2022","pages":"462--477","volume":"35","journal":"Advances in Neural Information Processing Systems","url":"https:\/\/papers.nips.cc\/paper_files\/paper\/2022\/file\/0346c148ba1c21c6b4780a961ea141dc-Paper-Conference.pdf"}}
{"bib_id":"wu-etal-2021-polyjuice","title":"Polyjuice: Generating Counterfactuals for Explaining, Evaluating, and Improving Models","author":"Wu, Tongshuang  and\nRibeiro, Marco Tulio  and\nHeer, Jeffrey  and\nWeld, Daniel","meta_info":{"pages":"6707--6723","doi":"10.18653\/v1\/2021.acl-long.523","url":"https:\/\/aclanthology.org\/2021.acl-long.523","publisher":"Association for Computational Linguistics","address":"Online","year":"2021","month":"August","booktitle":"Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)"}}
{"bib_id":"wachter2017counterfactual","title":"Counterfactual explanations without opening the black box: Automated decisions and the GDPR","author":"Wachter, Sandra and Mittelstadt, Brent and Russell, Chris","meta_info":{"publisher":"HeinOnline","year":"2017","pages":"841","volume":"31","journal":"Harv. JL & Tech.","url":"https:\/\/jolt.law.harvard.edu\/assets\/articlePDFs\/v31\/Counterfactual-Explanations-without-Opening-the-Black-Box-Sandra-Wachter-et-al.pdf"}}
{"bib_id":"kaushik2019learning","title":"Learning The Difference That Makes A Difference With Counterfactually-Augmented Data","author":"Kaushik, Divyansh and Hovy, Eduard and Lipton, Zachary","meta_info":{"pages":"1-17","year":"2019","booktitle":"International Conference on Learning Representations","url":"https:\/\/openreview.net\/pdf\/6267805abf4a2ab7b3f971c759b8c669e6060774.pdf"}}
{"bib_id":"bhattacharjee2024llmguided","title":"Towards LLM-guided Causal Explainability for Black-box Text Classifiers","author":"Amrita Bhattacharjee and Raha Moraffah and Joshua Garland and Huan Liu","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2309.13340","url":"https:\/\/arxiv.org\/pdf\/2309.13340"}}
{"bib_id":"verma2020counterfactual","title":"Counterfactual explanations and algorithmic recourses for machine learning: A review","author":"Verma, Sahil and Boonsanong, Varich and Hoang, Minh and Hines, Keegan E and Dickerson, John P and Shah, Chirag","meta_info":{"year":"2020","journal":"arXiv preprint arXiv:2010.10596","url":"https:\/\/arxiv.org\/pdf\/2010.10596"}}
{"bib_id":"stepin2021survey","title":"A survey of contrastive and counterfactual explanation generation methods for explainable artificial intelligence","author":"Stepin, Ilia and Alonso, Jose M and Catala, Alejandro and Pereira-Fariña, Martı́n","meta_info":{"publisher":"IEEE","year":"2021","pages":"11974--12001","volume":"9","journal":"IEEE Access","url":"https:\/\/ieeexplore.ieee.org\/document\/9321372"}}
{"bib_id":"guidotti2022counterfactual","title":"Counterfactual explanations and how to find them: literature review and benchmarking","author":"Guidotti, Riccardo","meta_info":{"publisher":"Springer","year":"2022","pages":"1--55","journal":"Data Mining and Knowledge Discovery","url":"https:\/\/doi.org\/10.1007\/s10618-022-00831-6"}}
{"bib_id":"madaan2021generate","title":"Generate your counterfactuals: Towards controlled counterfactual generation for text","author":"Madaan, Nishtha and Padhi, Inkit and Panwar, Naveen and Saha, Diptikalyan","meta_info":{"year":"2021","pages":"13516--13524","volume":"35(15)","booktitle":"Proceedings of the AAAI Conference on Artificial Intelligence","url":"https:\/\/ojs.aaai.org\/index.php\/AAAI\/article\/view\/17594"}}
{"bib_id":"robeer2021generating","title":"Generating realistic natural language counterfactuals","author":"Robeer, Marcel and Bex, Floris and Feelders, Ad","meta_info":{"year":"2021","pages":"3611--3625","booktitle":"Findings of the Association for Computational Linguistics: EMNLP 2021","url":"https:\/\/aclanthology.org\/2021.findings-emnlp.306"}}
{"bib_id":"chen2023disco","title":"DISCO: distilling counterfactuals with large language models","author":"Chen, Zeming and Gao, Qiyue and Bosselut, Antoine and Sabharwal, Ashish and Richardson, Kyle","meta_info":{"year":"2023","pages":"5514--5528","booktitle":"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","url":"https:\/\/aclanthology.org\/2023.acl-long.302"}}
{"bib_id":"ross-etal-2021-explaining","title":"Explaining NLP Models via Minimal Contrastive Editing (MiCE)","author":"Ross, Alexis  and\nMarasović, Ana  and\nPeters, Matthew","meta_info":{"pages":"3840--3852","doi":"10.18653\/v1\/2021.findings-acl.336","url":"https:\/\/aclanthology.org\/2021.findings-acl.336","publisher":"Association for Computational Linguistics","address":"Online","year":"2021","month":"August","booktitle":"Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021","editor":"Zong, Chengqing  and\nXia, Fei  and\nLi, Wenjie  and\nNavigli, Roberto"}}
{"bib_id":"qin2019counterfactual","title":"Counterfactual Story Reasoning and Generation","author":"Qin, Lianhui and Bosselut, Antoine and Holtzman, Ari and Bhagavatula, Chandra and Clark, Elizabeth and Choi, Yejin","meta_info":{"year":"2019","pages":"5043--5053","booktitle":"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)","url":"https:\/\/aclanthology.org\/D19-1509"}}
{"bib_id":"chen2021kace","title":"KACE: Generating knowledge aware contrastive explanations for natural language inference","author":"Chen, Qianglong and Ji, Feng and Zeng, Xiangji and Li, Feng-Lin and Zhang, Ji and Chen, Haiqing and Zhang, Yin","meta_info":{"year":"2021","pages":"2516--2527","booktitle":"Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)","url":"https:\/\/aclanthology.org\/2021.acl-long.196"}}
{"bib_id":"gat2023faithful","title":"Faithful Explanations of Black-box NLP Models Using LLM-generated Counterfactuals","author":"Yair Ori Gat and Nitay Calderon and Amir Feder and Alexander Chapanin and Amit Sharma and Roi Reichart","meta_info":{"pages":"1-34","url":"https:\/\/openreview.net\/pdf?id=UMfcdRIotC","year":"2024","booktitle":"The Twelfth International Conference on Learning Representations"}}
{"bib_id":"jung2022counterfactual","title":"Counterfactual explanation based on gradual construction for deep networks","author":"Jung, Hong-Gyu and Kang, Sin-Han and Kim, Hee-Dong and Won, Dong-Ok and Lee, Seong-Whan","meta_info":{"publisher":"Elsevier","year":"2022","pages":"108958","volume":"132","journal":"Pattern Recognition","url":"https:\/\/www.sciencedirect.com\/science\/article\/pii\/S0031320322004381"}}
{"bib_id":"khashabi-etal-2020-bang","title":"More Bang for Your Buck: Natural Perturbation for Robust Question Answering","author":"Khashabi, Daniel  and\nKhot, Tushar  and\nSabharwal, Ashish","meta_info":{"pages":"163--170","doi":"10.18653\/v1\/2020.emnlp-main.12","url":"https:\/\/aclanthology.org\/2020.emnlp-main.12","publisher":"Association for Computational Linguistics","address":"Online","year":"2020","month":"November","booktitle":"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)","editor":"Webber, Bonnie  and\nCohn, Trevor  and\nHe, Yulan  and\nLiu, Yang"}}
{"bib_id":"dixit-etal-2022-core","title":"CORE: A Retrieve-then-Edit Framework for Counterfactual Data Generation","author":"Dixit, Tanay  and\nParanjape, Bhargavi  and\nHajishirzi, Hannaneh  and\nZettlemoyer, Luke","meta_info":{"abstract":"Counterfactual data augmentation (CDA) -- i.e., adding minimally perturbed inputs during training -- helps reduce model reliance on spurious correlations and improves generalization to out-of-distribution (OOD) data. Prior work on generating counterfactuals only considered restricted classes of perturbations, limiting their effectiveness. We present Counterfactual Generation via Retrieval and Editing (CORE), a retrieval-augmented generation framework for creating diverse counterfactual perturbations for CDA. For each training example, CORE first performs a dense retrieval over a task-related unlabeled text corpus using a learned bi-encoder and extracts relevant counterfactual excerpts. CORE then incorporates these into prompts to a large language model with few-shot learning capabilities, for counterfactual editing. Conditioning language model edits on naturally occurring data results in more diverse perturbations. Experiments on natural language inference and sentiment analysis benchmarks show that CORE counterfactuals are more effective at improving generalization to OOD data compared to other DA approaches. We also show that the CORE retrieval framework can be used to encourage diversity in manually authored perturbations.","pages":"2964--2984","doi":"10.18653\/v1\/2022.findings-emnlp.216","url":"https:\/\/aclanthology.org\/2022.findings-emnlp.216","publisher":"Association for Computational Linguistics","address":"Abu Dhabi, United Arab Emirates","year":"2022","month":"December","booktitle":"Findings of the Association for Computational Linguistics: EMNLP 2022","editor":"Goldberg, Yoav  and\nKozareva, Zornitsa  and\nZhang, Yue"}}
{"bib_id":"balashankar2023improving","title":"Improving Classifier Robustness through Active Generative Counterfactual Data Augmentation","author":"Balashankar, Ananth and Wang, Xuezhi and Qin, Yao and Packer, Ben and Thain, Nithum and Chi, Ed and Chen, Jilin and Beutel, Alex","meta_info":{"year":"2023","pages":"127--139","booktitle":"Findings of the Association for Computational Linguistics: EMNLP 2023","url":"https:\/\/aclanthology.org\/2023.findings-emnlp.10"}}
{"bib_id":"wen2022autocad","title":"AutoCAD: Automatically Generate Counterfactuals for Mitigating Shortcut Learning","author":"Wen, Jiaxin and Zhu, Yeshuang and Zhang, Jinchao and Zhou, Jie and Huang, Minlie","meta_info":{"year":"2022","pages":"2302--2317","booktitle":"Findings of the Association for Computational Linguistics: EMNLP 2022","url":"https:\/\/aclanthology.org\/2022.findings-emnlp.170"}}
{"bib_id":"paranjape-etal-2022-retrieval","title":"Retrieval-guided Counterfactual Generation for QA","author":"Paranjape, Bhargavi  and\nLamm, Matthew  and\nTenney, Ian","meta_info":{"pages":"1670--1686","doi":"10.18653\/v1\/2022.acl-long.117","url":"https:\/\/aclanthology.org\/2022.acl-long.117","publisher":"Association for Computational Linguistics","address":"Dublin, Ireland","year":"2022","month":"May","booktitle":"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","editor":"Muresan, Smaranda  and\nNakov, Preslav  and\nVillavicencio, Aline"}}
{"bib_id":"zhu-etal-2023-explain","title":"EXPLAIN, EDIT, GENERATE: Rationale-Sensitive Counterfactual Data Augmentation for Multi-hop Fact Verification","author":"Zhu, Yingjie  and\nSi, Jiasheng  and\nZhao, Yibo  and\nZhu, Haiyang  and\nZhou, Deyu  and\nHe, Yulan","meta_info":{"pages":"13377--13392","doi":"10.18653\/v1\/2023.emnlp-main.826","url":"https:\/\/aclanthology.org\/2023.emnlp-main.826","publisher":"Association for Computational Linguistics","address":"Singapore","year":"2023","month":"December","booktitle":"Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing","editor":"Bouamor, Houda  and\nPino, Juan  and\nBali, Kalika"}}
{"bib_id":"yan2024counterfactual","title":"Counterfactual generation with identifiability guarantees","author":"Yan, Hanqi and Kong, Lingjing and Gui, Lin and Chi, Yuejie and Xing, Eric and He, Yulan and Zhang, Kun","meta_info":{"year":"2024","volume":"36","journal":"Advances in Neural Information Processing Systems","url":"https:\/\/proceedings.neurips.cc\/paper_files\/paper\/2023\/file\/afda6bf3fb086eabbaf161ba1cec5a9a-Paper-Conference.pdf"}}
{"bib_id":"sen-etal-2023-people","title":"People Make Better Edits: Measuring the Efficacy of LLM-Generated Counterfactually Augmented Data for Harmful Language Detection","author":"Sen, Indira  and\nAssenmacher, Dennis  and\nSamory, Mattia  and\nAugenstein, Isabelle  and\nAalst, Wil  and\nWagner, Claudia","meta_info":{"pages":"10480--10504","doi":"10.18653\/v1\/2023.emnlp-main.649","url":"https:\/\/aclanthology.org\/2023.emnlp-main.649","publisher":"Association for Computational Linguistics","address":"Singapore","year":"2023","month":"December","booktitle":"Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing","editor":"Bouamor, Houda  and\nPino, Juan  and\nBali, Kalika"}}
{"bib_id":"gardner-etal-2020-evaluating","title":"Evaluating Models′ Local Decision Boundaries via Contrast Sets","author":"Gardner, Matt  and\nArtzi, Yoav  and\nBasmov, Victoria  and\nBerant, Jonathan  and\nBogin, Ben  and\nChen, Sihao  and\nDasigi, Pradeep  and\nDua, Dheeru  and\nElazar, Yanai  and\nGottumukkala, Ananth  and\nGupta, Nitish  and\nHajishirzi, Hannaneh  and\nIlharco, Gabriel  and\nKhashabi, Daniel  and\nLin, Kevin  and\nLiu, Jiangming  and\nLiu, Nelson F.  and\nMulcaire, Phoebe  and\nNing, Qiang  and\nSingh, Sameer  and\nSmith, Noah A.  and\nSubramanian, Sanjay  and\nTsarfaty, Reut  and\nWallace, Eric  and\nZhang, Ally  and\nZhou, Ben","meta_info":{"pages":"1307--1323","doi":"10.18653\/v1\/2020.findings-emnlp.117","url":"https:\/\/aclanthology.org\/2020.findings-emnlp.117","publisher":"Association for Computational Linguistics","address":"Online","year":"2020","month":"November","booktitle":"Findings of the Association for Computational Linguistics: EMNLP 2020","editor":"Cohn, Trevor  and\nHe, Yulan  and\nLiu, Yang"}}
{"bib_id":"ribeiro-etal-2020-beyond","title":"Beyond Accuracy: Behavioral Testing of NLP Models with CheckList","author":"Ribeiro, Marco Tulio  and\nWu, Tongshuang  and\nGuestrin, Carlos  and\nSingh, Sameer","meta_info":{"pages":"4902--4912","doi":"10.18653\/v1\/2020.acl-main.442","url":"https:\/\/aclanthology.org\/2020.acl-main.442","publisher":"Association for Computational Linguistics","address":"Online","year":"2020","month":"July","booktitle":"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics","editor":"Jurafsky, Dan  and\nChai, Joyce  and\nSchluter, Natalie  and\nTetreault, Joel"}}
{"bib_id":"samory2021call","title":"“Call me sexist, but...”: Revisiting Sexism Detection Using Psychological Scales and Adversarial Samples","author":"Samory, Mattia and Sen, Indira and Kohne, Julian and Flöck, Fabian and Wagner, Claudia","meta_info":{"year":"2021","pages":"573--584","volume":"15(1)","booktitle":"Proceedings of the international AAAI conference on web and social media","url":"https:\/\/ojs.aaai.org\/index.php\/ICWSM\/article\/view\/18085"}}
{"bib_id":"betti2023relevance","title":"Relevance-based Infilling for Natural Language Counterfactuals","author":"Betti, Lorenzo and Abrate, Carlo and Bonchi, Francesco and Kaltenbrunner, Andreas","meta_info":{"year":"2023","pages":"88--98","booktitle":"Proceedings of the 32nd ACM International Conference on Information and Knowledge Management","url":"https:\/\/doi.org\/10.1145\/3583780.3615029"}}
{"bib_id":"howard-etal-2022-neurocounterfactuals","title":"NeuroCounterfactuals: Beyond Minimal-Edit Counterfactuals for Richer Data Augmentation","author":"Howard, Phillip  and\nSinger, Gadi  and\nLal, Vasudev  and\nChoi, Yejin  and\nSwayamdipta, Swabha","meta_info":{"pages":"5056--5072","doi":"10.18653\/v1\/2022.findings-emnlp.371","url":"https:\/\/aclanthology.org\/2022.findings-emnlp.371","publisher":"Association for Computational Linguistics","address":"Abu Dhabi, United Arab Emirates","year":"2022","month":"December","booktitle":"Findings of the Association for Computational Linguistics: EMNLP 2022","editor":"Goldberg, Yoav  and\nKozareva, Zornitsa  and\nZhang, Yue"}}
{"bib_id":"sachdeva-etal-2024-catfood","title":"CATfOOD: Counterfactual Augmented Training for Improving Out-of-Domain Performance and Calibration","author":"Sachdeva, Rachneet  and\nTutek, Martin  and\nGurevych, Iryna","meta_info":{"pages":"1876--1898","url":"https:\/\/aclanthology.org\/2024.eacl-long.113","publisher":"Association for Computational Linguistics","address":"St. Julian′s, Malta","year":"2024","month":"March","booktitle":"Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)","editor":"Graham, Yvette  and\nPurver, Matthew"}}
{"bib_id":"wu2024novel","title":"A Novel Counterfactual Data Augmentation Method for Aspect-Based Sentiment Analysis","author":"Wu, Dongming and Wen, Lulu and Chen, Chao and Shi, Zhaoshu","meta_info":{"organization":"PMLR","year":"2024","pages":"1479--1493","booktitle":"Asian Conference on Machine Learning","url":"https:\/\/proceedings.mlr.press\/v222\/wu24a\/wu24a.pdf"}}
{"bib_id":"yang-etal-2021-exploring","title":"Exploring the Efficacy of Automatically Generated Counterfactuals for Sentiment Analysis","author":"Yang, Linyi  and\nLi, Jiazheng  and\nCunningham, Padraig  and\nZhang, Yue  and\nSmyth, Barry  and\nDong, Ruihai","meta_info":{"pages":"306--316","url":"https:\/\/aclanthology.org\/2021.acl-long.26","publisher":"Association for Computational Linguistics","address":"Online","year":"2021","month":"August","booktitle":"Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)"}}
{"bib_id":"ross-etal-2022-tailor","title":"Tailor: Generating and Perturbing Text with Semantic Controls","author":"Ross, Alexis  and\nWu, Tongshuang  and\nPeng, Hao  and\nPeters, Matthew  and\nGardner, Matt","meta_info":{"abstract":"Controlled text perturbation is useful for evaluating and improving model generalizability. However, current techniques rely on training a model for every target perturbation, which is expensive and hard to generalize. We present Tailor, a semantically-controlled text generation system. Tailor builds on a pretrained seq2seq model and produces textual outputs conditioned on control codes derived from semantic representations. We craft a set of operations to modify the control codes, which in turn steer generation towards targeted attributes. These operations can be further composed into higher-level ones, allowing for flexible perturbation strategies. We demonstrate the effectiveness of these perturbations in multiple applications. First, we use Tailor to automatically create high-quality contrast sets for four distinct natural language processing (NLP) tasks. These contrast sets contain fewer spurious artifacts and are complementary to manually annotated ones in their lexical diversity. Second, we show that Tailor perturbations can improve model generalization through data augmentation. Perturbing just ∼2% of training data leads to a 5.8-point gain on an NLI challenge set measuring reliance on syntactic heuristics.","pages":"3194--3213","doi":"10.18653\/v1\/2022.acl-long.228","url":"https:\/\/aclanthology.org\/2022.acl-long.228","publisher":"Association for Computational Linguistics","address":"Dublin, Ireland","year":"2022","month":"May","booktitle":"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","editor":"Muresan, Smaranda  and\nNakov, Preslav  and\nVillavicencio, Aline"}}
{"bib_id":"li-etal-2020-linguistically","title":"Linguistically-Informed Transformations (LIT): A Method for Automatically Generating Contrast Sets","author":"Li, Chuanrong  and\nShengshuo, Lin  and\nLiu, Zeyu  and\nWu, Xinyi  and\nZhou, Xuhui  and\nSteinert-Threlkeld, Shane","meta_info":{"abstract":"Although large-scale pretrained language models, such as BERT and RoBERTa, have achieved superhuman performance on in-distribution test sets, their performance suffers on out-of-distribution test sets (e.g., on contrast sets). Building contrast sets often requires human-expert annotation, which is expensive and hard to create on a large scale. In this work, we propose a Linguistically-Informed Transformation (LIT) method to automatically generate contrast sets, which enables practitioners to explore linguistic phenomena of interests as well as compose different phenomena. Experimenting with our method on SNLI and MNLI shows that current pretrained language models, although being claimed to contain sufficient linguistic knowledge, struggle on our automatically generated contrast sets. Furthermore, we improve models′ performance on the contrast sets by applying LIT to augment the training data, without affecting performance on the original data.","pages":"126--135","doi":"10.18653\/v1\/2020.blackboxnlp-1.12","url":"https:\/\/aclanthology.org\/2020.blackboxnlp-1.12","publisher":"Association for Computational Linguistics","address":"Online","year":"2020","month":"November","booktitle":"Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP","editor":"Alishahi, Afra  and\nBelinkov, Yonatan  and\nChrupała, Grzegorz  and\nHupkes, Dieuwke  and\nPinter, Yuval  and\nSajjad, Hassan"}}
{"bib_id":"wang2021robustness","title":"Robustness to Spurious Correlations in Text Classification via Automatically Generated Counterfactuals","author":"Wang, Zhao and Culotta, Aron","meta_info":{"pages":"14024-14031","month":"May","year":"2021","journal":"Proceedings of the AAAI Conference on Artificial Intelligence","abstractnote":"Spurious correlations threaten the validity of statistical classifiers. While model accuracy may appear high when the test data is from the same distribution as the training data, it can quickly degrade when the test distribution changes. For example, it has been shown that classifiers perform poorly when humans make minor modifications to change the label of an example. One solution to increase model reliability and generalizability is to identify causal associations between features and classes. In this paper, we propose to train a robust text classifier by augmenting the training data with automatically generated counterfactual data. We first identify likely causal features using a statistical matching approach. Next, we generate counterfactual samples for the original training data by substituting causal features with their antonyms and then assigning opposite labels to the counterfactual samples. Finally, we combine the original data and counterfactual data to train a robust classifier. Experiments on two classification tasks show that a traditional classifier trained on the original data does very poorly on human-generated counterfactual samples (e.g., 10%-37% drop in accuracy). However, the classifier trained on the combined data is more robust and performs well on both the original test data and the counterfactual test data (e.g., 12%-25% increase in accuracy compared with the traditional classifier). Detailed analysis shows that the robust classifier makes meaningful and trustworthy predictions by emphasizing causal features and de-emphasizing non-causal features.","doi":"10.1609\/aaai.v35i16.17651","url":"https:\/\/ojs.aaai.org\/index.php\/AAAI\/article\/view\/17651","volume":"35(16)"}}
{"bib_id":"lee2021crossaug","title":"Crossaug: A contrastive data augmentation method for debiasing fact verification models","author":"Lee, Minwoo and Won, Seungpil and Kim, Juae and Lee, Hwanhee and Park, Cheoneum and Jung, Kyomin","meta_info":{"year":"2021","pages":"3181--3185","booktitle":"Proceedings of the 30th ACM International Conference on Information & Knowledge Management","url":"https:\/\/doi.org\/10.1145\/3459637.3482078"}}
{"bib_id":"gilo2024general","title":"A General Search-Based Framework for Generating Textual Counterfactual Explanations","author":"Gilo, Daniel and Markovitch, Shaul","meta_info":{"year":"2024","pages":"18073--18081","volume":"38(16)","booktitle":"Proceedings of the AAAI Conference on Artificial Intelligence","url":"https:\/\/ojs.aaai.org\/index.php\/AAAI\/article\/view\/29764"}}
{"bib_id":"treviso-etal-2023-crest","title":"CREST: A Joint Framework for Rationalization and Counterfactual Text Generation","author":"Treviso, Marcos  and\nRoss, Alexis  and\nGuerreiro, Nuno M.  and\nMartins, André","meta_info":{"abstract":"Selective rationales and counterfactual examples have emerged as two effective, complementary classes of interpretability methods for analyzing and training NLP models. However, prior work has not explored how these methods can be integrated to combine their complementary advantages. We overcome this limitation by introducing CREST (ContRastive Edits with Sparse raTionalization), a joint framework for selective rationalization and counterfactual text generation, and show that this framework leads to improvements in counterfactual quality, model robustness, and interpretability. First, CREST generates valid counterfactuals that are more natural than those produced by previous methods, and subsequently can be used for data augmentation at scale, reducing the need for human-generated examples. Second, we introduce a new loss function that leverages CREST counterfactuals to regularize selective rationales and show that this regularization improves both model robustness and rationale quality, compared to methods that do not leverage CREST counterfactuals. Our results demonstrate that CREST successfully bridges the gap between selective rationales and counterfactual examples, addressing the limitations of existing methods and providing a more comprehensive view of a model′s predictions.","pages":"15109--15126","doi":"10.18653\/v1\/2023.acl-long.842","url":"https:\/\/aclanthology.org\/2023.acl-long.842","publisher":"Association for Computational Linguistics","address":"Toronto, Canada","year":"2023","month":"July","booktitle":"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","editor":"Rogers, Anna  and\nBoyd-Graber, Jordan  and\nOkazaki, Naoaki"}}
{"bib_id":"hu2021causal","title":"A causal lens for controllable text generation","author":"Hu, Zhiting and Li, Li Erran","meta_info":{"year":"2021","pages":"24941--24955","volume":"34","journal":"In Advances in Neural Information Processing Systems","url":"https:\/\/papers.nips.cc\/paper\/2021\/file\/d0f5edad9ac19abed9e235c0fe0aa59f-Paper.pdf"}}
{"bib_id":"kuhn2023semantic","title":"Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation","author":"Lorenz Kuhn and Yarin Gal and Sebastian Farquhar","meta_info":{"pages":"1-17","url":"https:\/\/openreview.net\/pdf?id=VD-AYtP0dve","year":"2023","booktitle":"The Eleventh International Conference on Learning Representations "}}
{"bib_id":"zhu2018texygen","title":"Texygen: A benchmarking platform for text generation models","author":"Zhu, Yaoming and Lu, Sidi and Zheng, Lei and Guo, Jiaxian and Zhang, Weinan and Wang, Jun and Yu, Yong","meta_info":{"year":"2018","pages":"1097--1100","booktitle":"The 41st international ACM SIGIR conference on research & development in information retrieval","url":"https:\/\/doi.org\/10.1145\/3209978.3210080"}}
{"bib_id":"papineni-etal-2002-bleu","title":"Bleu: a Method for Automatic Evaluation of Machine Translation","author":"Papineni, Kishore  and\nRoukos, Salim  and\nWard, Todd  and\nZhu, Wei-Jing","meta_info":{"pages":"311--318","doi":"10.3115\/1073083.1073135","url":"https:\/\/aclanthology.org\/P02-1040","publisher":"Association for Computational Linguistics","address":"Philadelphia, Pennsylvania, USA","year":"2002","month":"July","booktitle":"Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics","editor":"Isabelle, Pierre  and\nCharniak, Eugene  and\nLin, Dekang"}}
{"bib_id":"li-etal-2016-diversity","title":"A Diversity-Promoting Objective Function for Neural Conversation Models","author":"Li, Jiwei  and\nGalley, Michel  and\nBrockett, Chris  and\nGao, Jianfeng  and\nDolan, Bill","meta_info":{"pages":"110--119","doi":"10.18653\/v1\/N16-1014","url":"https:\/\/aclanthology.org\/N16-1014","publisher":"Association for Computational Linguistics","address":"San Diego, California","year":"2016","month":"June","booktitle":"Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies","editor":"Knight, Kevin  and\nNenkova, Ani  and\nRambow, Owen"}}
{"bib_id":"reimers2019sentence","title":"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks","author":"Reimers, Nils and Gurevych, Iryna","meta_info":{"year":"2019","pages":"3982--3992","booktitle":"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)","url":"https:\/\/aclanthology.org\/D19-1410"}}
{"bib_id":"alvarez2020geometric","title":"Geometric dataset distances via optimal transport","author":"Alvarez-Melis, David and Fusi, Nicolo","meta_info":{"year":"2020","pages":"21428--21439","volume":"33","journal":"Advances in Neural Information Processing Systems","url":"https:\/\/proceedings.neurips.cc\/paper\/2020\/file\/f52a7b2610fb4d3f74b4106fb80b233d-Paper.pdf"}}
{"bib_id":"zhang2019bertscore","title":"BERTScore: Evaluating Text Generation with BERT","author":"Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q and Artzi, Yoav","meta_info":{"pages":"1-43","year":"2020","booktitle":"International Conference on Learning Representations","url":"http:\/\/www.openreview.net\/pdf?id=SkeHuCVFDr"}}
{"bib_id":"zhang1989simple","title":"Simple fast algorithms for the editing distance between trees and related problems","author":"Zhang, Kaizhong and Shasha, Dennis","meta_info":{"publisher":"SIAM","year":"1989","pages":"1245--1262","number":"6","volume":"18","journal":"SIAM journal on computing","url":"https:\/\/doi.org\/10.1137\/0218082"}}
{"bib_id":"levenshtein1966binary","title":"Binary codes capable of correcting deletions, insertions, and reversals","author":"Levenshtein, Vladimir I and others","meta_info":{"organization":"Soviet Union","year":"1966","pages":"707--710","volume":"10","booktitle":"Soviet physics doklady","url":"https:\/\/nymity.ch\/sybilhunting\/pdf\/Levenshtein1966a.pdf"}}
{"bib_id":"cer-etal-2018-universal","title":"Universal Sentence Encoder for English","author":"Cer, Daniel  and\nYang, Yinfei  and\nKong, Sheng-yi  and\nHua, Nan  and\nLimtiaco, Nicole  and\nSt. John, Rhomni  and\nConstant, Noah  and\nGuajardo-Cespedes, Mario  and\nYuan, Steve  and\nTar, Chris  and\nStrope, Brian  and\nKurzweil, Ray","meta_info":{"pages":"169--174","doi":"10.18653\/v1\/D18-2029","url":"https:\/\/aclanthology.org\/D18-2029","publisher":"Association for Computational Linguistics","address":"Brussels, Belgium","year":"2018","month":"November","booktitle":"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations","editor":"Blanco, Eduardo  and\nLu, Wei"}}
{"bib_id":"lin-2004-rouge","title":"ROUGE: A Package for Automatic Evaluation of Summaries","author":"Lin, Chin-Yew","meta_info":{"pages":"74--81","url":"https:\/\/aclanthology.org\/W04-1013","publisher":"Association for Computational Linguistics","address":"Barcelona, Spain","year":"2004","month":"July","booktitle":"Text Summarization Branches Out"}}
{"bib_id":"denkowski-lavie-2011-meteor","title":"Meteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems","author":"Denkowski, Michael  and\nLavie, Alon","meta_info":{"pages":"85--91","url":"https:\/\/aclanthology.org\/W11-2107","publisher":"Association for Computational Linguistics","address":"Edinburgh, Scotland","year":"2011","month":"July","booktitle":"Proceedings of the Sixth Workshop on Statistical Machine Translation","editor":"Callison-Burch, Chris  and\nKoehn, Philipp  and\nMonz, Christof  and\nZaidan, Omar F."}}
{"bib_id":"zhao-etal-2019-moverscore","title":"MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance","author":"Zhao, Wei  and\nPeyrard, Maxime  and\nLiu, Fei  and\nGao, Yang  and\nMeyer, Christian M.  and\nEger, Steffen","meta_info":{"pages":"563--578","doi":"10.18653\/v1\/D19-1053","url":"https:\/\/aclanthology.org\/D19-1053","publisher":"Association for Computational Linguistics","address":"Hong Kong, China","year":"2019","month":"November","booktitle":"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)","editor":"Inui, Kentaro  and\nJiang, Jing  and\nNg, Vincent  and\nWan, Xiaojun"}}
{"bib_id":"salazar-etal-2020-masked","title":"Masked Language Model Scoring","author":"Salazar, Julian  and\nLiang, Davis  and\nNguyen, Toan Q.  and\nKirchhoff, Katrin","meta_info":{"pages":"2699--2712","doi":"10.18653\/v1\/2020.acl-main.240","url":"https:\/\/aclanthology.org\/2020.acl-main.240","publisher":"Association for Computational Linguistics","address":"Online","year":"2020","month":"July","booktitle":"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics","editor":"Jurafsky, Dan  and\nChai, Joyce  and\nSchluter, Natalie  and\nTetreault, Joel"}}
{"bib_id":"longpre-etal-2021-entity","title":"Entity-Based Knowledge Conflicts in Question Answering","author":"Longpre, Shayne  and\nPerisetla, Kartik  and\nChen, Anthony  and\nRamesh, Nikhil  and\nDuBois, Chris  and\nSingh, Sameer","meta_info":{"pages":"7052--7063","doi":"10.18653\/v1\/2021.emnlp-main.565","url":"https:\/\/aclanthology.org\/2021.emnlp-main.565","publisher":"Association for Computational Linguistics","address":"Online and Punta Cana, Dominican Republic","year":"2021","month":"November","booktitle":"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing","editor":"Moens, Marie-Francine  and\nHuang, Xuanjing  and\nSpecia, Lucia  and\nYih, Scott Wen-tau"}}
{"bib_id":"reid-zhong-2021-lewis","title":"LEWIS: Levenshtein Editing for Unsupervised Text Style Transfer","author":"Reid, Machel  and\nZhong, Victor","meta_info":{"pages":"3932--3944","doi":"10.18653\/v1\/2021.findings-acl.344","url":"https:\/\/aclanthology.org\/2021.findings-acl.344","publisher":"Association for Computational Linguistics","address":"Online","year":"2021","month":"August","booktitle":"Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021","editor":"Zong, Chengqing  and\nXia, Fei  and\nLi, Wenjie  and\nNavigli, Roberto"}}
{"bib_id":"madaan-etal-2020-politeness","title":"Politeness Transfer: A Tag and Generate Approach","author":"Madaan, Aman  and\nSetlur, Amrith  and\nParekh, Tanmay  and\nPoczos, Barnabas  and\nNeubig, Graham  and\nYang, Yiming  and\nSalakhutdinov, Ruslan  and\nBlack, Alan W  and\nPrabhumoye, Shrimai","meta_info":{"pages":"1869--1881","doi":"10.18653\/v1\/2020.acl-main.169","url":"https:\/\/aclanthology.org\/2020.acl-main.169","publisher":"Association for Computational Linguistics","address":"Online","year":"2020","month":"July","booktitle":"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics","editor":"Jurafsky, Dan  and\nChai, Joyce  and\nSchluter, Natalie  and\nTetreault, Joel"}}
{"bib_id":"malmi-etal-2020-unsupervised","title":"Unsupervised Text Style Transfer with Padded Masked Language Models","author":"Malmi, Eric  and\nSeveryn, Aliaksei  and\nRothe, Sascha","meta_info":{"pages":"8671--8680","doi":"10.18653\/v1\/2020.emnlp-main.699","url":"https:\/\/aclanthology.org\/2020.emnlp-main.699","publisher":"Association for Computational Linguistics","address":"Online","year":"2020","month":"November","booktitle":"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)","editor":"Webber, Bonnie  and\nCohn, Trevor  and\nHe, Yulan  and\nLiu, Yang"}}
{"bib_id":"martens2014explaining","title":"Explaining data-driven document classifications","author":"Martens, David and Provost, Foster","meta_info":{"publisher":"JSTOR","year":"2014","pages":"73--100","number":"1","volume":"38","journal":"MIS quarterly","url":"https:\/\/doi.org\/10.25300\/MISQ\/2014\/38.1.04"}}
{"bib_id":"ramon2020comparison","title":"A comparison of instance-level counterfactual explanation algorithms for behavioral and textual data: SEDC, LIME-C and SHAP-C","author":"Ramon, Yanou and Martens, David and Provost, Foster and Evgeniou, Theodoros","meta_info":{"publisher":"Springer","year":"2020","pages":"801--819","volume":"14","journal":"Advances in Data Analysis and Classification","url":"https:\/\/doi.org\/10.1007\/s11634-020-00418-3"}}
{"bib_id":"li-etal-2018-delete","title":"Delete, Retrieve, Generate: a Simple Approach to Sentiment and Style Transfer","author":"Li, Juncen  and\nJia, Robin  and\nHe, He  and\nLiang, Percy","meta_info":{"pages":"1865--1874","doi":"10.18653\/v1\/N18-1169","url":"https:\/\/aclanthology.org\/N18-1169","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)","editor":"Walker, Marilyn  and\nJi, Heng  and\nStent, Amanda"}}
{"bib_id":"yang-etal-2020-generating","title":"Generating Plausible Counterfactual Explanations for Deep Transformers in Financial Text Classification","author":"Yang, Linyi  and\nKenny, Eoin  and\nNg, Tin Lok James  and\nYang, Yi  and\nSmyth, Barry  and\nDong, Ruihai","meta_info":{"pages":"6150--6160","doi":"10.18653\/v1\/2020.coling-main.541","url":"https:\/\/aclanthology.org\/2020.coling-main.541","publisher":"International Committee on Computational Linguistics","address":"Barcelona, Spain (Online)","year":"2020","month":"December","booktitle":"Proceedings of the 28th International Conference on Computational Linguistics","editor":"Scott, Donia  and\nBel, Nuria  and\nZong, Chengqing"}}
{"bib_id":"ijcai2019-732","title":"Mask and Infill: Applying Masked Language Model for Sentiment Transfer","author":"Wu, Xing and Zhang, Tao and Zang, Liangjun and Han, Jizhong and Hu, Songlin","meta_info":{"url":"https:\/\/doi.org\/10.24963\/ijcai.2019\/732","doi":"10.24963\/ijcai.2019\/732","month":"7","year":"2019","pages":"5271--5277","publisher":"International Joint Conferences on Artificial Intelligence Organization","booktitle":"Proceedings of the Twenty-Eighth International Joint Conference on\nArtificial Intelligence, IJCAI-19"}}
{"bib_id":"sha-etal-2021-controlling","title":"Controlling Text Edition by Changing Answers of Specific Questions","author":"Sha, Lei  and\nHohenecker, Patrick  and\nLukasiewicz, Thomas","meta_info":{"pages":"1288--1299","doi":"10.18653\/v1\/2021.findings-acl.110","url":"https:\/\/aclanthology.org\/2021.findings-acl.110","publisher":"Association for Computational Linguistics","address":"Online","year":"2021","month":"August","booktitle":"Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021","editor":"Zong, Chengqing  and\nXia, Fei  and\nLi, Wenjie  and\nNavigli, Roberto"}}
{"bib_id":"ross2021learning","title":"Learning models for actionable recourse","author":"Ross, Alexis and Lakkaraju, Himabindu and Bastani, Osbert","meta_info":{"year":"2021","pages":"18734--18746","volume":"34","journal":"Advances in Neural Information Processing Systems","url":"https:\/\/proceedings.neurips.cc\/paper\/2021\/file\/9b82909c30456ac902e14526e63081d4-Paper.pdf"}}
{"bib_id":"zhang2023towards","title":"Towards Model Robustness: Generating Contextual Counterfactuals for Entities in Relation Extraction","author":"Zhang, Mi and Qian, Tieyun and Zhang, Ting and Miao, Xin","meta_info":{"year":"2023","pages":"1832--1842","booktitle":"Proceedings of the ACM Web Conference 2023","url":"https:\/\/doi.org\/10.1145\/3543507.3583504"}}
{"bib_id":"geva2022break","title":"Break, perturb, build: Automatic perturbation of reasoning paths through question decomposition","author":"Geva, Mor and Wolfson, Tomer and Berant, Jonathan","meta_info":{"publisher":"MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…","year":"2022","pages":"111--126","volume":"10","journal":"Transactions of the Association for Computational Linguistics","url":"https:\/\/doi.org\/10.1162\/tacl_a_00450"}}
{"bib_id":"fu-etal-2023-scene","title":"SCENE: Self-Labeled Counterfactuals for Extrapolating to Negative Examples","author":"Fu, Deqing  and\nGodbole, Ameya  and\nJia, Robin","meta_info":{"abstract":"Detecting negatives (such as non-entailment relationships, unanswerable questions, and false claims) is an important and challenging aspect of many natural language understanding tasks. Though manually collecting challenging negative examples can help models detect them, it is both costly and domain-specific. In this work, we propose Self-labeled Counterfactuals for Extrapolating to Negative Examples (SCENE), an automatic method for synthesizing training data that greatly improves models′ ability to detect challenging negative examples. In contrast with standard data augmentation, which synthesizes new examples for existing labels, SCENE can synthesize negative examples zero-shot from only positive ones. Given a positive example, SCENE perturbs it with a mask infilling model, then determines whether the resulting example is negative based on a self-training heuristic. With access to only answerable training examples, SCENE can close 69.6% of the performance gap on SQuAD 2.0, a dataset where half of the evaluation examples are unanswerable, compared to a model trained on SQuAD 2.0. Our method also extends to boolean question answering and recognizing textual entailment, and improves generalization from SQuAD to ACE-whQA, an out-of-domain extractive QA benchmark.","pages":"7832--7848","doi":"10.18653\/v1\/2023.emnlp-main.485","url":"https:\/\/aclanthology.org\/2023.emnlp-main.485","publisher":"Association for Computational Linguistics","address":"Singapore","year":"2023","month":"December","booktitle":"Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing","editor":"Bouamor, Houda  and\nPino, Juan  and\nBali, Kalika"}}
{"bib_id":"miao2023generating","title":"Generating Commonsense Counterfactuals for Stable Relation Extraction","author":"Miao, Xin and Li, Yongqi and Qian, Tieyun","meta_info":{"year":"2023","booktitle":"The 2023 Conference on Empirical Methods in Natural Language Processing","url":"https:\/\/aclanthology.org\/2023.emnlp-main.344"}}
{"bib_id":"hong-etal-2023-robust","title":"A Robust Information-Masking Approach for Domain Counterfactual Generation","author":"Hong, Pengfei  and\nBhardwaj, Rishabh  and\nMajumder, Navonil  and\nAditya, Somak  and\nPoria, Soujanya","meta_info":{"pages":"3756--3769","doi":"10.18653\/v1\/2023.findings-acl.231","url":"https:\/\/aclanthology.org\/2023.findings-acl.231","publisher":"Association for Computational Linguistics","address":"Toronto, Canada","year":"2023","month":"July","booktitle":"Findings of the Association for Computational Linguistics: ACL 2023","editor":"Rogers, Anna  and\nBoyd-Graber, Jordan  and\nOkazaki, Naoaki"}}
{"bib_id":"calderon-etal-2022-docogen","title":"DoCoGen: Domain Counterfactual Generation for Low Resource Domain Adaptation","author":"Calderon, Nitay  and\nBen-David, Eyal  and\nFeder, Amir  and\nReichart, Roi","meta_info":{"pages":"7727--7746","doi":"10.18653\/v1\/2022.acl-long.533","url":"https:\/\/aclanthology.org\/2022.acl-long.533","publisher":"Association for Computational Linguistics","address":"Dublin, Ireland","year":"2022","month":"May","booktitle":"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","editor":"Muresan, Smaranda  and\nNakov, Preslav  and\nVillavicencio, Aline"}}
{"bib_id":"li2024prompting","title":"Prompting Large Language Models for Counterfactual Generation: An Empirical Study","author":"Li, Yongqi  and\nXu, Mayi  and\nMiao, Xin  and\nZhou, Shen  and\nQian, Tieyun","meta_info":{"abstract":"Large language models (LLMs) have made remarkable progress in a wide range of natural language understanding and generation tasks. However, their ability to generate counterfactuals has not been examined systematically. To bridge this gap, we present a comprehensive evaluation framework on various types of NLU tasks, which covers all key factors in determining LLMs′ capability of generating counterfactuals. Based on this framework, we 1) investigate the strengths and weaknesses of LLMs as the counterfactual generator, and 2) disclose the factors that affect LLMs when generating counterfactuals, including both the intrinsic properties of LLMs and prompt designing. The results show that, though LLMs are promising in most cases, they face challenges in complex tasks like RE since they are bounded by task-specific performance, entity constraints, and inherent selection bias. We also find that alignment techniques, e.g., instruction-tuning and reinforcement learning from human feedback, may potentially enhance the counterfactual generation ability of LLMs. On the contrary, simply increasing the parameter size does not yield the desired improvements. Besides, from the perspective of prompt designing, task guidelines unsurprisingly play an important role. However, the chain-of-thought approach does not always help due to inconsistency issues.","pages":"13201--13221","url":"https:\/\/aclanthology.org\/2024.lrec-main.1156","publisher":"ELRA and ICCL","address":"Torino, Italy","year":"2024","month":"May","booktitle":"Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)","editor":"Calzolari, Nicoletta  and\nKan, Min-Yen  and\nHoste, Veronique  and\nLenci, Alessandro  and\nSakti, Sakriani  and\nXue, Nianwen"}}
{"bib_id":"madaan2023counterfactual","title":"Counterfactual Sentence Generation with Plug-and-Play Perturbation","author":"Madaan, Nishtha and Saha, Diptikalyan and Bedathur, Srikanta","meta_info":{"organization":"IEEE","year":"2023","pages":"306--315","booktitle":"2023 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)","url":"https:\/\/ieeexplore.ieee.org\/document\/10516631"}}
{"bib_id":"hao2021sketch","title":"Sketch and customize: A counterfactual story generator","author":"Hao, Changying and Pang, Liang and Lan, Yanyan and Wang, Yan and Guo, Jiafeng and Cheng, Xueqi","meta_info":{"year":"2021","pages":"12955--12962","volume":"35(14)","booktitle":"Proceedings of the AAAI Conference on Artificial Intelligence","url":"https:\/\/ojs.aaai.org\/index.php\/AAAI\/article\/view\/17532"}}
{"bib_id":"chen2022unsupervised","title":"Unsupervised editing for counterfactual stories","author":"Chen, Jiangjie and Gan, Chun and Cheng, Sijie and Zhou, Hao and Xiao, Yanghua and Li, Lei","meta_info":{"year":"2022","pages":"10473--10481","volume":"36(10)","booktitle":"Proceedings of the AAAI Conference on Artificial Intelligence","url":"https:\/\/ojs.aaai.org\/index.php\/AAAI\/article\/view\/21290"}}
{"bib_id":"sen-etal-2021-counterfactually","title":"How Does Counterfactually Augmented Data Impact Models for Social Computing Constructs?","author":"Sen, Indira  and\nSamory, Mattia  and\nFlöck, Fabian  and\nWagner, Claudia  and\nAugenstein, Isabelle","meta_info":{"abstract":"As NLP models are increasingly deployed in socially situated settings such as online abusive content detection, it is crucial to ensure that these models are robust. One way of improving model robustness is to generate counterfactually augmented data (CAD) for training models that can better learn to distinguish between core features and data artifacts. While models trained on this type of data have shown promising out-of-domain generalizability, it is still unclear what the sources of such improvements are. We investigate the benefits of CAD for social NLP models by focusing on three social computing constructs --- sentiment, sexism, and hate speech. Assessing the performance of models trained with and without CAD across different types of datasets, we find that while models trained on CAD show lower in-domain performance, they generalize better out-of-domain. We unpack this apparent discrepancy using machine explanations and find that CAD reduces model reliance on spurious features. Leveraging a novel typology of CAD to analyze their relationship with model performance, we find that CAD which acts on the construct directly or a diverse set of CAD leads to higher performance.","pages":"325--344","doi":"10.18653\/v1\/2021.emnlp-main.28","url":"https:\/\/aclanthology.org\/2021.emnlp-main.28","publisher":"Association for Computational Linguistics","address":"Online and Punta Cana, Dominican Republic","year":"2021","month":"November","booktitle":"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing","editor":"Moens, Marie-Francine  and\nHuang, Xuanjing  and\nSpecia, Lucia  and\nYih, Scott Wen-tau"}}
{"bib_id":"li2023click","title":"CLICK: Integrating Causal Inference and Commonsense Knowledge Incorporation for Counterfactual Story Generation","author":"Li, Dandan and Guo, Ziyu and Liu, Qing and Jin, Li and Zhang, Zequn and Wei, Kaiwen and Li, Feng","meta_info":{"year":"2023","number":"19","volume":"12","journal":"Electronics (2079-9292)","url":"https:\/\/www.mdpi.com\/2079-9292\/12\/19\/4173"}}
{"bib_id":"nguyen2024llms","title":"LLMs for Generating and Evaluating Counterfactuals: A Comprehensive Study","author":"Nguyen, Van Bach and Youssef, Paul and Schlötterer, Jörg and Seifert, Christin","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2405.00722","url":"https:\/\/arxiv.org\/pdf\/2405.00722"}}
{"bib_id":"chang2024counterfactual","title":"Counterfactual-Enhanced Information Bottleneck for Aspect-Based Sentiment Analysis","author":"Chang, Mingshan and Yang, Min and Jiang, Qingshan and Xu, Ruifeng","meta_info":{"year":"2024","pages":"17736--17744","volume":"38(16)","booktitle":"Proceedings of the AAAI Conference on Artificial Intelligence","url":"https:\/\/ojs.aaai.org\/index.php\/AAAI\/article\/view\/29726"}}
{"bib_id":"asai-hajishirzi-2020-logic","title":"Logic-Guided Data Augmentation and Regularization for Consistent Question Answering","author":"Asai, Akari  and\nHajishirzi, Hannaneh","meta_info":{"pages":"5642--5650","doi":"10.18653\/v1\/2020.acl-main.499","url":"https:\/\/aclanthology.org\/2020.acl-main.499","year":"2020","month":"July","booktitle":"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics","editor":"Jurafsky, Dan  and\nChai, Joyce  and\nSchluter, Natalie  and\nTetreault, Joel"}}
{"bib_id":"chemmengath-etal-2022-cat","title":"Let the CAT out of the bag: Contrastive Attributed explanations for Text","author":"Chemmengath, Saneem  and\nAzad, Amar Prakash  and\nLuss, Ronny  and\nDhurandhar, Amit","meta_info":{"pages":"7190--7206","doi":"10.18653\/v1\/2022.emnlp-main.484","url":"https:\/\/aclanthology.org\/2022.emnlp-main.484","publisher":"Association for Computational Linguistics","address":"Abu Dhabi, United Arab Emirates","year":"2022","month":"December","booktitle":"Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing","editor":"Goldberg, Yoav  and\nKozareva, Zornitsa  and\nZhang, Yue"}}
{"bib_id":"joshi-he-2022-investigation","title":"An Investigation of the (In)effectiveness of Counterfactually Augmented Data","author":"Joshi, Nitish  and\nHe, He","meta_info":{"pages":"3668--3681","doi":"10.18653\/v1\/2022.acl-long.256","url":"https:\/\/aclanthology.org\/2022.acl-long.256","publisher":"Association for Computational Linguistics","address":"Dublin, Ireland","year":"2022","month":"May","booktitle":"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","editor":"Muresan, Smaranda  and\nNakov, Preslav  and\nVillavicencio, Aline"}}
{"bib_id":"fern-pope-2021-text","title":"Text Counterfactuals via Latent Optimization and Shapley-Guided Search","author":"Fern, Xiaoli  and\nPope, Quintin","meta_info":{"pages":"5578--5593","doi":"10.18653\/v1\/2021.emnlp-main.452","url":"https:\/\/aclanthology.org\/2021.emnlp-main.452","publisher":"Association for Computational Linguistics","address":"Online and Punta Cana, Dominican Republic","year":"2021","month":"November","booktitle":"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing","editor":"Moens, Marie-Francine  and\nHuang, Xuanjing  and\nSpecia, Lucia  and\nYih, Scott Wen-tau"}}
{"bib_id":"10.1145\/3527848","title":"A Survey of Algorithmic Recourse: Contrastive Explanations and Consequential Recommendations","author":"Karimi, Amir-Hossein and Barthe, Gilles and Schölkopf, Bernhard and Valera, Isabel","meta_info":{"keywords":"contrastive explanations and consequential recommendations, Algorithmic recourse","numpages":"29","articleno":"95","month":"dec","journal":"ACM Comput. Surv.","doi":"10.1145\/3527848","url":"https:\/\/doi.org\/10.1145\/3527848","issn":"0360-0300","number":"5","volume":"55","address":"New York, NY, USA","publisher":"Association for Computing Machinery","issue_date":"May 2023","year":"2022"}}
{"bib_id":"chen-etal-2021-reinforced","title":"Reinforced Counterfactual Data Augmentation for Dual Sentiment Classification","author":"Chen, Hao  and\nXia, Rui  and\nYu, Jianfei","meta_info":{"pages":"269--278","doi":"10.18653\/v1\/2021.emnlp-main.24","url":"https:\/\/aclanthology.org\/2021.emnlp-main.24","publisher":"Association for Computational Linguistics","address":"Online and Punta Cana, Dominican Republic","year":"2021","month":"November","booktitle":"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing","editor":"Moens, Marie-Francine  and\nHuang, Xuanjing  and\nSpecia, Lucia  and\nYih, Scott Wen-tau"}}
{"bib_id":"radford2019language","title":"Language models are unsupervised multitask learners","author":"Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others","meta_info":{"year":"2019","pages":"9","number":"8","volume":"1","journal":"OpenAI blog","url":"https:\/\/d4mucfpksywv.cloudfront.net\/better-language-models\/language_models_are_unsupervised_multitask_learners.pdf"}}
{"bib_id":"geirhos2020shortcut","title":"Shortcut learning in deep neural networks","author":"Geirhos, Robert and Jacobsen, Jörn-Henrik and Michaelis, Claudio and Zemel, Richard and Brendel, Wieland and Bethge, Matthias and Wichmann, Felix A","meta_info":{"publisher":"Nature Publishing Group UK London","year":"2020","pages":"665--673","number":"11","volume":"2","journal":"Nature Machine Intelligence","url":"https:\/\/doi.org\/10.1038\/s42256-020-00257-z"}}
{"bib_id":"hermann2020shapes","title":"What shapes feature representations? exploring datasets, architectures, and training","author":"Hermann, Katherine and Lampinen, Andrew","meta_info":{"year":"2020","pages":"9995--10006","volume":"33","journal":"Advances in Neural Information Processing Systems","url":"https:\/\/papers.nips.cc\/paper\/2020\/file\/71e9c6620d381d60196ebe694840aaaa-Paper.pdf"}}
{"bib_id":"zhang-etal-2019-paws","title":"PAWS: Paraphrase Adversaries from Word Scrambling","author":"Zhang, Yuan  and\nBaldridge, Jason  and\nHe, Luheng","meta_info":{"abstract":"Existing paraphrase identification datasets lack sentence pairs that have high lexical overlap without being paraphrases. Models trained on such data fail to distinguish pairs like flights from New York to Florida and flights from Florida to New York. This paper introduces PAWS (Paraphrase Adversaries from Word Scrambling), a new dataset with 108,463 well-formed paraphrase and non-paraphrase pairs with high lexical overlap. Challenging pairs are generated by controlled word swapping and back translation, followed by fluency and paraphrase judgments by human raters. State-of-the-art models trained on existing datasets have dismal performance on PAWS (\\textless40% accuracy); however, including PAWS training data for these models improves their accuracy to 85% while maintaining performance on existing tasks. In contrast, models that do not capture non-local contextual information fail even with PAWS training examples. As such, PAWS provides an effective instrument for driving further progress on models that better exploit structure, context, and pairwise comparisons.","pages":"1298--1308","doi":"10.18653\/v1\/N19-1131","url":"https:\/\/aclanthology.org\/N19-1131","publisher":"Association for Computational Linguistics","address":"Minneapolis, Minnesota","year":"2019","month":"June","booktitle":"Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)","editor":"Burstein, Jill  and\nDoran, Christy  and\nSolorio, Thamar"}}
{"bib_id":"brown2020language","title":"Language models are few-shot learners","author":"Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others","meta_info":{"year":"2020","pages":"1877--1901","volume":"33","journal":"Advances in neural information processing systems","url":"https:\/\/proceedings.neurips.cc\/paper_files\/paper\/2020\/file\/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf"}}
{"bib_id":"miller2019explanation","title":"Explanation in artificial intelligence: Insights from the social sciences","author":"Miller, Tim","meta_info":{"publisher":"Elsevier","year":"2019","pages":"1--38","volume":"267","url":"https:\/\/www.sciencedirect.com\/science\/article\/pii\/S0004370218305988","journal":"Artificial intelligence"}}
{"bib_id":"achiam2023gpt","title":"Gpt-4 technical report","author":"Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2303.08774","url":"https:\/\/arxiv.org\/pdf\/2303.08774"}}
{"bib_id":"wang2023counterfactual","title":"Counterfactual explanations for machine learning models on heterogeneous data","author":"Wang, Yongjie","meta_info":{"publisher":"Nanyang Technological University","year":"2023","url":"https:\/\/dr.ntu.edu.sg\/handle\/10356\/169968"}}
{"bib_id":"sudhakar2019transforming","title":"“Transforming” Delete, Retrieve, Generate Approach for Controlled Text Style Transfer","author":"Sudhakar, Akhilesh and Upadhyay, Bhargav and Maheswaran, Arjun","meta_info":{"year":"2019","pages":"3269--3279","booktitle":"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)","url":"https:\/\/aclanthology.org\/D19-1322"}}
{"bib_id":"xu2020variational","title":"On variational learning of controllable representations for text without supervision","author":"Xu, Peng and Cheung, Jackie Chi Kit and Cao, Yanshuai","meta_info":{"organization":"PMLR","year":"2020","pages":"10534--10543","booktitle":"International Conference on Machine Learning","url":"https:\/\/dl.acm.org\/doi\/pdf\/10.5555\/3524938.3525914"}}
{"bib_id":"wei2022chain","title":"Chain-of-thought prompting elicits reasoning in large language models","author":"Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others","meta_info":{"year":"2022","pages":"24824--24837","volume":"35","journal":"Advances in neural information processing systems","url":"https:\/\/proceedings.neurips.cc\/paper_files\/paper\/2022\/file\/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf"}}
{"bib_id":"chowdhery2023palm","title":"Palm: Scaling language modeling with pathways","author":"Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others","meta_info":{"year":"2023","pages":"1--113","number":"240","volume":"24","journal":"Journal of Machine Learning Research","url":"https:\/\/dl.acm.org\/doi\/pdf\/10.5555\/3648699.3648939"}}
{"bib_id":"Dathathri2020Plug","title":"Plug and Play Language Models: A Simple Approach to Controlled Text Generation","author":"Sumanth Dathathri and Andrea Madotto and Janice Lan and Jane Hung and Eric Frank and Piero Molino and Jason Yosinski and Rosanne Liu","meta_info":{"pages":"1-34","url":"https:\/\/openreview.net\/forum?id=H1edEyBKDS","year":"2020","booktitle":"International Conference on Learning Representations"}}
{"bib_id":"devlin2019bert","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","author":"Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina","meta_info":{"year":"2019","pages":"4171--4186","booktitle":"Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)","url":"https:\/\/aclanthology.org\/N19-1423"}}
{"bib_id":"8579014","title":"StarGAN: Unified Generative Adversarial Networks for Multi-domain Image-to-Image Translation","author":"Choi, Yunjey and Choi, Minje and Kim, Munyoung and Ha, Jung-Woo and Kim, Sunghun and Choo, Jaegul","meta_info":{"doi":"10.1109\/CVPR.2018.00916","keywords":"Generators;Task analysis;Training;Image reconstruction;Generative adversarial networks;Hair;Gallium nitride","pages":"8789-8797","number":"","volume":"","year":"2018","url":"https:\/\/doi.ieeecomputersociety.org\/10.1109\/CVPR.2018.00916","booktitle":"2018 IEEE\/CVF Conference on Computer Vision and Pattern Recognition"}}
{"bib_id":"li2020bert","title":"BERT-ATTACK: Adversarial Attack Against BERT Using BERT","author":"Li, Linyang and Ma, Ruotian and Guo, Qipeng and Xue, Xiangyang and Qiu, Xipeng","meta_info":{"year":"2020","pages":"6193--6202","booktitle":"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)","url":"https:\/\/aclanthology.org\/2020.emnlp-main.500"}}
{"bib_id":"garg2020bae","title":"BAE: BERT-based Adversarial Examples for Text Classification","author":"Garg, Siddhant and Ramakrishnan, Goutham","meta_info":{"year":"2020","pages":"6174--6181","booktitle":"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)","url":"https:\/\/aclanthology.org\/2020.emnlp-main.498"}}
{"bib_id":"sudhakar-etal-2019-transforming","title":"``Transforming″ Delete, Retrieve, Generate Approach for Controlled Text Style Transfer","author":"Sudhakar, Akhilesh  and\nUpadhyay, Bhargav  and\nMaheswaran, Arjun","meta_info":{"pages":"3269--3279","doi":"10.18653\/v1\/D19-1322","url":"https:\/\/aclanthology.org\/D19-1322","publisher":"Association for Computational Linguistics","address":"Hong Kong, China","year":"2019","month":"November","booktitle":"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)","editor":"Inui, Kentaro  and\nJiang, Jing  and\nNg, Vincent  and\nWan, Xiaojun"}}
{"bib_id":"hu2017toward","title":"Toward controlled generation of text","author":"Hu, Zhiting and Yang, Zichao and Liang, Xiaodan and Salakhutdinov, Ruslan and Xing, Eric P","meta_info":{"organization":"PMLR","year":"2017","pages":"1587--1596","url":"https:\/\/proceedings.mlr.press\/v70\/hu17e.html","booktitle":"International conference on machine learning"}}
{"bib_id":"ribeiro2016should","title":"``Why Should I Trust You?″: Explaining the Predictions of Any Classifier","author":"Ribeiro, Marco  and\nSingh, Sameer  and\nGuestrin, Carlos","meta_info":{"pages":"97--101","doi":"10.18653\/v1\/N16-3020","url":"https:\/\/aclanthology.org\/N16-3020","publisher":"Association for Computational Linguistics","address":"San Diego, California","year":"2016","month":"June","booktitle":"Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations","editor":"DeNero, John  and\nFinlayson, Mark  and\nReddy, Sravana"}}
{"bib_id":"lundberg2017unified","title":"A unified approach to interpreting model predictions","author":"Lundberg, Scott M and Lee, Su-In","meta_info":{"year":"2017","volume":"30","journal":"Advances in neural information processing systems","url":"https:\/\/dl.acm.org\/doi\/pdf\/10.5555\/3295222.3295230"}}
{"bib_id":"si2023consistent","title":"Consistent multi-granular rationale extraction for explainable multi-hop fact verification","author":"Si, Jiasheng and Zhu, Yingjie and Zhou, Deyu","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2305.09400","url":"https:\/\/arxiv.org\/pdf\/2305.09400"}}
{"bib_id":"salazar2020masked","title":"Masked Language Model Scoring","author":"Salazar, Julian and Liang, Davis and Nguyen, Toan Q and Kirchhoff, Katrin","meta_info":{"year":"2020","pages":"2699--2712","url":"https:\/\/aclanthology.org\/2020.acl-main.240","booktitle":"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"}}
{"bib_id":"10.1145\/3531146.3533188","title":"DualCF: Efficient Model Extraction Attack from Counterfactual Explanations","author":"Wang, Yongjie and Qian, Hangwei and Miao, Chunyan","meta_info":{"series":"FAccT '22","location":"<conf-loc>, <city>Seoul<\/city>, <country>Republic of Korea<\/country>, <\/conf-loc>","keywords":"Counterfactual Explanations, Decision Boundary Shift, Model Extraction Attack, Model Security and Privacy","numpages":"12","pages":"1318–1329","booktitle":"Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency","abstract":"Cloud service providers have launched Machine-Learning-as-a-Service (MLaaS) platforms to allow users to access large-scale cloud-based models via APIs. In addition to prediction outputs, these APIs can also provide other information in a more human-understandable way, such as counterfactual explanations (CF). However, such extra information inevitably causes the cloud models to be more vulnerable to extraction attacks which aim to steal the internal functionality of models in the cloud. Due to the black-box nature of cloud models, however, a vast number of queries are inevitably required by existing attack strategies before the substitute model achieves high fidelity. In this paper, we propose a novel simple yet efficient querying strategy to greatly enhance the querying efficiency to steal a classification model. This is motivated by our observation that current querying strategies suffer from decision boundary shift issue induced by taking far-distant queries and close-to-boundary CFs into substitute model training. We then propose DualCF strategy to circumvent the above issues, which is achieved by taking not only CF but also counterfactual explanation of CF (CCF) as pairs of training samples for the substitute model. Extensive and comprehensive experimental evaluations are conducted on both synthetic and real-world datasets. The experimental results favorably illustrate that DualCF can produce a high-fidelity model with fewer queries efficiently and effectively.","doi":"10.1145\/3531146.3533188","url":"https:\/\/doi.org\/10.1145\/3531146.3533188","address":"New York, NY, USA","publisher":"Association for Computing Machinery","isbn":"9781450393522","year":"2022"}}
{"bib_id":"aivodji2020model","title":"Model extraction from counterfactual explanations","author":"Aı̈vodji, Ulrich and Bolot, Alexandre and Gambs, Sébastien","meta_info":{"year":"2020","journal":"arXiv preprint arXiv:2009.01884","url":"https:\/\/arxiv.org\/abs\/2009.01884"}}
{"bib_id":"10.1145\/3351095.3372850","title":"Explaining machine learning classifiers through diverse counterfactual explanations","author":"Mothilal, Ramaravind K. and Sharma, Amit and Tan, Chenhao","meta_info":{"series":"FAT* '20","location":"Barcelona, Spain","numpages":"11","pages":"607–617","booktitle":"Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency","abstract":"Post-hoc explanations of machine learning models are crucial for people to understand and act on algorithmic predictions. An intriguing class of explanations is through counterfactuals, hypothetical examples that show people how to obtain a different prediction. We posit that effective counterfactual explanations should satisfy two properties: feasibility of the counterfactual actions given user context and constraints, and diversity among the counterfactuals presented. To this end, we propose a framework for generating and evaluating a diverse set of counterfactual explanations based on determinantal point processes. To evaluate the actionability of counterfactuals, we provide metrics that enable comparison of counterfactual-based methods to other local explanation methods. We further address necessary tradeoffs and point to causal implications in optimizing for counterfactuals. Our experiments on four real-world datasets show that our framework can generate a set of counterfactuals that are diverse and well approximate local decision boundaries, outperforming prior approaches to generating diverse counterfactuals. We provide an implementation of the framework at https:\/\/github.com\/microsoft\/DiCE.","doi":"10.1145\/3351095.3372850","url":"https:\/\/doi.org\/10.1145\/3351095.3372850","address":"New York, NY, USA","publisher":"Association for Computing Machinery","isbn":"9781450369367","year":"2020"}}
{"bib_id":"pmlr-v70-sundararajan17a","title":"Axiomatic Attribution for Deep Networks","author":"Mukund Sundararajan and Ankur Taly and Qiqi Yan","meta_info":{"abstract":"We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms—Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.","url":"https:\/\/proceedings.mlr.press\/v70\/sundararajan17a.html","pdf":"http:\/\/proceedings.mlr.press\/v70\/sundararajan17a\/sundararajan17a.pdf","publisher":"PMLR","month":"06--11 Aug","series":"Proceedings of Machine Learning Research","volume":"70","editor":"Precup, Doina and Teh, Yee Whye","year":"2017","pages":"3319--3328","booktitle":"Proceedings of the 34th International Conference on Machine Learning"}}
{"bib_id":"DBLP:journals\/corr\/SimonyanVZ13","title":"Deep Inside Convolutional Networks: Visualising Image Classification\nModels and Saliency Maps","author":"Karen Simonyan and\nAndrea Vedaldi and\nAndrew Zisserman","meta_info":{"pages":"1-8","bibsource":"dblp computer science bibliography, https:\/\/dblp.org","biburl":"https:\/\/dblp.org\/rec\/journals\/corr\/SimonyanVZ13.bib","timestamp":"Thu, 25 Jul 2019 14:36:46 +0200","url":"http:\/\/arxiv.org\/abs\/1312.6034","year":"2014","booktitle":"2nd International Conference on Learning Representations, ICLR 2014,\nBanff, AB, Canada, April 14-16, 2014, Workshop Track Proceedings","editor":"Yoshua Bengio and\nYann LeCun"}}
{"bib_id":"wang-cho-2019-bert","title":"BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language Model","author":"Wang, Alex  and\nCho, Kyunghyun","meta_info":{"abstract":"We show that BERT (Devlin et al., 2018) is a Markov random field language model. This formulation gives way to a natural procedure to sample sentences from BERT. We generate from BERT and find that it can produce high quality, fluent generations. Compared to the generations of a traditional left-to-right language model, BERT generates sentences that are more diverse but of slightly worse quality.","pages":"30--36","doi":"10.18653\/v1\/W19-2304","url":"https:\/\/aclanthology.org\/W19-2304","publisher":"Association for Computational Linguistics","address":"Minneapolis, Minnesota","year":"2019","month":"June","booktitle":"Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation","editor":"Bosselut, Antoine  and\nCelikyilmaz, Asli  and\nGhazvininejad, Marjan  and\nIyer, Srinivasan  and\nKhandelwal, Urvashi  and\nRashkin, Hannah  and\nWolf, Thomas"}}
{"bib_id":"sathe-etal-2020-automated","title":"Automated Fact-Checking of Claims from Wikipedia","author":"Sathe, Aalok  and\nAther, Salar  and\nLe, Tuan Manh  and\nPerry, Nathan  and\nPark, Joonsuk","meta_info":{"isbn":"979-10-95546-34-4","language":"English","pages":"6874--6882","url":"https:\/\/aclanthology.org\/2020.lrec-1.849","publisher":"European Language Resources Association","address":"Marseille, France","year":"2020","month":"May","booktitle":"Proceedings of the Twelfth Language Resources and Evaluation Conference","editor":"Calzolari, Nicoletta  and\nBéchet, Frédéric  and\nBlache, Philippe  and\nChoukri, Khalid  and\nCieri, Christopher  and\nDeclerck, Thierry  and\nGoggi, Sara  and\nIsahara, Hitoshi  and\nMaegaard, Bente  and\nMariani, Joseph  and\nMazo, Hélène  and\nMoreno, Asuncion  and\nOdijk, Jan  and\nPiperidis, Stelios"}}
{"bib_id":"qiu2024paircfr","title":"PairCFR: Enhancing Model Training on Paired Counterfactually Augmented Data through Contrastive Learning","author":"Xiaoqi Qiu and Yongjie Wang and Xu Guo and Zhiwei Zeng and Yue Yu and Yuhong Feng and Chunyan Miao","meta_info":{"journal":"arXiv preprint arXiv:2406.01382","url":"https:\/\/arxiv.org\/pdf\/2406.06633v1","year":"2024"}}
{"bib_id":"10.1121\/1.2016299","title":"Perplexity—a measure of the difficulty of speech recognition tasks","author":"Jelinek, F. and Mercer, R. L. and Bahl, L. R. and Baker, J. K.","meta_info":{"eprint":"https:\/\/pubs.aip.org\/asa\/jasa\/article-pdf\/62\/S1\/S63\/11558910\/s63_5_online.pdf","url":"https:\/\/doi.org\/10.1121\/1.2016299","doi":"10.1121\/1.2016299","issn":"0001-4966","abstract":"Using counterexamples, we show that vocabulary size and static and dynamic branching factors are all inadequate as measures of speech recognition complexity of finite state grammars. Information theoretic arguments show that perplexity (the logarithm of which is the familiar entropy) is a more appropriate measure of equivalent choice. It too has certain weaknesses which we discuss. We show that perplexity can also be applied to languages having no obvious statistical description, since an entropy‐maximizing probability assignment can be found for any finite‐state grammar. Table I shows perplexity values for some well‐known speech recognition tasks. Perplexity Vocabulary Dynamic Phone Word size branching factorIBM‐Lasers 2.14 21.11 1000 1000IBM‐Raleigh 1.69 7.74 250 7.32CMU‐AIX05 1.52 6.41 1011 35","month":"08","year":"2005","pages":"S63-S63","number":"S1","volume":"62","journal":"The Journal of the Acoustical Society of America"}}
{"bib_id":"zhou2023explore","title":"Explore Spurious Correlations at the Concept Level in Language Models for Text Classification","author":"Zhou, Yuhang and Xu, Paiheng and Liu, Xiaoyu and An, Bang and Ai, Wei and Huang, Furong","meta_info":{"year":"2023","page":"1-14","url":"https:\/\/arxiv.org\/pdf\/2311.08648","journal":"arXiv preprint arXiv:2311.08648"}}
{"bib_id":"NIPS2017_a486cd07","title":"Counterfactual Fairness","author":"Kusner, Matt J and Loftus, Joshua and Russell, Chris and Silva, Ricardo","meta_info":{"year":"2017","volume":"30","url":"https:\/\/proceedings.neurips.cc\/paper_files\/paper\/2017\/file\/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf","publisher":"Curran Associates, Inc.","pages":"1-11","editor":"I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett","booktitle":"Advances in Neural Information Processing Systems"}}
{"bib_id":"NIPS2017_1271a702","title":"When Worlds Collide: Integrating Different Counterfactual Assumptions in Fairness","author":"Russell, Chris and Kusner, Matt J and Loftus, Joshua and Silva, Ricardo","meta_info":{"year":"2017","volume":"30","url":"https:\/\/proceedings.neurips.cc\/paper_files\/paper\/2017\/file\/1271a7029c9df08643b631b02cf9e116-Paper.pdf","publisher":"Curran Associates, Inc.","pages":"","editor":"I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett","booktitle":"Advances in Neural Information Processing Systems"}}
{"bib_id":"ji-etal-2023-towards","title":"Towards Mitigating LLM Hallucination via Self Reflection","author":"Ji, Ziwei  and\nYu, Tiezheng  and\nXu, Yan  and\nLee, Nayeon  and\nIshii, Etsuko  and\nFung, Pascale","meta_info":{"pages":"1827--1843","doi":"10.18653\/v1\/2023.findings-emnlp.123","url":"https:\/\/aclanthology.org\/2023.findings-emnlp.123","publisher":"Association for Computational Linguistics","address":"Singapore","year":"2023","month":"December","booktitle":"Findings of the Association for Computational Linguistics: EMNLP 2023","editor":"Bouamor, Houda  and\nPino, Juan  and\nBali, Kalika"}}
{"bib_id":"ye-etal-2022-zerogen","title":"ZeroGen: Efficient Zero-shot Learning via Dataset Generation","author":"Ye, Jiacheng  and\nGao, Jiahui  and\nLi, Qintong  and\nXu, Hang  and\nFeng, Jiangtao  and\nWu, Zhiyong  and\nYu, Tao  and\nKong, Lingpeng","meta_info":{"pages":"11653--11669","doi":"10.18653\/v1\/2022.emnlp-main.801","url":"https:\/\/aclanthology.org\/2022.emnlp-main.801","publisher":"Association for Computational Linguistics","address":"Abu Dhabi, United Arab Emirates","year":"2022","month":"December","booktitle":"Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing","editor":"Goldberg, Yoav  and\nKozareva, Zornitsa  and\nZhang, Yue"}}
{"bib_id":"ijcai2024p721","title":"Beyond What If: Advancing Counterfactual Text Generation with Structural Causal Modeling","author":"Wang, Ziao and Zhang, Xiaofeng and Du, Hongwei","meta_info":{"url":"https:\/\/doi.org\/10.24963\/ijcai.2024\/721","doi":"10.24963\/ijcai.2024\/721","note":"Main Track","month":"8","year":"2024","pages":"6522--6530","editor":"Kate Larson","publisher":"International Joint Conferences on Artificial Intelligence Organization","booktitle":"Proceedings of the Thirty-Third International Joint Conference on\nArtificial Intelligence, IJCAI-24"}}
{"bib_id":"miao-etal-2024-episodic","title":"Episodic Memory Retrieval from LLMs: A Neuromorphic Mechanism to Generate Commonsense Counterfactuals for Relation Extraction","author":"Miao, Xin  and\nLi, Yongqi  and\nZhou, Shen  and\nQian, Tieyun","meta_info":{"abstract":"Large language models (LLMs) have achieved satisfactory performance in counterfactual generation. However, confined by the stochastic generation process of LLMs, there often are misalignments between LLMs and humans which hinder LLMs from handling complex tasks like relation extraction. As a result, LLMs may generate commonsense-violated counterfactuals like `eggs were produced by a box′. To bridge this gap, we propose to mimick the episodic memory retrieval, the working mechanism of human hippocampus, to align LLMs′ generation process with that of humans. In this way, LLMs can derive experience from their extensive memory, which keeps in line with the way humans gain commonsense. We then implement two central functions in the hippocampus, i.e., pattern separation and pattern completion, to retrieve the episodic memory from LLMs and generate commonsense counterfactuals for relation extraction. Experimental results demonstrate the improvements of our framework over existing methods in terms of the quality of counterfactuals.","pages":"2489--2511","doi":"10.18653\/v1\/2024.findings-acl.146","url":"https:\/\/aclanthology.org\/2024.findings-acl.146","publisher":"Association for Computational Linguistics","address":"Bangkok, Thailand and virtual meeting","year":"2024","month":"August","booktitle":"Findings of the Association for Computational Linguistics ACL 2024","editor":"Ku, Lun-Wei  and\nMartins, Andre  and\nSrikumar, Vivek"}}
