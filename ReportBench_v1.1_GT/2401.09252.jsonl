{"bib_id":"won:arxiv:2020","title":"OmniSLAM: Omnidirectional Localization and Dense Mapping for Wide-baseline Multi-camera Systems","author":"Won, Changhee and Seok, Hochang and Cui, Zhaopeng and Pollefeys, Marc and Lim, Jongwoo","meta_info":{"doi":"10.1109\/ICRA40945.2020.9196695","pages":"559-566","number":"","volume":"","year":"2020","booktitle":"2020 IEEE International Conference on Robotics and Automation (ICRA)"}}
{"bib_id":"zhai:pr:2021","title":"Optical flow and scene flow estimation: A survey","author":"Zhai, M. and Xiang, X. and Lv, N. and Kong, X.","meta_info":{"year":"2021","pages":"107861","volume":"114","journal":"Pattern Recognition"}}
{"bib_id":"furukawa:book:2015","title":"Multi-View Stereo: A Tutorial","author":"Yasutaka Furukawa and Carlos Hernández","meta_info":{"journal":"Foundations and Trends® in Computer Graphics and Vision","pages":"1--148","number":"1-2","volume":"9","publisher":"Now Publishers","year":"2015","doi":"10.1561\/0600000052"}}
{"bib_id":"jin:cvpr:2020","title":"Geometric structure based and regularized depth estimation from 360 indoor imagery","author":"Jin, L. and Xu, Y. and Zheng, J. and Zhang, J. and Tang, R. and Xu, S. and Yu, J. and Gao, S.","meta_info":{"year":"2020","pages":"889--898","booktitle":"IEEE Conference on Computer Vision and Pattern Recognition"}}
{"bib_id":"mallya:iccv:2015","title":"Learning informative edge maps for indoor scene layout prediction","author":"Mallya, A. and Lazebnik, S.","meta_info":{"year":"2015","pages":"936--944","booktitle":"IEEE International Conference on Computer Vision"}}
{"bib_id":"kovsecka:eccv:2002","title":"Video compass","author":"Košecká, J. and Zhang, W.","meta_info":{"year":"2002","pages":"476--490","booktitle":"European Conference on Computer Vision"}}
{"bib_id":"lee:cvpr:2019","title":"SpherePHD: Applying CNNs on a spherical polyhedron representation of 360 images","author":"Lee, Y. and Jeong, J. and Yun, J. and Cho, W. and Yoon, K.-J.","meta_info":{"year":"2019","pages":"9181--9189","booktitle":"IEEE Conference on Computer Vision and Pattern Recognition"}}
{"bib_id":"Lai:VR:2019","title":"Real-Time Panoramic Depth Maps from Omni-directional Stereo Images for 6 DoF Videos in Virtual Reality","author":"Lai, P. K. and Xie, S. and Lang, J. and Laqaruere, R.","meta_info":{"year":"2019","pages":"405--412","journal":"IEEE Conference on Virtual Reality and 3D User Interfaces","isbn":"978-1-7281-1377-7","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Lai et al. - 2019 - Real-Time Panoramic Depth Maps from Omni-directional Stereo Images for 6 DoF Videos in Virtual Reality.pdf:pdf"}}
{"bib_id":"Wegner:ICIP:2018","title":"Depth Estimation from Stereoscopic 360-Degree Video","author":"Wegner, K. and Stankiewicz, O. and Grajek, T. and Domanski, M.","meta_info":{"year":"2018","pages":"2945--2948","keywords":"360 degree 3D video,360 degree video,Circular projection,Depth estimation,Omnidirectional video,Stereoscopic omnidirectional video","journal":"IEEE International Conference on Image Processing","issn":"15224880","isbn":"9781479970612","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Wegner et al. - 2018 - Depth Estimation from Stereoscopic 360-Degree Video.pdf:pdf"}}
{"bib_id":"Pathak:ICCAS:2016","title":"3D reconstruction of structures using spherical cameras with small motion","author":"Pathak, S. and Moro, A. and Fujii, H. and Yamashita, A. and Asama, H.","meta_info":{"year":"2016","pages":"117--122","keywords":"3d reconstruction,optical flow,spherical images","isbn":"978-89-93215-11-3","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Pathak et al. - 2016 - 3D reconstruction of structures using spherical cameras with small motion.pdf:pdf","booktitle":"International Conference on Control, Automation and Systems"}}
{"bib_id":"Pathak:JCMSI:2017","title":"Optical Flow-Based Epipolar Estimation of Spherical Image Pairs for 3D Reconstruction","author":"Pathak, S. and Moro, A. and Yamashita, A. and Asama, H.","meta_info":{"year":"2017","volume":"10","pages":"476--485","number":"5","mendeley-groups":"Jung et al.\/References\/SfM and Omni-SfM","keywords":"3d reconstruction,spherical camera,stereo vision","journal":"SICE Journal of Control, Measurement, and System Integration","issn":"1882-4889","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Pathak et al. - 2017 - Optical Flow-Based Epipolar Estimation of Spherical Image Pairs for 3D Reconstruction.pdf:pdf"}}
{"bib_id":"Pathak:SII:2017","title":"Virtual reality with motion parallax by dense optical flow-based depth generation from two spherical images","author":"Pathak, S. and Moro, A. and Fujii, H. and Yamashita, A. and Asama, H.","meta_info":{"year":"2017","pages":"887--892","mendeley-groups":"Jung et al.\/References\/SfM and Omni-SfM","keywords":"Entertainment systems,Robo,Virtual reality systems","isbn":"978-1-5386-2263-6","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Pathak et al. - 2017 - Virtual Reality with Motion Parallax by Dense Optical Flow-Based Depth Generation from Two Spherical Images.pdf:pdf","booktitle":"IEEE\/SICE International Symposium on System Integration"}}
{"bib_id":"Pathak:IST:2016","title":"Dense 3D reconstruction from two spherical images via optical flow-based equirectangular epipolar rectification","author":"Pathak, S. and Moro, A. and Yamashita, A. and Asama, H.","meta_info":{"year":"2016","pages":"140--145","mendeley-groups":"Jung et al.\/References\/SfM and Omni-SfM","isbn":"978-1-5090-1817-8","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Pathak et al. - 2016 - Dense 3D reconstruction from two spherical images via optical flow-based equirectangular epipolar rectification.pdf:pdf","booktitle":"IEEE International Conference on Imaging Systems and Techniques"}}
{"bib_id":"Kim:3DV:2016","title":"Room Layout Estimation with Object and Material Attributes Information Using a Spherical Camera","author":"H. Kim and T. d. Campos and A. Hilton","meta_info":{"keywords":"cameras;convolution;estimation theory;image colour analysis;image matching;neural nets;stereo image processing;convolutional neural network;depth estimation;spherical RGB;stereo matching;global world coordinate system;spherical stereo alignment;3D room layout estimation;spherical camera;material attribute information;object attribute information;Cameras;Three-dimensional displays;Image reconstruction;Semantics;Geometry;Layout;Estimation","pages":"519-527","number":"","volume":"","year":"2016","booktitle":"Conference on 3D Vision"}}
{"bib_id":"kangni:ICCV:2007","title":"Orientation and pose recovery from spherical panoramas","author":"Kangni, F. and Laganiere, R.","meta_info":{"year":"2007","pages":"1--8","booktitle":"IEEE International Conference on Computer Vision"}}
{"bib_id":"Gastal:TOG:2011","title":"Domain Transform for Edge-aware Image and Video Processing","author":"Gastal, E. S. L. and Oliveira, M. M.","meta_info":{"keywords":"anisotropic diffusion, bilateral filter, domain transform, edge-preserving filtering","address":"New York, NY, USA","publisher":"ACM","acmid":"1964964","articleno":"69","pages":"69:1--69:12","issn":"0730-0301","year":"2011","number":"4","volume":"30","issue_date":"July 2011","journal":"ACM Trans. Graph."}}
{"bib_id":"Li:VR:2005","title":"Spherical stereo for the construction of immersive VR environment","author":" S. Li and K. Fukumori","meta_info":{"issn":"2375-5334","keywords":"virtual reality;solid modelling;computational geometry;sherical stereo;immersive virtual reality environment;point projection representation;spherical image;spherical projection;stereo algorithm;3D structure;plane images;environment structure;image pairs;spherical stereo;Virtual reality;Cameras;Robot vision systems;Image converters;Lenses;Image reconstruction;Fuses;Stereo image processing;Concrete;Chromium","pages":"217-222","number":"","volume":"","year":"2005","booktitle":"IEEE Virtual Reality"}}
{"bib_id":"Xu:ROBIO:2016","title":"Optical flow-based video completion in spherical image sequences","author":"Xu, B. and Pathak, S. and Fujii, H. and Yamashita, A. and Asama, H.","meta_info":{"year":"2016","pages":"388--395","isbn":"978-1-5090-4364-4","booktitle":"IEEE International Conference on Robotics and Biomimetics"}}
{"bib_id":"Wang:arXiv:2019","title":"360SD-Net: 360° Stereo Depth Estimation with Learnable Cost Volume","author":"Ning-Hsu Wang and Bolivar Solarte and Yi-Hsuan Tsai and Wei-Chen Chiu and Min Sun","meta_info":{"journal":"IEEE International Conference on Robotics and Automation","year":"2020","url":"https:\/\/doi.org\/10.1109\/icra40945.2020.9196975","doi":"10.1109\/icra40945.2020.9196975"}}
{"bib_id":"Kim:ICCV:2009","title":"Environment modelling using spherical stereo imaging","author":"H. Kim and A. Hilton","meta_info":{"pages":"1534-1541","number":"","volume":"","year":"2009","booktitle":"International Conference on Computer Vision Workshops"}}
{"bib_id":"Arican:AVSS:2007","title":"Dense disparity estimation from omnidirectional images","author":"Z. Arican and P. Frossard","meta_info":{"keywords":"graph theory;image representation;dense disparity estimation;omnidirectional images;spherical framework;plenoptic function;graph-cut algorithm;block matching;Layout;Cameras;Signal processing algorithms;Minimization methods;Image sensors;Standards development;Geometry;Signal processing;Belief propagation;Image converters","pages":"399-404","number":"","volume":"","year":"2007","booktitle":"IEEE Conference on Advanced Video and Signal Based Surveillance"}}
{"bib_id":"Kim:3DV:2017","title":"3D Room Geometry Reconstruction Using Audio-Visual Sensors","author":"H. Kim and L. Remaggi and P. J. Jackson and F. M. Fazi and A. Hilton","meta_info":{"issn":"2475-7888","keywords":"acoustic signal processing;architectural acoustics;cameras;computer vision;geometry;image matching;image reconstruction;image sensors;loudspeakers;microphone arrays;robot vision;stereo image processing;transient response;3D room geometry reconstruction;audio-visual sensors;cuboid-based air-tight indoor room geometry estimation method;transparent objects;reflective objects;multimodal sensory information;purely visual reconstruction;complex scenes;transparent mirror surfaces;acoustic room impulse responses;audio-visual data;complete room model;room structures;Cameras;Acoustics;Microphones;Sensors;Three-dimensional displays;Geometry;Image reconstruction;indoor-room-geometry-estimation;audio-visual-processing;3D-reconstruction","pages":"621-629","number":"","volume":"","year":"2017","booktitle":"Conference on 3D Vision"}}
{"bib_id":"Blender:misc:2020","title":"Blender - A 3D Modelling and Rendering Package","author":"Blender Online Community","meta_info":{"year":"2020","organization":"Blender Foundation"}}
{"bib_id":"Wegner:ISOIEC:2015","title":"Depth based view blending in View Synthesis Reference Software (VSRS)","author":"K. Wegner and O. Stankiewicz and M. Domański","meta_info":{"year":"2015","note":"ISO\/IEC JTC1\/SC29\/WG11 MPEG2015, M37232, Geneva, Switzerland"}}
{"bib_id":"achanta:pami:2012","title":"SLIC superpixels compared to state-of-the-art superpixel methods","author":"Achanta, R. and Shaji, A. and Smith, K. and Lucchi, A. and Fua, P. and Süsstrunk, S.","meta_info":{"year":"2012","pages":"2274--2282","number":"11","volume":"34","journal":"IEEE Transactions on Pattern Analysis and Machine Intelligence"}}
{"bib_id":"Giovanni:ECCV:2016","title":"Mobile Mapping and Visualization of Indoor Structures to Simplify Scene Understanding and Location Awareness","author":"Pintore, G.\nand Ganovelli, F.\nand Gobbetti, E.\nand Scopigno, R.","meta_info":{"isbn":"978-3-319-48881-3","pages":"130--145","year":"2016","booktitle":"European Conference on Computer Vision"}}
{"bib_id":"Fan:CSUR:2019","title":"A Survey on 360° Video Streaming","author":"Ching-Ling Fan and Wen-Chih Lo and Yu-Tung Pai and Cheng-Hsin Hsu","meta_info":{"journal":"ACM Computing Surveys","pages":"1--36","number":"4","volume":"52","publisher":"Association for Computing Machinery (ACM)","year":"2019"}}
{"bib_id":"Khan:Sensors:2020","title":"Deep Learning-Based Monocular Depth Estimation Methods - A State-of-the-Art Review","author":"F. Khan and S. Salahuddin and H. Javidnia","meta_info":{"journal":"Sensors","pages":"2272","number":"8","volume":"20","publisher":"MDPI AG","year":"2020"}}
{"bib_id":"Silberman:ICCV:2011","title":"Indoor scene segmentation using a structured light sensor","author":"N. Silberman and R. Fergus","meta_info":{"pages":"601-608","year":"2011","booktitle":"IEEE International Conference on Computer Vision Workshops"}}
{"bib_id":"Xu:JSTSP:2020","title":"State-of-the-Art in 360° Video\/Image Processing: Perception, Assessment and Compression","author":"M. Xu and C. Li and S. Zhang and P. L. Callet","meta_info":{"pages":"5-26","number":"1","volume":"14","year":"2020","journal":"IEEE Journal of Selected Topics in Signal Processing"}}
{"bib_id":"Lukierski:ECMR:2015","title":"Rapid free-space mapping from a single omnidirectional camera","author":"Lukierski, R. and Leutenegger, S. and Davison, A. J.","meta_info":{"year":"2015","pages":"1--8","mendeley-groups":"Jung et al.\/References\/Stereo Multiview and Omni-Stereo","keywords":"Cameras,Image reconstruction,Robot vision systems,Standards,Three-dimensional displays","isbn":"978-1-4673-9163-4","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Lukierski, Leutenegger, Davison - 2015 - Rapid free-space mapping from a single omnidirectional camera.pdf:pdf","booktitle":"IEEE European Conference on Mobile Robots","abstract":"— Low-cost robots such as floor cleaners generally rely on limited perception and simple algorithms, but some new models now have enough sensing capability and compu-tation power to enable Simultaneous Localisation And Map-ping (SLAM) and intelligent guided navigation. In particular, computer vision is now a serious option in low cost robotics, though its use to date has been limited to feature-based mapping for localisation. Dense environment perception such as free space finding has required additional specialised sensors, adding expense and complexity. Here we show that a robot with a single passive omnidirec-tional camera can perform rapid global free-space reasoning within typical rooms. Upon entering a new room, the robot makes a circular movement to capture a closely-spaced omni image sequence with disparity in all horizontal directions. A feature-based visual SLAM procedure obtains accurate poses for these frames before passing them to a dense matching step, 3D semi-dense reconstruction and visibility reasoning. The result is turned into a 2D occupancy map, which can be improved and extended if necessary through further movement. This rapid, passive technique can capture high quality free space information which gives a robot a global understanding of the space around it. We present results in several scenes, including quantitative comparison with laser-based mapping."}}
{"bib_id":"Kuhn:GCPR:2017","title":"Down to Earth: Using Semantics for Robust Hypothesis Selection for the Five-Point Algorithm","author":"Kuhn, A. and Price, T. and Frahm, J.-M. and Mayer, H.","meta_info":{"year":"2017","volume":"10496","booktitle":"German Conference on Pattern Recognition","pages":"389--400","isbn":"978-3-319-66708-9"}}
{"bib_id":"liu:nips:2018","title":"An intriguing failing of convolutional neural networks and the CoordConv solution","author":"Liu, R. and Lehman, J. and Molino, P. and Such, F. P. and Frank, E. and Sergeev, A. and Yosinski, J.","meta_info":{"year":"2018","pages":"9605--9616","booktitle":"Advances in Neural Information Processing Systems"}}
{"bib_id":"wang:arxiv:2021","title":"LED2-Net: Monocular 360deg Layout Estimation via Differentiable Depth Rendering","author":"Wang, Fu-En and Yeh, Yu-Hsuan and Sun, Min and Chiu, Wei-Chen and Tsai, Yi-Hsuan","meta_info":{"pages":"12956-12965","year":"2021","booktitle":"IEEE\/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"}}
{"bib_id":"menze2015object","title":"Object scene flow for autonomous vehicles","author":"Menze, M. and Geiger, A.","meta_info":{"year":"2015","pages":"3061--3070","booktitle":"IEEE Conference on Computer Vision and Pattern Recognition"}}
{"bib_id":"Abdel-Aziz:PERS:1971","title":"Direct linear transformation from comparator coordinates into object space in close-range photogrammetry","author":"Abdel-Aziz, Y. I. and Karara, H. M","meta_info":{"year":"1971","pages":"1--18","booktitle":"Proceedings of the ASP Symposium on Close-Range Photogrammetry"}}
{"bib_id":"Wood:CS-SC:1994","title":"Simulation of the von mises fisher distribution","author":"Wood, A. T.","meta_info":{"year":"1994","volume":"23","pages":"157--164","number":"1","mendeley-groups":"Jung et al.\/References\/Filtering and retrieval","journal":"Communications in Statistics - Simulation and Computation","issn":"0361-0918","isbn":"0361091940881","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Wood - 1994 - Simulation of the von mises fisher distribution.pdf:pdf","abstract":"Ulrich's (1984) proposal for simulating unit vectors from the von Mises-Fisher distribution is discussed and an alternative specification of the algorithm is given. Then we describe an application to the simulation of the von Mises-Fisher matrix distribution for rotations of R3 or,equivalently, the Bingham distribution on the unit sphere in R4. The same idea also leads to a procedure for simulating the Bingham distribution on the unit sphere in Rq when q \\textgreater 4."}}
{"bib_id":"Bay:ECCV:2006","title":"SURF: Speeded Up Robust Features","author":"Bay, H.\nand Tuytelaars, T.\nand Van Gool, L.","meta_info":{"isbn":"978-3-540-33833-8","abstract":"In this paper, we present a novel scale- and rotation-invariant interest point detector and descriptor, coined SURF (Speeded Up Robust Features). It approximates or even outperforms previously proposed schemes with respect to repeatability, distinctiveness, and robustness, yet can be computed and compared much faster.","pages":"404--417","year":"2006","booktitle":"European Conference on Computer Vision"}}
{"bib_id":"Pollefeys:IJCV:2008","title":"Detailed real-time urban 3d reconstruction from video","author":"Pollefeys, M. and Nistér, D. and Frahm, J.-M. and Akbarzadeh, A. and Mordohai, P. and Clipp, B. and Engels, C. and Gallup, D. and Kim, S.-J. and Merrell, P. and others","meta_info":{"publisher":"Springer","year":"2008","pages":"143--167","number":"2-3","volume":"78","journal":"International Journal of Computer Vision"}}
{"bib_id":"bertel:tog:2020","title":"OmniPhotos: Casual $360^∘$ VR Photography","author":"Bertel, T. and Yuan, M. and Lindroos, R. and Richardt, C.","meta_info":{"publisher":"Association for Computing Machinery","year":"2020","pages":"1--12","number":"6","volume":"39","journal":"ACM Transactions on Graphics"}}
{"bib_id":"bertel:tvcg:2019","title":"Megaparallax: Casual 360 panoramas with motion parallax","author":"Bertel, T. and Campbell, N. D. F. and Richardt, C.","meta_info":{"year":"2019","pages":"1828--1835","number":"5","volume":"25","journal":"IEEE Transactions on Visualization and Computer Graphics"}}
{"bib_id":"Agarwal:CACM:2011","title":"Building Rome in a day","author":"Agarwal, S. and Furukawa, Y. and Snavely, N. and Simon, I. and Curless, B. and Seitz, S. M and Szeliski, R.","meta_info":{"year":"2011","pages":"105--112","number":"10","volume":"54","journal":"Communications of the ACM","abstract":"We present a system that can match and reconstruct 3D scenes from extremely large collections of photographs such as those found by searching for a given city (e.g., Rome) on Internet photo sharing sites. Our system uses a collection of novel parallel distributed matching and reconstruction algorithms, designed to maximize parallelism at each stage in the pipeline and minimize serialization bottlenecks. It is designed to scale gracefully with both the size of the problem and the amount of available computation. We have experi- mented with a variety of alternative algorithms at each stage of the pipeline and report on which ones work best in a parallel computing environment. Our experimental results demonstrate that it is now possible to reconstruct cities con- sisting of 150K images in less than a day on a cluster with 500 compute cores."}}
{"bib_id":"Song:CVPR:2016","title":"Semantic Scene Completion from a Single Depth Image","author":"Song, S. and Yu, F.  and Zeng, A. and Chang, A. X and Savva, M. and Funkhouser, T.","meta_info":{"year":"2017","journal":"IEEE Conference on Computer Vision and Pattern Recognition"}}
{"bib_id":"Chang:3DV:2017","title":"Matterport3D: Learning from RGB-D Data in Indoor Environments","author":"Chang, A. and Dai, A. and Funkhouser, T. and Halber, M. and Niessner, M. and Savva, M. and Song, S. and Zeng, A. and Zhang, Y.","meta_info":{"year":"2017","journal":"International Conference on 3D Vision"}}
{"bib_id":"Song:CVPR:2018","title":"Im2Pano3D: Extrapolating 360° Structure and Semantics Beyond the Field of View","author":"Song, S and Zeng, A. and Chang, A. X. and Savva, M. and Savarese, S. and Funkhouser, T.","meta_info":{"year":"2018","volume":"1","pages":"3847--3856","mendeley-groups":"Jung et al.\/References\/Deep learning","isbn":"978-1-5386-6420-9","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Song et al. - 2017 - Im2Pano3D Extrapolating 360 Structure and Semantics Beyond the Field of View.pdf:pdf","booktitle":"IEEE\/CVF Conference on Computer Vision and Pattern Recognition","annote":"Uses SUNCG and Matterport for training","abstract":"We present Im2Pano3D, a convolutional neural network that generates a dense prediction of 3D structure and a probability distribution of semantic labels for a full 360 panoramic view of an indoor scene when given only a partial observation (\\textless= 50%) in the form of an RGB-D image. To make this possible, Im2Pano3D leverages strong contextual priors learned from large-scale synthetic and real-world indoor scenes. To ease the prediction of 3D structure, we propose to parameterize 3D surfaces with their plane equations and train the model to predict these parameters directly. To provide meaningful training supervision, we use multiple loss functions that consider both pixel level accuracy and global context consistency. Experiments demon- strate that Im2Pano3D is able to predict the semantics and 3D structure of the unobserved scene with more than 56% pixel accuracy and less than 0.52m average distance error, which is significantly better than alternative approaches."}}
{"bib_id":"Shen:TIP:2013","title":"Accurate Multiple View 3D Reconstruction Using Patch-Based Stereo for Large-Scale Scenes","author":"S. Shen","meta_info":{"issn":"1941-0042","keywords":"image matching;image reconstruction;image resolution;stereo image processing;multiple view 3D reconstruction;multiple view stereo method;patch-based stereo matching process;depth-map refinement process;dense point cloud;computational efficiency;large-scale scene reconstruction;image resolution;benchmark data set;Cameras;Image reconstruction;Image resolution;Merging;Accuracy;Reliability;Stereo image processing;3D reconstruction;depth-map;multiple view stereo (MVS)","pages":"1901-1914","number":"5","volume":"22","year":"2013","journal":"IEEE Transactions on Image Processing"}}
{"bib_id":"Aggarwal:CVPR:2016","title":"Panoramic Stereo Videos with a Single Camera","author":"Aggarwal, R. and Vohra, A. and Namboodiri, A. M.","meta_info":{"year":"2016","pages":"3755--3763","isbn":"978-1-4673-8851-1","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Aggarwal, Vohra, Namboodiri - 2016 - Panoramic Stereo Videos with a Single Camera.pdf:pdf","booktitle":"IEEE Conference on Computer Vision and Pattern Recognition"}}
{"bib_id":"Saxena:NIPS:2006","title":"Learning depth from single monocular images","author":"Saxena, A. and Chung, S. H and Ng, A. Y.","meta_info":{"year":"2006","pages":"1161--1168","booktitle":"Advances in Peural Information Processing Systems"}}
{"bib_id":"bergmann:sibgrapi:2021","title":"Gravity Alignment for Single Panorama Depth Inference","author":"Matheus A. Bergmann and Paulo G. L. Pinto and Thiago L. T. da Silveira and Claudio R. Jung","meta_info":{"booktitle":"Conference on Graphics,  Patterns and Images (SIBGRAPI)","publisher":"IEEE","pages":"1--8","year":"2021","url":"http:\/\/sibgrapi.sid.inpe.br\/rep\/8JMKD3MGPEW34M\/45CTJNE"}}
{"bib_id":"zou:ijcv:2021","title":"Manhattan Room Layout Reconstruction from a Single 360$^∘$ Image: A Comparative Study of State-of-the-Art Methods","author":"Zou, C. and Su, J.-W. and Peng, C.-H. and Colburn, A. and Shan, Q. and Wonka, P. and Chu, H.-K. and Hoiem, D.","meta_info":{"publisher":"Springer","year":"2021","pages":"1--22","journal":"International Journal of Computer Vision"}}
{"bib_id":"jiang:arxiv:2021","title":"UniFuse: Unidirectional Fusion for 360$^∘$ Panorama Depth Estimation","author":"Jiang, Hualie and Sheng, Zhe and Zhu, Siyu and Dong, Zilong and Huang, Rui","meta_info":{"publisher":"IEEE","year":"2021","pages":"1519--1526","number":"2","volume":"6","journal":"IEEE Robotics and Automation Letters"}}
{"bib_id":"Armeni:arXiv:2017","title":"Joint 2D-3D-Semantic Data for Indoor Scene Understanding","author":"Armeni, I. and Sax, S. and Zamir, A. R. and Savarese, S.","meta_info":{"year":"2017","mendeley-groups":"Jung et al.\/References\/SfM and Omni-SfM","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Armeni et al. - 2017 - Joint 2D-3D-Semantic Data for Indoor Scene Understanding.pdf:pdf","eprint":"1702.01105","arxivid":"1702.01105","archiveprefix":"arXiv","abstract":"We present a dataset of large-scale indoor spaces that provides a variety of mutually registered modalities from 2D, 2.5D and 3D domains, with instance-level semantic and geometric annotations. The dataset covers over 6,000m2 and contains over 70,000 RGB images, along with the corresponding depths, surface normals, semantic annotations, global XYZ images (all in forms of both regular and 360\\$\\backslash$deg\\ equirectangular images) as well as camera information. It also includes registered raw and semantically annotated 3D meshes and point clouds. The dataset enables development of joint and cross-modal learning models and potentially unsupervised approaches utilizing the regularities present in large-scale indoor spaces. The dataset is available here: http:\/\/3Dsemantics.stanford.edu\/"}}
{"bib_id":"Guan:CVPR:2017","title":"BRISKS: Binary Features for Spherical Images on a Geodesic Grid","author":"Guan, H. and Smith, W. A. P.","meta_info":{"year":"2017","pages":"4886--4894","mendeley-groups":"Jung et al.\/References\/SfM and Omni-SfM","isbn":"978-1-5386-0457-1","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Guan, Smith - 2017 - BRISKS Binary Features for Spherical Images on a Geodesic Grid.pdf:pdf","booktitle":"IEEE Conference on Computer Vision and Pattern Recognition"}}
{"bib_id":"Zhao:IJCV:2014","title":"SPHORB: A Fast and Robust Binary Feature on the Sphere","author":"Zhao, Q. and Feng, W. and Wan, L. and Zhang, J.","meta_info":{"year":"2014","volume":"113","publisher":"Springer US","pages":"143--159","number":"2","mendeley-groups":"Jung et al.\/References\/SfM and Omni-SfM","keywords":"Binary feature,Geodesic grid,Panoramic image matching,Spherical image","journal":"International Journal of Computer Vision","issn":"15731405","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Zhao et al. - 2014 - SPHORB A Fast and Robust Binary Feature on the Sphere.pdf:pdf","abstract":"In this paper, we propose SPHORB, a new fast and robust binary feature detector and descriptor for spher-ical panoramic images. In contrast to state-of-the-art spher-ical features, our approach stems from the geodesic grid, a nearly equal-area hexagonal grid parametrization of the sphere used in climate modeling. It enables us to directly build fine-grained pyramids and construct robust features on the hexagonal spherical grid, thus avoiding the costly compu-tation of spherical harmonics and their associated bandwidth limitation. We further study how to achieve scale and rota-tion invariance for the proposed SPHORB feature. Extensive experiments show that SPHORB consistently outperforms other existing spherical features in accuracy, efficiency and robustness to camera movements. The superior performance of SPHORB has also been validated by real-world matching tests."}}
{"bib_id":"Cruz-Mota:IJCV:2012","title":"Scale invariant feature transform on the sphere: Theory and applications","author":"Cruz-Mota, J. and Bogdanova, I. and Paquier, B. and Bierlaire, M. and Thiran, J. P.","meta_info":{"year":"2012","volume":"98","pages":"217--241","number":"2","mendeley-groups":"Jung et al.\/References","keywords":"(Spherical) image processing,Feature extraction,Matching,Object detection,Omnidirectional vision,SIFT","journal":"International Journal of Computer Vision","issn":"09205691","isbn":"4121693760","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Cruz-Mota et al. - 2012 - Scale invariant feature transform on the sphere Theory and applications.pdf:pdf"}}
{"bib_id":"Silveira:SIBGRAPI:2017","title":"Evaluation of Keypoint Extraction and Matching for Pose Estimation Using Pairs of Spherical Images","author":"da Silveira, T. L. T. and Jung, C. R.","meta_info":{"year":"2017","pages":"374--381","mendeley-groups":"Jung et al.\/Published","journal":"Conference on Graphics, Patterns and Images","isbn":"978-1-5386-2219-3","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Silveira, Jung - 2017 - Evaluation of Keypoint Extraction and Matching for Pose Estimation Using Pairs of Spherical Images.pdf:pdf"}}
{"bib_id":"Silveira:CVPR:2019","title":"Perturbation Analysis of the 8-Point Algorithm: A Case Study for Wide FoV Cameras","author":"da Silveira, T. L. T. and Jung, C. R.","meta_info":{"year":"2019","issn":"1063-6919","pages":"11757--11766","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Silveira, Jung - 2019 - Perturbation Analysis of the 8-Point Algorithm A Case Study for Wide FoV Cameras.pdf:pdf","booktitle":"IEEE Conference on Computer Vision and Pattern Recognition"}}
{"bib_id":"Kumari:IJEM:2016","title":"A Survey on Stereo Matching Techniques for 3D Vision in Image Processing","author":"D. Kumari and  K. Kaur","meta_info":{"journal":"International Journal of Engineering and Manufacturing","pages":"40--49","number":"4","volume":"6","publisher":"MECS Publisher","year":"2016"}}
{"bib_id":"Hamzah:Sensors:2016","title":"Literature Survey on Stereo Vision Disparity Map Algorithms","author":"R. A. Hamzah and H. Ibrahim","meta_info":{"journal":"Journal of Sensors","pages":"1--23","volume":"2016","publisher":"Hindawi Limited","year":"2016"}}
{"bib_id":"Paya:Sensors:2017","title":"A State-of-the-Art Review on Mapping and Localization of Mobile Robots Using Omnidirectional Vision Sensors","author":"L. Payá and A. Gil and O. Reinoso","meta_info":{"journal":"Journal of Sensors","pages":"1--20","volume":"2017","publisher":"Hindawi Limited","year":"2017"}}
{"bib_id":"Gledhill:CAG:2003","title":"Panoramic imaging - A review","author":"Gledhill, D. and Tian, G. Yun and Taylor, D. and Clarke, D.","meta_info":{"year":"2003","volume":"27","pages":"435--445","number":"3","keywords":"3D Modelling,3D Panoramic imaging,Correspondence,Image-based rendering,Stereo vision,Virtual environment","journal":"Computers and Graphics (Pergamon)","issn":"00978493","file":":home\/trugillo\/Downloads\/1-s2.0-S0097849303000384-main.pdf:pdf"}}
{"bib_id":"Xie:3DV:2019","title":"Effective Convolutional Neural Network Layers in Flow Estimation for Omni-Directional Images","author":"S. Xie and P. K. Lai and R. Laganiere and J. Lang","meta_info":{"issn":"2378-3826","pages":"671-680","number":"","volume":"","year":"2019","booktitle":"Conference on 3D Vision"}}
{"bib_id":"Zhang:ICRA:2016","title":"Benefit of large field-of-view cameras for visual odometry","author":"Zhang, Z. and Rebecq, H. and Forster, C. and Scaramuzza, D.","meta_info":{"year":"2016","pages":"801--808","mendeley-groups":"Jung et al.\/References\/Pose estimation","keywords":"Omnidirectional Vision,SLAM,Visual-Based Navigation","journal":"IEEE International Conference on Robotics and Automation","issn":"10504729","isbn":"9781467380263","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Zhang et al. - 2016 - Benefit of large field-of-view cameras for visual odometry.pdf:pdf","abstract":"The transition of visual-odometry technology from research demonstrators to commercial applications naturally raises the question: “what is the optimal camera for vision- based motion estimation?” This question is crucial as the choice of camera has a tremendous impact on the robustness and accuracy of the employed visual odometry algorithm. While many properties of a camera (e.g. resolution, frame-rate, global- shutter\/rolling-shutter) could be considered, in this work we focus on evaluating the impact of the camera field-of-view (FoV) and optics (i.e., fisheye or catadioptric) on the quality of the motion estimate. Since the motion-estimation performance depends highly on the geometry of the scene and the motion of the camera, we analyze two common operational environments in mobile robotics: an urban environment and an indoor scene. To confirm the theoretical observations, we implement a state- of-the-art VO pipeline that works with large FoV fisheye and catadioptric cameras. We evaluate the proposed VO pipeline in both synthetic and real experiments. The experiments point out that it is advantageous to use a large FoV camera (e.g., fisheye or catadioptric) for indoor scenes and a smaller FoV for urban canyon environments."}}
{"bib_id":"Zioulis:arxiv:2021","title":"Single-shot cuboids: Geodesics-based end-to-end Manhattan aligned layout estimation from spherical panoramas","author":"Zioulis, Nikolaos and Alvarez, Federico and Zarpalas, Dimitrios and Daras, Petros","meta_info":{"publisher":"Elsevier","year":"2021","pages":"104160","volume":"110","journal":"Image and Vision Computing"}}
{"bib_id":"Fujiki:MIRAGE:2007","title":"Epipolar Geometry Via Rectification of Spherical Images","author":"Fujiki, J. and Torii, A. and Akaho, S.","meta_info":{"year":"2007","volume":"4418","publisher":"Springer Berlin Heidelberg","pages":"461--471","mendeley-groups":"Jung et al.\/References\/SfM and Omni-SfM","issn":"03029743","isbn":"3540714561","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Fujiki, Torii, Akaho - 2007 - Epipolar Geometry Via Rectification of Spherical Images.pdf:pdf","booktitle":"Computer Vision\/Computer Graphics Collaboration Techniques"}}
{"bib_id":"Su:NIPS:2017","title":"Learning Spherical Convolution for Fast Features from 360$\\backslash$textdegree Imagery","author":"Su, Y.-C. and Grauman, K.","meta_info":{"year":"2017","pages":"529--539","mendeley-groups":"Jung et al.\/References\/SfM and Omni-SfM,Jung et al.\/References\/Deep learning,Jung et al.\/References\/Maths","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Su, Grauman - 2017 - Learning Spherical Convolution for Fast Features from 360textdegree Imagery.pdf:pdf","booktitle":"Conference on Neural Information Processing Systems"}}
{"bib_id":"Morel:JIS:2009","title":"ASIFT: A New Framework for Fully Affine Invariant Image Comparison","author":"Morel, J.-M. and Yu, G.","meta_info":{"year":"2009","volume":"2","pages":"438--469","number":"2","mendeley-groups":"Jung et al.\/References\/Keypoints & descriptors","keywords":"080732730,10,1137,68t10,68t40,68t45,93c85,affine invariance,affine normalization,ams subject classifications,descriptors,doi,feature transform,image matching,scale invariance,scale-invariant,sift","journal":"SIAM Journal on Imaging Sciences","issn":"1936-4954","isbn":"1936-4954","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Morel, Yu - 2009 - ASIFT A New Framework for Fully Affine Invariant Image Comparison.pdf:pdf"}}
{"bib_id":"Alcantarilla:ECCV:2012","title":"KAZE Features","author":"Alcantarilla, P. and Bartoli, A. and Davison, A. J.","meta_info":{"year":"2012","publisher":"Springer Berlin Heidelberg","pages":"214--227","mendeley-groups":"Jung et al.\/References\/Keypoints & descriptors","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Alcantarilla, Bartoli, Davison - 2012 - KAZE Features.pdf:pdf","booktitle":"European Conference on Computer Vision"}}
{"bib_id":"Alcantarilla:BMVC:2013","title":"Fast Explicit Diffusion for Accelerated Features in Nonlinear Scale Spaces","author":"Alcantarilla, P. and Nuevo, J. and Bartoli, A.","meta_info":{"year":"2013","pages":"13.1--13.11","mendeley-groups":"Jung et al.\/References\/Keypoints & descriptors","journal":"British Machine Vision Conference","isbn":"1-901725-49-9","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Alcantarilla, Nuevo, Bartoli - 2013 - Fast Explicit Diffusion for Accelerated Features in Nonlinear Scale Spaces.pdf:pdf;:home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Alcantarilla, Nuevo, Bartoli - 2013 - Fast Explicit Diffusion for Accelerated Features in Nonlinear Scale Spaces(2).pdf:pdf","annote":"AKAZE"}}
{"bib_id":"Oliveira:ICASSP:2019","title":"On the Performance of DIBR Methods When Using Depth Maps from State-of-the-art Stereo Matching Algorithms","author":"de Oliveira, Adriano Q. and da Silveira, Thiago L. T. and Walter, Marcelo and Jung, Claudio R.","meta_info":{"year":"2019","publisher":"IEEE","pages":"2272--2276","number":"Vi","isbn":"978-1-4799-8131-1","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/de Oliveira et al. - 2019 - On the Performance of DIBR Methods When Using Depth Maps from State-of-the-art Stereo Matching Algorithms.pdf:pdf","booktitle":"ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"}}
{"bib_id":"Eder:WCVPR:2019","title":"Convolutions on Spherical Images","author":"Eder, M. and Frahm, J.-M.","meta_info":{"year":"2019","journal":"IEEE Conference on Computer Vision and Pattern Recognition Workshops","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Eder, Frahm - 2019 - Convolutions on Spherical Images.pdf:pdf","abstract":"Applying convolutional neural networks to spherical images requires particular considerations. We look to the millennia of work on cartographic map projections to provide the tools to define an optimal representation of spherical images for the convolution operation. We propose a representation for deep spherical image inference based on the icosahedral Snyder equal-area (ISEA) projection, a projection onto a geodesic grid, and show that it vastly exceeds the state-of-the-art for convolution on spherical images, improving semantic segmentation results by 12.6%."}}
{"bib_id":"barazzetti:isprs:2017","title":"3D modelling with the Samsung Gear 360","author":"Barazzetti, L. and Previtali, M. and Roncoroni, F.","meta_info":{"organization":"International Society for Photogrammetry and Remote Sensing","year":"2017","pages":"85--90","number":"2W3","volume":"42","booktitle":"Virtual Reconstruction and Visualization of Complex Architectures"}}
{"bib_id":"felzenszwalb:ijcv:2004","title":"Efficient graph-based image segmentation","author":"Felzenszwalb, P. F. and Huttenlocher, D. P","meta_info":{"publisher":"Springer","year":"2004","pages":"167--181","number":"2","volume":"59","journal":"International Journal of Computer Vision"}}
{"bib_id":"Weinzaepfel:ICCV:2013","title":"DeepFlow: Large displacement optical flow with deep matching","author":"Weinzaepfel, P. and Revaud, J. and Harchaoui, Z. and Schmid, C.","meta_info":{"year":"2013","pages":"1385--1392","mendeley-groups":"Jung et al.\/References\/Deep learning","keywords":"deep convolutional networks,dense matching,large displacements,non-rigid matching,optical flow","journal":"IEEE International Conference on Computer Vision","issn":"1550-5499","isbn":"9781479928392","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Weinzaepfel et al. - 2013 - DeepFlow Large displacement optical flow with deep matching.pdf:pdf","abstract":"Optical flow computation is a key component in many computer vision systems designed for tasks such as action detection or activity recognition. However, despite several major advances over the last decade, handling large displacement in optical flow remains an open problem. Inspired by the large displacement optical flow of Brox and Malik, our approach, termed Deep Flow, blends a matching algorithm with a variational approach for optical flow. We propose a descriptor matching algorithm, tailored to the optical flow problem, that allows to boost performance on fast motions. The matching algorithm builds upon a multi-stage architecture with 6 layers, interleaving convolutions and max-pooling, a construction akin to deep convolutional nets. Using dense sampling, it allows to efficiently retrieve quasi-dense correspondences, and enjoys a built-in smoothing effect on descriptors matches, a valuable asset for integration into an energy minimization framework for optical flow estimation. Deep Flow efficiently handles large displacements occurring in realistic videos, and shows competitive performance on optical flow benchmarks. Furthermore, it sets a new state-of-the-art on the MPI-Sintel dataset. View full abstract"}}
{"bib_id":"Longuet-Higgins:RCV:1987","title":"A computer algorithm for reconstructing a scene from two projections","author":"H. C. Longuet-Higgins","meta_info":{"isbn":"978-0-08-051581-6","year":"1987","pages":"61 - 62","publisher":"Morgan Kaufmann","booktitle":"Readings in Computer Vision"}}
{"bib_id":"Hartley:TPAMI:1997","title":"In defense of the eight-point algorithm","author":"Hartley, R.","meta_info":{"year":"1997","volume":"19","pages":"580--593","number":"6","mendeley-groups":"Jung et al.\/References\/Cameras and stitching","journal":"IEEE Transactions on Pattern Analysis and Machine Intelligence","issn":"01628828","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Hartley - 1997 - In defense of the eight-point algorithm.pdf:pdf"}}
{"bib_id":"Fuentes-Pacheco:AIR:2012","title":"Visual simultaneous localization and mapping: a survey","author":"Fuentes-Pacheco, J. and Ruiz-Ascencio, J. and Rendón-Mancha, J. M.","meta_info":{"year":"2012","volume":"43","pages":"55--81","number":"1","mendeley-groups":"Jung et al.\/References\/SfM and Omni-SfM","keywords":"Data association,Image matching,Salient feature selection,Topological and metric maps,Visual SLAM","journal":"Artificial Intelligence Review","issn":"15737462","isbn":"02692821 (ISSN)","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Fuentes-Pacheco, Ruiz-Ascencio, Rendón-Mancha - 2012 - Visual simultaneous localization and mapping a survey.pdf:pdf","abstract":"Visual SLAM (simultaneous localization and mapping) refers to the problem of using images, as the only source of external information, in order to establish the position of a robot, a vehicle, or a moving camera in an environment, and at the same time, construct a representation of the explored zone. SLAM is an essential task for the autonomy of a robot. Nowadays, the problem of SLAM is considered solved when range sensors such as lasers or sonar are used to built 2D maps of small static environments. However SLAM for dynamic, complex and large scale environments, using vision as the sole external sensor, is an active area of research. The computer vision techniques employed in visual SLAM, such as detection, description and matching of salient features, image recognition and retrieval, among others, are still susceptible of improvement. The objective of this article is to provide new researchers in the field of visual SLAM a brief and comprehensible review of the state-of-the-art. © 2012 Springer Science+Business Media Dordrecht."}}
{"bib_id":"Johannsen:CVPR:2017","title":"A Taxonomy and Evaluation of Dense Light Field Depth Estimation Algorithms","author":"O. Johannsen and K. Honauer and B. Goldluecke and A. Alperovich and F. Battisti and Y. Bok and M. Brizzi and M. Carli and G. Choe and M. Diebold and M. Gutsche and H. Jeon and I. S. Kweon and J. Park and J. Park and H. Schilling and H. Sheng and L. Si and M. Strecke and A. Sulc and Y. Tai and Q. Wang and T. Wang and S. Wanner and Z. Xiong and J. Yu and S. Zhang and H. Zhu","meta_info":{"pages":"1795-1812","number":"","volume":"","year":"2017","booktitle":"IEEE Conference on Computer Vision and Pattern Recognition Workshops"}}
{"bib_id":"Bhoi:arXiv:2019","title":"Monocular Depth Estimation: A Survey","author":"A. Bhoi","meta_info":{"year":"2019","journal":"CoRR"}}
{"bib_id":"Scharstein:SMBV:2001","title":"A taxonomy and evaluation of dense two-frame stereo correspondence algorithms","author":"Scharstein, D. and Szeliski, R. and Zabih, R.","meta_info":{"year":"2001","volume":"47","pmid":"350","pages":"131--140","number":"1","mendeley-groups":"Jung et al.\/References\/Stereo Multiview and Omni-Stereo","keywords":"evaluation of performance,stereo correspondence software,stereo matching survey","journal":"IEEE Workshop on Stereo and Multi-Baseline Vision","issn":"09205691","isbn":"0769513271","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Scharstein, Szeliski, Zabih - 2001 - A taxonomy and evaluation of dense two-frame stereo correspondence algorithms.pdf:pdf"}}
{"bib_id":"Akihiko:WOVCNNC:2005","title":"Two-and three-view geometry for spherical cameras","author":"Akihiko, T. and Atsushi, I. and Ohnishi, N.","meta_info":{"year":"2005","volume":"105","pages":"29--34","mendeley-groups":"Jung et al.\/References\/SfM and Omni-SfM","journal":"Workshop on Omnidirectional Vision, Camera Networks and Non-classical Cameras","issn":"0913-5685","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Akihiko, Atsushi, Ohnishi - 2005 - Two-and three-view geometry for spherical cameras.pdf:pdf","abstract":"In this paper, we introduce two- and three- view geometry for spherical cameras using geometric duality. Geometric duality provides us to deal with points and lines simultaneously using homogeneous coordinate systems. Using the geometric duality on a sphere, we derive geometric constraints, that is, epipolar circle constraint, for the three-dimensional reconstruction of points and lines. (author abst.)"}}
{"bib_id":"Xiao:CVPR:2012","title":"Recognizing scene viewpoint using panoramic place representation","author":"J. Xiao and E., Krista A. and Oliva, A. and Torralba, A.","meta_info":{"year":"2012","pages":"2695--2702","mendeley-groups":"Jung et al.\/References","isbn":"978-1-4673-1228-8","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Jianxiong Xiao et al. - 2012 - Recognizing scene viewpoint using panoramic place representation.pdf:pdf","booktitle":"IEEE Conference on Computer Vision and Pattern Recognition"}}
{"bib_id":"Hartley:Book:2003","title":"Multiple View Geometry in Computer Vision","author":"Hartley, R. and Zisserman, A.","meta_info":{"year":"2003","publisher":"Cambridge","mendeley-groups":"Jung et al.\/References\/Books and theses","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Hartley, Zisserman - 2003 - Multiple View Geometry in Computer Vision.pdf:pdf"}}
{"bib_id":"Ozyesil:AN:2017","title":"A survey of structure from motion.","author":"Özyeşil, O. and Voroninski, V. and Basri, R. and Singer, A.","meta_info":{"year":"2017","pmid":"19963286","pages":"305--364","mendeley-groups":"Jung et al.\/References\/SfM and Omni-SfM","journal":"Acta Numerica","issn":"0962-4929","isbn":"978-1-5090-1437-8","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Ozyesil et al. - 2017 - A Survey of Structure from Motion.pdf:pdf","abstract":"The structure from motion (SfM) problem in computer vision is to recover the three-dimensional (3D) structure of a stationary scene from a set of projective measurements, represented as a collection of two-dimensional (2D) images, via estimation of motion of the cameras corresponding to these images. In essence, SfM involves the three main stages of (i) extracting features in images ( e.g. points of interest, lines, etc. ) and matching these features between images, (ii) camera motion estimation ( e.g. using relative pairwise camera positions estimated from the extracted features), and (iii) recovery of the 3D structure using the estimated motion and features ( e.g. by minimizing the so-called reprojection error ). This survey mainly focuses on relatively recent developments in the literature pertaining to stages (ii) and (iii). More specifically, after touching upon the early factorization-based techniques for motion and structure estimation, we provide a detailed account of some of the recent camera location estimation methods in the literature, followed by discussion of notable techniques for 3D structure recovery. We also cover the basics of the simultaneous localization and mapping (SLAM) problem, which can be viewed as a specific case of the SfM problem. Further, our survey includes a review of the fundamentals of feature extraction and matching ( i.e. stage (i) above), various recent methods for handling ambiguities in 3D scenes, SfM techniques involving relatively uncommon camera models and image features, and popular sources of data and SfM software."}}
{"bib_id":"Kang:ISPRS:2020","title":"A Review of Techniques for 3D Reconstruction of Indoor Environments","author":"Z. Kang and J. Yang and Z. Yang and S. Cheng","meta_info":{"journal":"ISPRS International Journal of Geo-Information","pages":"330","number":"5","volume":"9","publisher":"MDPI AG","year":"2020"}}
{"bib_id":"Tchapmi2019","title":"The SUMO Challenge","author":"L. Tchapmi and D. Huber","meta_info":{"year":"2019"}}
{"bib_id":"Azevedo:TCSVT:2019","title":"Visual Distortions in 360-degree Videos","author":"Azevedo, R. G. de A. and Birkbeck, N. and De Simone, F. and Janatra, I. and Adsumilli, B. and Frossard, P.","meta_info":{"pages":"2524-2537","number":"8","volume":"30","year":"2020","publisher":"IEEE","journal":"IEEE Transactions on Circuits and Systems for Video Technology","issn":"1051-8215","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Azevedo et al. - 2019 - Visual Distortions in 360-degree Videos.pdf:pdf","abstract":"Omnidirectional (or 360-degree) images and videos are emergent signals in many areas such as robotics and virtual\/augmented reality. In particular, for virtual reality, they allow an immersive experience in which the user is provided with a 360-degree field of view and can navigate throughout a scene, e.g., through the use of Head Mounted Displays. Since it represents the full 360-degree field of view from one point of the scene, omnidirectional content is naturally represented as spherical visual signals. Current approaches for capturing, processing, delivering, and displaying 360-degree content, however, present many open technical challenges and introduce several types of distortions in these visual signals. Some of the distortions are specific to the nature of 360-degree images, and often different from those encountered in the classical image communication framework. This paper provides a first comprehensive review of the most common visual distortions that alter 360-degree signals undergoing state of the art processing in common applications. While their impact on viewers' visual perception and on the immersive experience at large is still unknown ---thus, it stays an open research topic--- this review serves the purpose of identifying the main causes of visual distortions in the end-to-end 360-degree content distribution pipeline. It is essential as a basis for benchmarking different processing techniques, allowing the effective design of new algorithms and applications. It is also necessary to the deployment of proper psychovisual studies to characterise the human perception of these new images in interactive and immersive applications."}}
{"bib_id":"Wien:JETCAS:2019","title":"Standardization Status of Immersive Video Coding","author":"M. Wien and J. M. Boyce and T. Stockhammer and W. Peng","meta_info":{"pages":"5-17","number":"1","volume":"9","year":"2019","journal":"IEEE Journal on Emerging and Selected Topics in Circuits and Systems"}}
{"bib_id":"Thatte:VCIP:2017","title":"Stacked Omnistereo for virtual reality with six degrees of freedom","author":"Thatte, J. and Lian, T. and Wandell, B. and Girod, B.","meta_info":{"year":"2017","pages":"1--4","mendeley-groups":"Jung et al.\/References\/Stereo Multiview and Omni-Stereo","isbn":"978-1-5386-0462-5","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Thatte et al. - 2017 - Stacked Omnistereo for virtual reality with six degrees of freedom.pdf:pdf","booktitle":"IEEE Visual Communications and Image Processing"}}
{"bib_id":"Seitz:CVPR:2006","title":"A Comparison and Evaluation of Multi-View Stereo Reconstruction Algorithms","author":"Seitz, S. M. and Curless, B. and Diebel, J. and Scharstein, D. and Szeliski, R.","meta_info":{"year":"2006","volume":"1","pmid":"18787244","pages":"519--528","mendeley-groups":"Jung et al.\/References\/SfM and Omni-SfM","keywords":"Cameras,Educational institutions,Image databases,Image reconstruction,Layout,Reconstruction algorithms,Shape measurement,Stereo image processing,Stereo vision,Taxonomy","issn":"1063-6919","isbn":"0-7695-2597-0","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Seitz et al. - 2006 - A Comparison and Evaluation of Multi-View Stereo Reconstruction Algorithms.pdf:pdf","booktitle":"IEEE Conference on Computer Vision and Pattern Recognition","abstract":"This paper presents a quantitative comparison of several multi-view stereo reconstruction algorithms. Until now, the lack of suitable calibrated multi-view image datasets with known ground truth (3D shape models) has prevented such direct comparisons. In this paper, we first survey multi-view stereo algorithms and compare them qualitatively using a taxonomy that differentiates their key properties. We then describe our process for acquiring and calibrating multiview image datasets with high-accuracy ground truth and introduce our evaluation methodology. Finally, we present the results of our quantitative comparison of state-of-the-art multi-view stereo reconstruction algorithms on six benchmark datasets. The datasets, evaluation details, and instructions for submitting new models are available online at http:\/\/vision.middlebury.edu\/mview."}}
{"bib_id":"silveira:sigpro2021","title":"Fast and Accurate Superpixel Algorithms for 360$^∘$ Images","author":"Thiago L.T. da Silveira and Adriano Q. de Oliveira and Marcelo Walter and Cláudio R. Jung","meta_info":{"abstract":"Superpixels are fundamental in many visual computing applications, and most existing algorithms are designed to work with pinhole-based images. However, immersive applications are gaining visibility with the growing number of devices for capturing and visualizing 360∘ media. This paper introduces two fast and accurate superpixel algorithms tailored to the spherical domain. The methods are assessed under common figures of merit, and a benchmark annotated dataset. Additionally, an annotation-free evaluation metric for exploiting larger datasets is introduced. The methods introduced in this paper perform quantitatively close to or better than state-of-the-art approaches and are faster.","keywords":"Superpixels, oversegmentation, spherical images, 360 images","url":"https:\/\/www.sciencedirect.com\/science\/article\/pii\/S0165168421003145","doi":"https:\/\/doi.org\/10.1016\/j.sigpro.2021.108277","issn":"0165-1684","volume":"189","year":"2021","pages":"108277","journal":"Signal Processing"}}
{"bib_id":"Nayar:CVPR:1997","title":"Catadioptric Omnidirectional Camera*","author":"Nayar, S. K.","meta_info":{"year":"1997","pages":"482--488","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Nayar - 1997 - Catadioptric Omnidirectional Camera.pdf:pdf","booktitle":"Conference on Computer Vision and Pattern Recognition","abstract":"Conventional video cameras have limited fields of view that make them restrictive in a variety of vision ap-plications. There are several ways to enhance the field of view of an imaging system. However, the entire imaging system must have a single effective viewpoint to enable the generation of pure perspective images from a sensed image. A new camera with a hemispherical field of view is presented. Two such cameras can be placed back-to-back, without violating the single viewpoint constraint, to arrive at a truly omnidirectional sensor. Results are presented on the software generation of pure perspective images from an omnidirectional image, given any user-selected viewing direction and magnification. The paper concludes with a discussion on the spatial resolution of the proposed camera."}}
{"bib_id":"Anderson:TGA:2016","title":"Jump: Virtual Reality Video","author":"Anderson, R. and Gallup, D. and Barron, J. T and K., Janne and Snavely, N. and Hernández, C. and Agarwal, S. and Seitz, S. M.","meta_info":{"year":"2016","volume":"3516","pages":"978--1","number":"1312","mendeley-groups":"Jung et al.\/References\/Stereo Multiview and Omni-Stereo","keywords":"Panoramic stereo imaging,Video stitching Concepts,Virtual reality","journal":"ACM Trans. Graph. Article","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Anderson et al. - 2016 - Jump Virtual Reality Video.pdf:pdf"}}
{"bib_id":"Eder:CVPR:2020","title":"Tangent Images for Mitigating Spherical Distortion","author":"Eder, M. and Shvets, M. and Lim, J. and Frahm, J.-M.","meta_info":{"year":"2020","booktitle":"IEEE Conference on Computer Vision and Pattern Recognition"}}
{"bib_id":"Lo:ICIP:2018","title":"Image Stitching for Dual Fisheye Cameras","author":"I. Lo and K. Shih and H. H. Chen","meta_info":{"issn":"2381-8549","keywords":"cameras;feature extraction;image capture;image matching;image segmentation;photography;stereo image processing;dual fisheye cameras;panoramic photography;camera centers;feature matching;equirectangular plane;fisheye images;local warping;image alignment;high-quality seamless panoramic images;image capture;geometric structure;image stitching;Cameras;Lenses;Feature extraction;Computer vision;Distortion;Robustness;Transforms;Fisheye lens camera;stitching;parallax;equirectangular transformation;virtual reality","pages":"3164-3168","number":"","volume":"","year":"2018","booktitle":"IEEE International Conference on Image Processing"}}
{"bib_id":"Luo:TVCG:2018","title":"Parallax360: Stereoscopic 360$^∘$ Scene Representation for Head-Motion Parallax","author":"Luo, B. and Xu, F. and Richardt, C. and Yong, J.-H.","meta_info":{"year":"2018","volume":"24","pages":"1545--1553","number":"4","mendeley-groups":"Jung et al.\/References","journal":"IEEE Transactions on Visualization and Computer Graphics","issn":"1077-2626"}}
{"bib_id":"Yang:ECCV:2014","title":"Optimal Essential Matrix Estimation via Inlier-Set Maximization","author":"Yang, J. and Li, H. and Jia, Y.","meta_info":{"year":"2014","pages":"111--126","number":"PART 1","mendeley-groups":"Jung et al.\/References\/Pose estimation","keywords":"Essential matrix,branch-and-bound,global optimization,robust estimation","issn":"16113349","isbn":"9783319105895","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Yang, Li, Jia - 2014 - Optimal essential matrix estimation via inlier-set maximization.pdf:pdf","booktitle":"European Conference on Computer Vision"}}
{"bib_id":"Nister:TPAMI:2004","title":"An efficient solution to the five-point relative pose problem","author":"Nistér, D.","meta_info":{"year":"2004","volume":"26","pmid":"18579936","pages":"756--770","number":"6","mendeley-groups":"Jung et al.\/References\/Pose estimation","keywords":"Camera calibration,Ego-motion estimation,Imaging geometry,Motion,Relative orientation,Scene reconstruction,Structure from motion","journal":"IEEE Transactions on Pattern Analysis and Machine Intelligence","issn":"01628828","isbn":"0-7695-1900-8","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Nistér - 2004 - An efficient solution to the five-point relative pose problem.pdf:pdf"}}
{"bib_id":"Labutov:ICRA:2011","title":"Fusing optical flow and stereo in a spherical depth panorama using a single-camera folded catadioptric rig","author":"I. Labutov and C. Jaramillo and J. Xiao","meta_info":{"issn":"1050-4729","keywords":"cameras;image resolution;image sequences;mirrors;stereo image processing;optical flow;single-camera folded catadioptric rig;catadioptric-stereo rig;coaxially-aligned perspective camera;spherical mirror;folded configuration;spherical dense depth panorama;depth resolution;depth cue;view-sphere;field-of-view;linearized model;single viewpoint constraint;Mirrors;Optical imaging;Adaptive optics;Cameras;Robots;Uncertainty;Approximation methods","pages":"3092-3097","number":"","volume":"","year":"2011","booktitle":"IEEE International Conference on Robotics and Automation"}}
{"bib_id":"Zheng:arXiv:2019","title":"Structured3D: A Large Photo-Realistic Dataset for Structured 3D Modeling","author":"Jia Zheng and Junfei Zhang and Jing Li and Rui Tang and Shenghua Gao and Zihan Zhou","meta_info":{"pages":"519--535","publisher":"Springer International Publishing","year":"2020","url":"https:\/\/doi.org\/10.1007\/978-3-030-58545-7_30","doi":"10.1007\/978-3-030-58545-7_30"}}
{"bib_id":"Furukawa:CVPR:2007","title":"Accurate, Dense, and Robust Multi-View Stereopsis","author":"Furukawa, Y. and Ponce, J.","meta_info":{"year":"2007","volume":"32","pages":"1--8","number":"8","mendeley-groups":"Jung et al.\/References\/Keypoints & descriptors","isbn":"1-4244-1179-3","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Furukawa, Ponce - 2007 - Accurate, Dense, and Robust Multi-View Stereopsis.pdf:pdf","booktitle":"IEEE Conference on Computer Vision and Pattern Recognition"}}
{"bib_id":"Godard:CVPR:2017","title":"Unsupervised Monocular Depth Estimation with Left-Right Consistency","author":"Godard, C. and Mac Aodha, O. and Brostow, G. J.","meta_info":{"year":"2017","pages":"270--279","mendeley-groups":"Jung et al.\/References\/Deep learning","booktitle":"Conference on Computer Vision and Pattern Recognition"}}
{"bib_id":"Taguchi:etal:TOG2010","title":"Axial-cones: Modeling spherical catadioptric cameras for wide-angle light field rendering","author":"Taguchi, Yuichi and Agrawal, Amit and Veeraraghavan, Ashok and Ramalingam, Srikumar and Raskar, Ramesh","meta_info":{"publisher":"Citeseer","year":"2010","pages":"172","number":"6","volume":"29","journal":"ACM Trans. Graph."}}
{"bib_id":"Birklbauer:etal:CGF2014","title":"Panorama light-field imaging","author":"Birklbauer, Clemens and Bimber, Oliver","meta_info":{"organization":"Wiley Online Library","year":"2014","pages":"43--52","number":"2","volume":"33","booktitle":"Computer Graphics Forum"}}
{"bib_id":"Bolles:etal:IJCV87","title":"Epipolar-plane image analysis: An approach to determining structure from motion","author":"Bolles, Robert C and Baker, H Harlyn and Marimont, David H","meta_info":{"publisher":"Springer","year":"1987","pages":"7--55","number":"1","volume":"1","journal":"International Journal of Computer Vision"}}
{"bib_id":"Levoy:Hanraran:CCGIT96","title":"Light field rendering","author":"Levoy, Marc and Hanrahan, Pat","meta_info":{"year":"1996","pages":"31--42","booktitle":"Proceedings of the 23rd annual conference on Computer graphics and interactive techniques"}}
{"bib_id":"vaswani:nips:2017","title":"Attention is all you need","author":"Vaswani, A. and Shazeer, N. and Parmar, N. and Uszkoreit, J. and Jones, L. and Gomez, A. N. and Kaiser, L. and Polosukhin, I.","meta_info":{"year":"2017","pages":"5998--6008","volume":"30","journal":"Advances in Neural Information Processing Systems"}}
{"bib_id":"sun:arxiv:2020","title":"Hohonet: 360 indoor holistic understanding with latent horizontal features","author":"Sun, Cheng and Sun, Min and Chen, Hwann-Tzong","meta_info":{"year":"2021","pages":"2573--2582","booktitle":"Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern Recognition"}}
{"bib_id":"Cohen:ICLR:2018","title":"Spherical CNNs","author":"T. S. Cohen and\nM. Geiger and\nJ. Köhler and\nM. Welling","meta_info":{"timestamp":"Mon, 13 Aug 2018 16:48:25 +0200","year":"2018","journal":"International Conference on Learning Representations"}}
{"bib_id":"Dai:ICCV:2017","title":"Deformable Convolutional Networks","author":"J. Dai and H. Qi and Y. Xiong and Y. Li and G. Zhang and H. Hu and Y. Wei","meta_info":{"issn":"2380-7504","keywords":"computer vision;convolution;feedforward neural nets;image segmentation;learning (artificial intelligence);object detection;deformable convolutional networks;convolutional neural networks;geometric transformations;fixed geometric structures;transformation modeling capability;deformable convolution;deformable RoI pooling;spatial sampling locations;dense spatial transformation;deep CNN;standard back-propagation;Convolution;Kernel;Object detection;Standards;Feature extraction;Two dimensional displays","pages":"764-773","number":"","volume":"","year":"2017","booktitle":"IEEE International Conference on Computer Vision"}}
{"bib_id":"Bentsen:MWR:1999","title":"Coordinate Transformation on a Sphere Using Conformal Mapping","author":"Bentsen, M. and Evensen, G. and Drange, H. and Jenkins, A. D.","meta_info":{"year":"1999","pages":"2733-2740","number":"12","volume":"127","journal":"Monthly Weather Review"}}
{"bib_id":"Harris:BMVA:1988","title":"A Combined Corner and Edge Detector","author":"Harris, C. and Stephens, M.","meta_info":{"year":"1988","pmid":"20130988","pages":"23.1--23.6","mendeley-groups":"Jung et al.\/References\/Keypoints & descriptors","journal":"Alvey Vision Conference","issn":"09639292","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Harris, Stephens - 1988 - A Combined Corner and Edge Detector.pdf:pdf","abstract":"Consistency of image edge filtering is of prime importance for 3D interpretation of image sequences using feature tracking algorithms. To cater for image regions containing texture and isolated features, a combined corner and edge detector based on the local auto-correlation function is utilised, and it is shown to perform with good consistency on natural imagery."}}
{"bib_id":"Lucas:IJCAI:1981","title":"An Iterative Image Registration Technique with an Application to Stereo Vision","author":"Lucas, B. D. and Kanade, T.","meta_info":{"acmid":"1623280","numpages":"6","pages":"674--679","year":"1981","booktitle":"International Joint Conference on Artificial Intelligence "}}
{"bib_id":"Li:TITS:2008","title":"Binocular Spherical Stereo","author":"S. Li","meta_info":{"issn":"1558-0016","keywords":"feature extraction;image matching;image representation;road safety;road vehicles;stereo image processing;traffic engineering computing;video cameras;binocular fish-eye spherical stereo camera model;wider-than-hemispherical field-of-view;road vehicle safety;pinhole camera model;spherical stereo image representation;feature point matching;Cameras;Vehicle driving;Vehicles;Stereo vision;Vehicle safety;Vehicle detection;Intelligent transportation systems;Displays;Driver circuits;Mirrors;Driving assistance;fish-eye camera;stereo vision;3-D information;Driving assistance;fish-eye camera;stereo vision;3-D information","pages":"589-600","number":"4","volume":"9","year":"2008","journal":"IEEE Transactions on Intelligent Transportation Systems"}}
{"bib_id":"Shewchuk:COMGEO:2014","title":"Reprint of: Delaunay refinement algorithms for triangular mesh generation","author":"Shewchuk, J. R.","meta_info":{"year":"2014","keywords":"Computational geometry,Constrained Delaunay triangulation,Delaunay refinement,Delaunay triangulation,Triangular mesh generation","journal":"Computational Geometry: Theory and Applications","issn":"09257721","isbn":"0925-7721","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Shewchuk - 2014 - Reprint of Delaunay refinement algorithms for triangular mesh generation.pdf:pdf","abstract":"Delaunay refinement is a technique for generating unstructured meshes of triangles for use in interpolation, the finite element method, and the finite volume method. In theory and practice, meshes produced by Delaunay refinement satisfy guaranteed bounds on angles, edge lengths, the number of triangles, and the grading of triangles from small to large sizes. This article presents an intuitive framework for analyzing Delaunay refinement algorithms that unifies the pioneering mesh generation algorithms of L. Paul Chew and Jim Ruppert, improves the algorithms in several minor ways, and most importantly, helps to solve the difficult problem of meshing nonmanifold domains with small angles. Although small angles inherent in the input geometry cannot be removed, one would like to triangulate a domain without creating any new small angles. Unfortunately, this problem is not always soluble. A compromise is necessary. A Delaunay refinement algorithm is presented that can create a mesh in which most angles are 30°or greater and no angle is smaller than arcsin[(3\/2)sin($ϕ$\/ 2)]∼(3\/4)$ϕ$, where $ϕ$≤60°is the smallest angle separating two segments of the input domain. New angles smaller than 30°appear only near input angles smaller than 60°. In practice, the algorithm's performance is better than these bounds suggest. Another new result is that Ruppert's analysis technique can be used to reanalyze one of Chew's algorithms. Chew proved that his algorithm produces no angle smaller than 30°(barring small input angles), but without any guarantees on grading or number of triangles. He conjectures that his algorithm offers such guarantees. His conjecture is conditionally confirmed here: if the angle bound is relaxed to less than 26.5°, Chew's algorithm produces meshes (of domains without small input angles) that are nicely graded and size-optimal. © 2014 Published by Elsevier B.V."}}
{"bib_id":"zhang:icml:2019","title":"Making convolutional networks shift-invariant again","author":"Zhang, R.","meta_info":{"year":"2019","pages":"7324--7334","booktitle":"International Conference on Machine Learning"}}
{"bib_id":"Besl:TPAMI:1992","title":"A method for registration of 3-D shapes","author":"P. J. Besl and N. D. McKay","meta_info":{"issn":"1939-3539","keywords":"computational geometry;convergence of numerical methods;iterative methods;optimisation;pattern recognition;picture processing;3D shape registration;pattern recognition;point set registration;iterative closest point;geometric entity;mean-square distance metric;convergence;geometric model;Solid modeling;Motion estimation;Iterative closest point algorithm;Iterative algorithms;Testing;Inspection;Shape measurement;Iterative methods;Convergence;Quaternions","pages":"239-256","number":"2","volume":"14","year":"1992","journal":"IEEE Transactions on Pattern Analysis and Machine Intelligence"}}
{"bib_id":"Radke:Book:2012","title":"Computer Vision for Visual Effects","author":"Radke, R. J.","meta_info":{"year":"2012","publisher":"Cambridge","mendeley-groups":"Jung et al.\/References\/Books and theses","keywords":"gnv64","isbn":"9781139019682","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Radke - 2012 - Computer Vision for Visual Effects.pdf:pdf"}}
{"bib_id":"Zhang:ECCV:2018","title":"Saliency Detection in 360° Videos","author":"Zhang, Z. and Xu, Y. and Yu, J. and Gao, S.","meta_info":{"pages":"504 -- 520","year":"2018","booktitle":"European Conference on Computer Vision"}}
{"bib_id":"Giraud:ARXIV:2020","title":"Generalized Shortest Path-based Superpixels for Accurate Segmentation of Spherical Images","author":"Giraud, Rémi and Pinheiro, Rodrigo Borba and Berthoumieu, Yannick","meta_info":{"doi":"10.1109\/ICPR48806.2021.9412505","pages":"2650-2656","number":"","volume":"","year":"2021","booktitle":"2020 25th International Conference on Pattern Recognition (ICPR)"}}
{"bib_id":"Zhao:TMM:2018","title":"Spherical Superpixel Segmentation","author":"Zhao, Q. and Dai, F. and Ma, Y. and Wan, L. and Zhang, J. and Zhang, Y.","meta_info":{"year":"2018","volume":"20","pages":"1406--1417","number":"6","journal":"IEEE Transactions on Multimedia","issn":"15209210","isbn":"9781467372589"}}
{"bib_id":"Eigen:ICCV:2015","title":"Predicting Depth, Surface Normals and Semantic Labels with a Common Multi-scale Convolutional Architecture","author":"Eigen, D. and Fergus, R.","meta_info":{"year":"2015","pages":"2650--2658","mendeley-groups":"Jung et al.\/References\/Deep learning","issn":"15505499","isbn":"978-1-4673-8391-2","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Eigen, Fergus - 2015 - Predicting Depth, Surface Normals and Semantic Labels with a Common Multi-scale Convolutional Architecture.pdf:pdf","booktitle":"IEEE International Conference on Computer Vision","abstract":"In this paper we address three different computer vision tasks using a single basic architecture: depth prediction, surface normal estimation, and semantic labeling. We use a multiscale convolutional network that is able to adapt easily to each task using only small modifications, regressing from the input image to the output map directly. Our method progressively refines predictions using a sequence of scales, and captures many image details without any superpixels or low-level segmentation. We achieve state-of-the-art performance on benchmarks for all three tasks."}}
{"bib_id":"won:pami:2020","title":"End-to-End Learning for Omnidirectional Stereo Matching with Uncertainty Prior","author":"Won, C. and Ryu, J. and Lim, J.","meta_info":{"publisher":"IEEE","year":"2020","journal":"IEEE Transactions on Pattern Analysis and Machine Intelligence"}}
{"bib_id":"Kolmogorov:ECCV:2002","title":"Multi-camera scene reconstruction via graph cuts","author":"Kolmogorov, V. and Zabih, R.","meta_info":{"year":"2002","pages":"82--96","booktitle":"European Conference on Computer Vision"}}
{"bib_id":"Jang:ISVC:2019","title":"Fast Omnidirectional Depth Densification","author":"Jang, Hyeonjoong\nand Jeon, Daniel S.\nand Ha, Hyunho\nand Kim, Min H.","meta_info":{"isbn":"978-3-030-33720-9","pages":"683--694","address":"Cham","publisher":"Springer International Publishing","year":"2019","booktitle":"Advances in Visual Computing","editor":"Bebis, George\nand Boyle, Richard\nand Parvin, Bahram\nand Koracin, Darko\nand Ushizima, Daniela\nand Chai, Sek\nand Sueda, Shinjiro\nand Lin, Xin\nand Lu, Aidong\nand Thalmann, Daniel\nand Wang, Chaoli\nand Xu, Panpan"}}
{"bib_id":"calonder:pami:2011","title":"BRIEF: Computing a local binary descriptor very fast","author":"Calonder, M. and Lepetit, V. and Ozuysal, M. and Trzcinski, T. and Strecha, C. and Fua, P.","meta_info":{"year":"2011","pages":"1281--1298","number":"7","volume":"34","journal":"IEEE Transactions on Pattern Analysis and Machine Intelligence"}}
{"bib_id":"roxas:iral:2020","title":"Variational Fisheye Stereo","author":"Roxas, M. and Oishi, T.","meta_info":{"publisher":"IEEE","year":"2020","pages":"1303--1310","number":"2","volume":"5","journal":"IEEE Robotics and Automation Letters"}}
{"bib_id":"Saputra:CSUR:2018","title":"Visual SLAM and Structure from Motion in Dynamic Environments","author":"M. R. U. Saputra and A. Markham and N. Trigoni","meta_info":{"journal":"ACM Computing Surveys","pages":"1--36","number":"2","volume":"51","publisher":"Association for Computing Machinery (ACM)","year":"2018"}}
{"bib_id":"Gava:VISAPP:2015","title":"SPHERA: A unifying structure from motion framework for central projection cameras","author":"Gava, C. C. and Stricker, D.","meta_info":{"year":"2015","volume":"3","pages":"285--293","keywords":"3D reconstruction,Central projection cameras,Spherical images,Structure from motion","journal":"International Conference on Computer Vision Theory and Applications","isbn":"9789897580918","file":":tmp\/53017.pdf:pdf","abstract":"As multi-view reconstruction techniques evolve, they accomplish to reconstruct larger environments. This is possible due to the availability of vast image collections of the target scenes. Within the next years it will be necessary to account for all available sources of visual information to supply future 3D reconstruction approaches. Accordingly, Structure from Motion (SfM) algorithms will need to handle such variety of image sources, i.e. perspective, wide-angle or spherical images. Although SfM for perspective and spherical images as well as catadioptric systems have already been studied, state of the art algorithms are not able to deal with these images simultaneously. To close this gap, we developed SPHERA, a unifying SfM framework designed for central projection cameras. It uses a sphere as underlying model, allowing single effective viewpoint vision systems to be treated in a unified way. We validate our framework with quantitative evaluations on synthetic spherical as well as real perspective, spherical and hybrid image datasets. Results show that SPHERA is a powerful framework to support upcoming algorithms and applications on large scale 3D reconstruction."}}
{"bib_id":"Won:ICCV:2019","title":"OmniMVS: End-to-End Learning for Omnidirectional Stereo Matching","author":"Won, C. and Ryu, J. and Lim, J.","meta_info":{"year":"2019","file":":tmp\/1908.06257.pdf:pdf","booktitle":"IEEE International Conference on Computer Vision"}}
{"bib_id":"Won:ICRA:2019","title":"SweepNet: Wide-baseline Omnidirectional Depth Estimation","author":"Won, C. and Ryu, J. and Lim, J.","meta_info":{"year":"2019","pages":"6073--6079","isbn":"978-1-5386-6027-0","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Won, Ryu, Lim - 2019 - SweepNet Wide-baseline Omnidirectional Depth Estimation.pdf:pdf","booktitle":"International Conference on Robotics and Automation"}}
{"bib_id":"Jeong:Sensors:2018","title":"3DoF+ 360 Video Location-Based Asymmetric Down-Sampling for View Synthesis to Immersive VR Video Streaming","author":"Jeong, J. and Jang, D. and Son, J. and Ryu, E.-S.","meta_info":{"year":"2018","volume":"18","number":"9","keywords":"3dof,hevc,multi-view video coding,view synthesis,virtual reality,vsrs","journal":"Sensors","issn":"1424-8220","isbn":"8210489321","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Jeong et al. - 2018 - 3DoF 360 Video Location-Based Asymmetric Down-Sampling for View Synthesis to Immersive VR Video Streaming.pdf:pdf","abstract":"Recently, with the increasing demand for virtual reality (VR), experiencing immersive contents with VR has become easier. However, a tremendous amount of calculation and bandwidth is required when processing 360 videos. Moreover, additional information such as the depth of the video is required to enjoy stereoscopic 360 contents. Therefore, this paper proposes an efficient method of streaming high-quality 360 videos. To reduce the bandwidth when streaming and synthesizing the 3DoF+ 360 videos, which supports limited movements of the user, a proper down-sampling ratio and quantization parameter are offered from the analysis of the graph between bitrate and peak signal-to-noise ratio. High-efficiency video coding (HEVC) is used to encode and decode the 360 videos, and the view synthesizer produces the video of intermediate view, providing the user with an immersive experience."}}
{"bib_id":"Silveira:VR:2019","title":"Dense 3D Scene Reconstruction from Multiple Spherical Images for 3-DoF+ VR Applications","author":"da Silveira, T. L. T. and Jung, C. R.","meta_info":{"year":"2019","pages":"9--18","isbn":"978-1-7281-1377-7","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/da Silveira, Jung - 2019 - Dense 3D Scene Reconstruction from Multiple Spherical Images for 3-DoF VR Applications.pdf:pdf","booktitle":"IEEE Conference on Virtual Reality and 3D User Interfaces"}}
{"bib_id":"Kim:JCVIU:2015","title":"Block world reconstruction from spherical stereo image pairs","author":"Kim, H. and Hilton, A.","meta_info":{"year":"2015","volume":"139","pages":"104--121","keywords":"3D reconstruction,Block world interpretation,Scene modelling,Spherical imaging","journal":"Computer Vision and Image Understanding","issn":"10773142","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Kim, Hilton - 2015 - Block world reconstruction from spherical stereo image pairs.pdf:pdf"}}
{"bib_id":"Schonbein:IROS:2014","title":"Omnidirectional 3D reconstruction in augmented Manhattan worlds","author":"Schönbein, M. and Geiger, A.","meta_info":{"year":"2014","issn":"21530866","isbn":"9781479969340","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Schönbein, Geiger - 2014 - Omnidirectional 3D reconstruction in augmented Manhattan worlds.pdf:pdf","booktitle":"IEEE International Conference on Intelligent Robots and Systems"}}
{"bib_id":"Kim:IJCV:2013","title":"3D Scene Reconstruction from Multiple Spherical Stereo Pairs","author":"Kim, H. and Hilton, A.","meta_info":{"year":"2013","volume":"104","pages":"94--116","number":"1","keywords":"3D reconstruction,3D registration and mesh integration,Disparity estimation,Environment modelling","journal":"International Journal of Computer Vision","issn":"0920-5691","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Kim, Hilton - 2013 - 3D scene reconstruction from multiple spherical stereo pairs.pdf:pdf"}}
{"bib_id":"Pagani:VAST:2011","title":"Dense 3D Point Cloud Generation from Multiple High-resolution Spherical Images","author":"Pagani, A. and Gava, C. and Cui, Y. and Krolla, B. and Hengen, J.-M. and Stricker, D.","meta_info":{"acmid":"2384499","numpages":"8","pages":"17--24","isbn":"978-3-905674-34-7","year":"2011","booktitle":"International Conference on Virtual Reality, Archaeology and Cultural Heritage"}}
{"bib_id":"Caruso:IROS:2015","title":"Large-scale direct SLAM for omnidirectional cameras","author":"Caruso, D. and Engel, J. and Cremers, D.","meta_info":{"year":"2015","pages":"141--148","keywords":"Cameras,Computational modeling,Lenses,Nonlinear distortion,Simultaneous localization and mapping,Three-dimensional displays","journal":"International Conference on Intelligent Robots and Systems","issn":"21530866","isbn":"9781479999941","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Caruso, Engel, Cremers - 2015 - Large-scale direct SLAM for omnidirectional cameras.pdf:pdf"}}
{"bib_id":"Wang:ACCV:2018","title":"Self-supervised Learning of Depth and Camera Motion from 360$^∘$ Videos","author":"Wang, F.-E. and Hu, H.-N. and Cheng, H.-T. and Lin, J.-T. and Yang, S.-T. and Shih, M.-L. and Chu, H.-K. and Sun, M.","meta_info":{"year":"2018","volume":"11364","publisher":"Asian Conference on Computer Vision","pages":"53--68","isbn":"978-3-030-20869-1","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Wang et al. - 2019 - Self-supervised Learning of Depth and Camera Motion from 360° Videos.pdf:pdf"}}
{"bib_id":"Sitzmann:arxiv2021","title":"Light Field Networks: Neural Scene Representations with Single-Evaluation Rendering","author":"Sitzmann, Vincent and Rezchikov, Semon and Freeman, William T and Tenenbaum, Joshua B and Durand, Fredo","meta_info":{"year":"2021","eprint":"2106.02634","arxivid":"2106.02634","archiveprefix":"arXiv"}}
{"bib_id":"Sumikura:etal:ICM2019","title":"Openvslam: a versatile visual slam framework","author":"Sumikura, Shinya and Shibuya, Mikiya and Sakurada, Ken","meta_info":{"year":"2019","pages":"2292--2295","booktitle":"Proceedings of the 27th ACM International Conference on Multimedia"}}
{"bib_id":"Dai:VR:2019","title":"Freely Explore the Scene with 360° Field of View","author":"Dai, F. and Zhu, C. and Ma, Y. and Cao, J. and Zhao, Q. and Zhang, Y.","meta_info":{"year":"2019","pages":"888--889","isbn":"978-1-7281-1377-7","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Dai et al. - 2019 - Freely Explore the Scene with 360°Field of View.pdf:pdf","booktitle":"IEEE Conference on Virtual Reality and 3D User Interfaces"}}
{"bib_id":"Gava:ICIP:2018","title":"Dense Scene Reconstruction from Spherical Light Fields","author":"Gava, C. C. and Stricker, D. and Yokota, S.","meta_info":{"year":"2018","pages":"4178--4182","mendeley-groups":"Jung et al.\/References\/SfM and Omni-SfM","isbn":"978-1-4799-7061-2","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Gava, Stricker, Yokota - 2018 - Dense Scene Reconstruction from Spherical Light Fields(3).pdf:pdf","booktitle":"IEEE International Conference on Image Processing"}}
{"bib_id":"Barron:etal:CVPR2015","title":"Fast bilateral-space stereo for synthetic defocus","author":"Barron, Jonathan T and Adams, Andrew and Shih, YiChang and Hernández, Carlos","meta_info":{"year":"2015","pages":"4466--4474","booktitle":"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"}}
{"bib_id":"Levoy:Computer:2006","title":"Light fields and computational imaging","author":"Levoy, Marc","meta_info":{"publisher":"IEEE","year":"2006","pages":"46--55","number":"8","volume":"39","journal":"Computer"}}
{"bib_id":"Guan:TIP:2017","title":"Structure-From-Motion in Spherical Video Using the von Mises-Fisher Distribution","author":"Guan, H. and Smith, W. A. P.","meta_info":{"year":"2017","volume":"26","pages":"711--723","number":"2","mendeley-groups":"Jung et al.\/References","journal":"IEEE Transactions on Image Processing","issn":"1057-7149","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Guan, Smith - 2017 - Structure-From-Motion in Spherical Video Using the von Mises-Fisher Distribution.pdf:pdf"}}
{"bib_id":"Huang:VR:2017","title":"6-DoF VR videos with a single 360-camera","author":"J. Huang and Z. Chen and D. Ceylan and H. Jin","meta_info":{"year":"2017","pages":"37--44","mendeley-groups":"Jung et al.\/References\/SfM and Omni-SfM","keywords":"I210 [Artificial Intelli-gence],I37 [Computer Graphics],Index Terms,Scene Analysis— Motion","isbn":"978-1-5090-6647-6","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Jingwei Huang et al. - 2017 - 6-DOF VR videos with a single 360-camera.pdf:pdf;:home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Jingwei Huang et al. - 2017 - 6-DOF VR videos with a single 360-camera(2).pdf:pdf","booktitle":"IEEE Virtual Reality"}}
{"bib_id":"Matas:CVIU:2000","title":"Robust Detection of Lines Using the Progressive Probabilistic Hough Transform","author":"J. Matas and C. Galambos and J. Kittler","meta_info":{"abstract":"In the paper we present the progressive probabilistic Hough transform (PPHT). Unlike the probabilistic HT, where the standard HT is performed on a preselected fraction of input points, the PPHT minimizes the amount of computation needed to detect lines by exploiting the difference in the fraction of votes needed to reliably detect lines with different numbers of supporting points. The fraction of points used for voting need not be specified ad hoc or using a priori knowledge, as in the probabilistic HT; it is a function of the inherent complexity of data. The algorithm is ideally suited for real-time applications with a fixed amount of available processing time, since voting and line detection are interleaved. The most salient features are likely to be detected first. While retaining its robustness, experiments show that the PPHT has, in many circumstances, advantages over the standard HT.","issn":"1077-3142","year":"2000","pages":"119 - 137","number":"1","volume":"78","journal":"Computer Vision and Image Understanding"}}
{"bib_id":"Im:ECCV:2016","title":"All-Around Depth from Small Motion with a Spherical Panoramic Camera","author":"Im, S.\nand Ha, H.\nand Rameau, F.\nand Jeon, H.-G.\nand Choe, G.\nand Kweon, I. S.","meta_info":{"isbn":"978-3-319-46487-9","pages":"156--172","year":"2016","booktitle":"European Conference on Computer Vision"}}
{"bib_id":"Pagani:ICCV:2011","title":"Structure from Motion using full spherical panoramic cameras","author":"Pagani, A. and Stricker, D.","meta_info":{"year":"2011","pages":"375--382","mendeley-groups":"Jung et al.\/References\/SfM and Omni-SfM","isbn":"978-1-4673-0063-6","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Pagani, Stricker - 2011 - Structure from Motion using full spherical panoramic cameras.pdf:pdf","booktitle":"IEEE International Conference on Computer Vision Workshops"}}
{"bib_id":"Krolla:BMVC:2014","title":"Spherical light fields","author":"Krolla, B. and Diebold, M. and Goldlücke, B. and Stricker, D.","meta_info":{"year":"2014","number":"67.1-67.12","journal":"British Machine Vision Conference","file":":tmp\/krolla2014spherical.pdf:pdf"}}
{"bib_id":"Kim:IVMSPW:2013","title":"Planar urban scene reconstruction from spherical images using facade alignment","author":"Kim, H. and Hilton, A.","meta_info":{"year":"2013","publisher":"IEEE","pages":"1--4","keywords":"3D Reconstruction,Spherical Imaging","journal":"IEEE Image, Video, and Multidimensional Signal Processing Workshop","isbn":"9781467358583","file":":tmp\/kim.pdf:pdf"}}
{"bib_id":"Kim:ECCV:2010","title":"3D Modelling of Static Environments Using Multiple Spherical Stereo","author":"Kim, H.\nand Hilton, A.","meta_info":{"isbn":"978-3-642-35740-4","pages":"169--183","year":"2010","booktitle":"European Conference on Computer Vision","editor":"Kutulakos, Kiriakos N."}}
{"bib_id":"Torii:ICCV:2009","title":"From Google Street View to 3D city models","author":"A. Torii and M. Havlena and T. Pajdla","meta_info":{"pages":"2188-2195","number":"","volume":"","year":"2009","booktitle":"International Conference on Computer Vision Workshops"}}
{"bib_id":"Bagnato:JMIV:2011","title":"A Variational Framework for Structure from Motion in Omnidirectional Image Sequences","author":"Bagnato, L.\nand Frossard, P.\nand Vandergheynst, P.","meta_info":{"issn":"1573-7683","pages":"182--193","number":"3","volume":"41","day":"01","year":"2011","journal":"Journal of Mathematical Imaging and Vision"}}
{"bib_id":"Micusik:CVPR:2009","title":"Piecewise planar city 3D modeling from street view panoramic sequences","author":"B. Micusik and J. Kosecka","meta_info":{"issn":"1063-6919","keywords":"image fusion;image segmentation;solid modelling;stereo image processing;piecewise planar city 3D modeling;street view panoramic sequences;city environment;repetitive structures;lighting changes;3D modeling pipelines;3D city models;image segmentation;scene orientation;piecewise planar structure;panoramic street view sequences;multiview stereo method;piecewise planarity constraint;depth fusion;urban environment;volumetric based fusion;viewpoint based fusion;space voxelization;kd-tree representation;Cities and towns;Image reconstruction;Surface reconstruction;Pipelines;Layout;Cameras;Surface texture;Image segmentation;Earth;Laser modes","pages":"2906-2912","number":"","volume":"","year":"2009","booktitle":"IEEE Conference on Computer Vision and Pattern Recognition"}}
{"bib_id":"Bagnato:ICIP:2009","title":"Optical flow and depth from motion for omnidirectional images using a TV-L1 variational framework on graphs","author":"L. Bagnato and P. Frossard and P. Vandergheynst","meta_info":{"issn":"2381-8549","pages":"1469-1472","year":"2009","booktitle":"IEEE International Conference on Image Processing"}}
{"bib_id":"overbeck:tog:2018","title":"A system for acquiring, processing, and rendering panoramic light field stills for virtual reality","author":"Overbeck, R. S. and Erickson, D. and Evangelakos, D. and Pharr, M. and Debevec, P.","meta_info":{"publisher":"ACM New York, NY, USA","year":"2018","pages":"1--15","number":"6","volume":"37","journal":"ACM Transactions on Graphics (TOG)"}}
{"bib_id":"Pintore:CAG:2018","title":"Recovering 3D existing-conditions of indoor structures from spherical images","author":"G. Pintore and R. Pintus and F. Ganovelli and R. Scopigno and E. Gobbetti","meta_info":{"abstract":"We present a vision-based approach to automatically recover the 3D existing-conditions information of an indoor structure, starting from a small set of overlapping spherical images. The recovered 3D model includes the as-built 3D room layout with the position of important functional elements located on room boundaries. We first recover the underlying 3D structure as interconnected rooms bounded by walls. This is done by combining geometric reasoning under an Augmented Manhattan World model and Structure-from-Motion. Then, we create, from the original registered spherical images, 2D rectified and metrically scaled images of the room boundaries. Using those undistorted images and the associated 3D data, we automatically detect the 3D position and shape of relevant wall-, floor-, and ceiling-mounted objects, such as electric outlets, light switches, air-vents and light points. As a result, our system is able to quickly and automatically draft an as-built model coupled with its existing conditions using only commodity mobile devices. We demonstrate the effectiveness and performance of our approach on real-world indoor scenes and publicly available datasets.","keywords":"Panoramic scene understanding, Omnidirectional images, Mobile capture, Indoor reconstruction, As-built models","issn":"0097-8493","year":"2018","pages":"16 - 29","volume":"77","journal":"Computers and Graphics"}}
{"bib_id":"Fangi:ISPRS:2018","title":"Improving spherical photogrammetry using 360$^∘$ OMNI-Cameras: Use cases and new applications","author":"Fangi, G. and Pierdicca, R. and Sturari, M. and Malinverni, E. S.","meta_info":{"year":"2018","volume":"42","pages":"331--337","number":"2","journal":"International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences","issn":"16821750"}}
{"bib_id":"Pintore:PCCGA:2018","title":"Recovering 3D Indoor Floor Plans by Exploiting Low-cost Spherical Photography","author":"Pintore, G. and Ganovelli, F. and Pintus, R. and Scopigno, R. and Gobbetti, E.","meta_info":{"acmid":"3308518","numpages":"4","pages":"45--48","isbn":"978-3-03868-073-4","year":"2018","series":"PG '18","booktitle":"Pacific Conference on Computer Graphics and Applications"}}
{"bib_id":"Cabral:CVPR:2014","title":"Piecewise Planar and Compact Floorplan Reconstruction from Images","author":"R. Cabral and Y. Furukawa","meta_info":{"issn":"1063-6919","keywords":"architecture;image classification;image motion analysis;image reconstruction;image texture;stereo image processing;piecewise planar floorplan reconstruction;free-viewpoint visualization;high quality texture-mapped models;image-based floorplan reconstruction;structure from motion;multiview stereo;regularization technique;piecewise planarity;structure classification technique;pixel classification;Three-dimensional displays;Image reconstruction;Visualization;Solid modeling;Cameras;Periodic structures;Semiconductor device modeling;indoor;3D reconstruction;multi-view stereo","pages":"628-635","number":"","volume":"","year":"2014","booktitle":"IEEE Conference on Computer Vision and Pattern Recognition"}}
{"bib_id":"Lee:TPAMI:2020","title":"SpherePHD: Applying CNNs on 360° Images with Non-Euclidean Spherical PolyHeDron Representation","author":"Y. Lee and J. Jeong and J. Yun and W. Cho and K.-J. Yoon","meta_info":{"pages":"1-1","number":"","volume":"","year":"2020","journal":"IEEE Transactions on Pattern Analysis and Machine Intelligence"}}
{"bib_id":"Fernandez-Labrador:LRA:2020","title":"Corners for Layout: End-to-End Layout Recovery from 360 Images","author":"C. Fernandez-Labrador and J. M. Facil and A. Perez-Yus and C. Demonceaux and J. Civera and J. Guerrero","meta_info":{"issn":"2377-3774","keywords":"Omnidirectional Vision;Semantic Scene Understanding","pages":"1-1","number":"","volume":"","year":"2020","journal":"IEEE Robotics and Automation Letters"}}
{"bib_id":"Yang:CVPR:2019","title":"DuLa-Net: A Dual-Projection Network for Estimating Room Layouts From a Single RGB Panorama","author":"Yang, S.-T. and Wang, F.-E. and Peng, C.-H. and Wonka, P. and Sun, M. and Chu, H-K.","meta_info":{"year":"2019","pages":"3363--3372","booktitle":"IEEE Conference on Computer Vision and Pattern Recognition"}}
{"bib_id":"Eder:3DV:2019","title":"Pano Popups: Indoor 3D Reconstruction with a Plane-Aware Network","author":"Eder, M. and Moulon, P. and Guan, L.","meta_info":{"year":"2019","publisher":"IEEE","pages":"76--84","isbn":"978-1-7281-3131-3","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Eder, Hill, Moulon - Unknown - Pano Popups Indoor 3D Reconstruction with a Plane-Aware Network.pdf:pdf","booktitle":"2019 International Conference on 3D Vision (3DV)"}}
{"bib_id":"Yang:CVPR:2016","title":"Efficient 3D Room Shape Recovery from a Single Panorama","author":"Yang, H. and Zhang, H.","meta_info":{"year":"2016","volume":"2016-Decem","pages":"5422--5430","mendeley-groups":"Jung et al.\/References\/Monocular depth","issn":"10636919","isbn":"978-1-4673-8851-1","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Yang, Zhang - 2016 - Efficient 3D Room Shape Recovery from a Single Panorama.pdf:pdf","booktitle":"IEEE Conference on Computer Vision and Pattern Recognition"}}
{"bib_id":"Zou:CVPR:2018","title":"LayoutNet: Reconstructing the 3D Room Layout from a Single RGB Image","author":"Zou, C. and Colburn, A. and Shan, Q. and Hoiem, D.","meta_info":{"year":"2018","pages":"2051--2059","mendeley-groups":"Jung et al.\/References\/Monocular depth","isbn":"978-1-5386-6420-9","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Zou et al. - 2018 - LayoutNet Reconstructing the 3D Room Layout from a Single RGB Image.pdf:pdf","booktitle":"IEEE\/CVF Conference on Computer Vision and Pattern Recognition"}}
{"bib_id":"Zioulis:ECCV:2018","title":"OmniDepth: Dense Depth Estimation for Indoors Spherical Panoramas","author":"Zioulis, N. and Karakottas, A. and Zarpalas, D. and Daras, P.","meta_info":{"year":"2018","pages":"453--471","mendeley-groups":"Jung et al.\/References\/SfM and Omni-SfM","keywords":"360 o,depth estimation,learning with vir-,omnidirectional media,scene,spherical panorama,synthetic dataset,understanding","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Zioulis et al. - 2018 - OmniDepth Dense Depth Estimation for Indoors Spherical Panoramas.pdf:pdf","booktitle":"European Conference on Computer Vision"}}
{"bib_id":"Silveira:ICIP:2018","title":"Indoor Depth Estimation from Single Spherical Images","author":"da Silveira, T. L. T. and Dalaqua, L. P. and Jung, C. R.","meta_info":{"year":"2018","pages":"2935--2939","mendeley-groups":"Jung et al.\/Published","isbn":"978-1-4799-7061-2","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Da Silveira, Dalaqua, Jung - 2018 - Indoor Depth Estimation from Single Spherical Images.pdf:pdf;:home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Da Silveira, Dalaqua, Jung - 2018 - Indoor Depth Estimation from Single Spherical Images(2).pdf:pdf","booktitle":"IEEE International Conference on Image Processing"}}
{"bib_id":"Yang:CVPR:2018","title":"Automatic 3D Indoor Scene Modeling from Single Panorama","author":"Yang, Y. and Liu, R. and Kang, S. B.","meta_info":{"year":"2018","publisher":"IEEE","pages":"5430","mendeley-groups":"Jung et al.\/References\/Monocular depth","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Yang, Liu, Kang - 2018 - Automatic 3D Indoor Scene Modeling from Single Panorama.pdf:pdf","booktitle":"Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition"}}
{"bib_id":"Moreau:SITIS:2012","title":"3D Reconstruction of Urban Environments Based on Fisheye Stereovision","author":"Moreau, J. and Ambellouis, S. and Ruichek, Y.","meta_info":{"year":"2012","pages":"36--41","mendeley-groups":"Jung et al.\/References\/SfM and Omni-SfM","keywords":"3D,Epipolar,Fisheye,Spherical,Stereovision,Urban","isbn":"978-1-4673-5152-2","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Moreau, Ambellouis, Ruiche - 2012 - 3D reconstruction of urban environments based on fisheye stereovision.pdf:pdf","booktitle":"IEEE International Conference on Signal Image Technology and Internet Based Systems"}}
{"bib_id":"Shan:ACCESS:2018","title":"Descriptor Matching for a Discrete Spherical Image With a Convolutional Neural Network","author":"Shan, Y. and Li, S.","meta_info":{"year":"2018","volume":"6","pages":"20748--20755","mendeley-groups":"Jung et al.\/References\/Keypoints & descriptors","journal":"IEEE Access","issn":"2169-3536","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Shan, Li - 2018 - Descriptor Matching for a Discrete Spherical Image With a Convolutional Neural Network.pdf:pdf"}}
{"bib_id":"Fernandez-Labrador:arXiv:2018","title":"PanoRoom: From the Sphere to the 3D Layout","author":"Fernandez-Labrador, C. and Facil, J. M. and Perez-Yus, A. and Demonceaux, c. and Guerrero, J. J.","meta_info":{"year":"2018","pages":"1--6","file":":tmp\/1808.09879.pdf:pdf","eprint":"1808.09879","arxivid":"1808.09879","archiveprefix":"arXiv","abstract":"We propose a novel FCN able to work with omnidirectional images that outputs accurate probability maps representing the main structure of indoor scenes, which is able to generalize on different data. Our approach handles occlusions and recovers complex shaped rooms more faithful to the actual shape of the real scenes. We outperform the state of the art not only in accuracy of the 3D models but also in speed."}}
{"bib_id":"Tateno:ECCV:2018","title":"Distortion-Aware Convolutional Filters for Dense Prediction in Panoramic Images","author":"Tateno, K. and Navab, N. and Tombari, F.","meta_info":{"year":"2018","pages":"732--750","journal":"European Conference on Computer Vision","issn":"16113349","isbn":"9783030012694","file":":tmp\/Keisuke_Tateno_Distortion-Aware_Convolutional_Filters_ECCV_2018_paper.pdf:pdf"}}
{"bib_id":"Fernandez-Labrador:LRA:2018","title":"Layouts from panoramic images with geometry and deep learning","author":"Fernandez-Labrador, C. and Perez-Yus, A. and Lopez-Nicolas, G. and Guerrero, J. J.","meta_info":{"year":"2018","volume":"3","publisher":"IEEE","pages":"3153--3160","number":"4","keywords":"Omnidirectional vision,semantic scene understanding","journal":"IEEE Robotics and Automation Letters","issn":"23773766","file":":tmp\/hey.pdf:pdf"}}
{"bib_id":"Payen:ECCV:2018","title":"Eliminating the blind spot: Adapting 3D object detection and monocular depth estimation to 360$^∘$ Panoramic Imagery","author":"G. P. de La Garanderie and A. Abarghouei, A. and Breckon, T. P.","meta_info":{"year":"2018","pages":"812--830","keywords":"360 depth,Monocular 3D object detection,Monocular depth,Object detection,Panoramic depth,Panoramic imagery,Style transfer","journal":"European Conference on Computer Vision","issn":"16113349"}}
{"bib_id":"Sun:CVPR:2019","title":"HorizonNet: Learning Room Layout with 1D Representation and Pano Stretch Data Augmentation","author":"Sun, C. and Hsiao, C.-W. and Sun, M. and Chen, H.-T.","meta_info":{"year":"2019","booktitle":"Conference on Computer Vision and Pattern Recognition (CVPR)","pages":"1047--1056","file":":tmp\/Sun_HorizonNet_Learning_Room_Layout_With_1D_Representation_and_Pano_Stretch_CVPR_2019_paper.pdf:pdf"}}
{"bib_id":"Zhang:ECCV:2014","title":"PanoContext: A whole-room 3D context model for panoramic scene understanding","author":"Zhang, Y. and Song, S. and Tan, P. and Xiao, J.","meta_info":{"year":"2014","mendeley-groups":"Jung et al.\/References\/SfM and Omni-SfM","issn":"16113349","isbn":"9783319105987","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Zhang et al. - 2014 - PanoContext A whole-room 3D context model for panoramic scene understanding.pdf:pdf","booktitle":"European Conference on Computer Vision"}}
{"bib_id":"Zioulis:3DV:2019","title":"Spherical View Synthesis for Self-Supervised 360$^∘$ Depth Estimation","author":"Zioulis, N. and Karakottas, A. and Zarpalas, D. and Alvarez, F. and Daras, P.","meta_info":{"year":"2019","pages":"690--699","journal":"Conference on 3D Vision","isbn":"9781728131313"}}
{"bib_id":"Karakottas:3DV:2019","title":"360$^∘$ Surface Regression with a Hyper-Sphere Loss","author":"A. Karakottas and N. Zioulis and S. Samaras and D. Ataloglou and V. Gkitsas and D. Zarpalas and P. Daras","meta_info":{"pages":"258-268","number":"","volume":"","year":"2019","booktitle":"2019 International Conference on 3D Vision (3DV)"}}
{"bib_id":"Lowe:IJCV:2004","title":"Distinctive Image Features from Scale-Invariant Keypoints","author":"D. G. Lowe","meta_info":{"journal":"International Journal of Computer Vision","pages":"91--110","number":"2","volume":"60","publisher":"Springer Science and Business Media LLC","year":"2004"}}
{"bib_id":"Eigen:NIPS:2014","title":"Depth Map Prediction from a Single Image Using a Multi-Scale Deep Network","author":"Eigen, D. and Puhrsch, C. and Fergus, R.","meta_info":{"pages":"2366--2374","booktitle":"International Conference on Neural Information Processing Systems","year":"2014"}}
{"bib_id":"Xu:WACV:2017","title":"Pano2CAD: Room Layout from a Single Panorama Image","author":"J. Xu and B. Stenger and T. Kerola and T. Tung","meta_info":{"keywords":"computational geometry;inference mechanisms;object detection;pose estimation;visual databases;Pano2CAD;room layout;panorama image;room geometry estimation;3D pose estimation;Manhattan World geometry;inference problem;wall orientation estimation;wall position estimation;object position estimation;2D detection;3D object pose estimation;quantitative analysis;synthetically generated 3D rooms;hand-labeled images;public SUN360 dataset;surface normal estimation;object orientation estimation;Three-dimensional displays;Solid modeling;Layout;Geometry;Estimation;Context;Cameras","pages":"354-362","number":"","volume":"","year":"2017","booktitle":"IEEE Winter Conference on Applications of Computer Vision"}}
{"bib_id":"Jia:ICRA:2015","title":"Estimating structure of indoor scene from a single full-view image","author":"H. Jia and S. Li","meta_info":{"issn":"1050-4729","keywords":"geometry;image processing;structure estimation;single full-view image;perspective images;hemispherical omnidirectional images;visually open boundary condition;open geometry;visually close boundary condition;close geometry;indoor scene understanding;Geometry;Cameras;Layout;Image segmentation;Floors;Estimation;Image edge detection","pages":"4851-4858","number":"","volume":"","year":"2015","booktitle":"IEEE International Conference on Robotics and Automation"}}
{"bib_id":"Pintore:WACV:2016","title":"Omnidirectional image capture on mobile devices for fast automatic generation of 2.5D indoor maps","author":"G. Pintore and V. Garro and F. Ganovelli and E. Gobbetti and M. Agus","meta_info":{"keywords":"image reconstruction;indoor environment;iterative methods;optimisation;transforms;omnidirectional image capture;mobile devices;fast automatic generation;2.5D indoor maps;lightweight automatic method;2.5D multiroom indoor environments;single omnidirectional image per room analysis;3D clues;floor plan reconstruction;specialized spatial transform;catadioptric theory;room structure;virtual projection;parametric model;global optimization;Levenberg-Marquardt iterations;multiroom indoor scenes;Three-dimensional displays;Mobile handsets;Transforms;Image reconstruction;Shape;Indoor environments;Measurement","pages":"1-9","number":"","volume":"","year":"2016","booktitle":"IEEE Winter Conference on Applications of Computer Vision"}}
{"bib_id":"Shum:CVPR:1998","title":"Interactive construction of 3D models from panoramic mosaics","author":" H.-Y. Shum and M. Han and R. Szeliski","meta_info":{"issn":"1063-6919","keywords":"image reconstruction;interactive modeling;3D models;panoramic image mosaics;transformation matrix;QR factorization;image reconstruction;Cameras;Layout;Buildings;Solid modeling;Image reconstruction;Computer vision;Calibration;Lenses;Robot vision systems;Wire","pages":"427-433","number":"","volume":"","year":"1998","booktitle":"IEEE Conference on Computer Vision and Pattern Recognition"}}
{"bib_id":"Serrano:TVCG:2019","title":"Motion parallax for 360$^∘$ RGBD video","author":"Serrano, A. and Kim, I. and Chen, Z. and DIVerdi, S. and Gutierrez, D. and Hertzmann, A. and Masia, B.","meta_info":{"year":"2019","volume":"25","pages":"1817--1827","number":"5","keywords":"Immersive environments,Virtual Reality video","journal":"IEEE Transactions on Visualization and Computer Graphics","issn":"19410506"}}
{"bib_id":"Fukano:ICPR:2016","title":"Room reconstruction from a single spherical image by higher-order energy minimization","author":"K. Fukano and Y. Mochizuki and S. Iizuka and E. Simo-Serra and A. Sugimoto and H. Ishikawa","meta_info":{"keywords":"edge detection;graph theory;image classification;image reconstruction;single spherical image;structural plane identification;structural plane reconstruction;room structure reconstruction;line segment detection;image classification;higher-order energy minimization problem;graph cuts;segment identification;ceiling;floor;walls;Image segmentation;Image reconstruction;Three-dimensional displays;Labeling;Minimization;Periodic structures;Motion segmentation","pages":"1768-1773","number":"","volume":"","year":"2016","booktitle":"International Conference on Pattern Recognition"}}
{"bib_id":"Yang:CADG:2013","title":"Indoor Structure Understanding from Single 360 Cylindrical Panoramic Image","author":"H. Yang and H. Zhang","meta_info":{"issn":"null","keywords":"object recognition;indoor structure understanding;cylindrical panoramic image;room structure recognition;perspective projected subimages;holistic parameterization framework;candidate evaluation;linear scoring function;normal indoor image data set;Layout;Computer vision;Conferences;Prediction algorithms;Periodic structures;Educational institutions;Partitioning algorithms;360 cylindrical panorama;indoor structure recognition","pages":"421-422","number":"","volume":"","year":"2013","booktitle":"International Conference on Computer-Aided Design and Computer Graphics"}}
{"bib_id":"Yang:VRCAI:2014","title":"Modeling Room Structure from Indoor Panorama","author":"Yang, H. and Zhang, H.","meta_info":{"keywords":"3D modeling, indoor structure, panorama, reconstruction","acmid":"2670485","numpages":"9","pages":"47--55","isbn":"978-1-4503-3254-5","year":"2014","booktitle":"ACM SIGGRAPH International Conference on Virtual-Reality Continuum and Its Applications in Industry"}}
{"bib_id":"Adarve:LRA:2017","title":"Spherepix: A Data Structure for Spherical Image Processing","author":"J. D. Adarve and R. Mahony","meta_info":{"year":"2017","volume":"2","pages":"483--490","number":"2","journal":"IEEE Robotics and Automation Letters","issn":"2377-3766"}}
{"bib_id":"brodsky:ijcv:1998","title":"Directions of motion fields are hardly ever ambiguous","author":"Brodsky, T. and Fermüller, C. and Aloimonos, Y.","meta_info":{"publisher":"Springer","year":"1998","pages":"5--24","number":"1","volume":"26","journal":"International Journal of Computer Vision"}}
{"bib_id":"Sun:SPL:2017","title":"Weighted-to-Spherically-Uniform Quality Evaluation for Omnidirectional Video","author":"Sun, Y. and Lu, A. and Yu, L.","meta_info":{"year":"2017","volume":"24","pages":"1--1","number":"9","keywords":"Objective quality evaluation,omnidirectional video,projection format","journal":"IEEE Signal Processing Letters","issn":"1070-9908","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Sun, Lu, Yu - 2017 - Weighted-to-Spherically-Uniform Quality Evaluation for Omnidirectional Video.pdf:pdf","abstract":"Omnidirectional video records a scene in all directions around one central position. It allows users to select viewing content freely in all directions. Assuming that viewing directions are uniformly distributed, the isotropic observation space can be regarded as a sphere. Omnidirectional video is commonly represented by different projection formats with one or multiple planes. To measure objective quality of omnidirectional video in observation space more accurately, a weighted-to-spherically-uniform quality evaluation method is proposed in this letter. The error of each pixel on projection planes is multiplied by a weight to ensure the equivalent spherical area in observation space, in which pixels with equal mapped spherical area have the same influence on distortion measurement. Our method makes the quality evaluation results more accurate and reliable since it avoids error propagation caused by the conversion from resampling representation space to observation space. As an example of such quality evaluation method, weighted-to-spherically-uniform peak signal-to-noise ratio is described and its performance is experimentally analyzed."}}
{"bib_id":"Ferreira:CAG:2017","title":"Local Moebius transformations applied to omnidirectional images","author":"Ferreira, L. S. and Sacht, L. and Velho, L.","meta_info":{"year":"2017","volume":"68","publisher":"Elsevier Ltd","pages":"77--83","mendeley-groups":"Jung et al.\/References\/Filtering and retrieval","keywords":"Moebius transformations,Omnidirectional images,Pan,moebius transformations,omnidirectional images","journal":"Computers & Graphics","issn":"00978493","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Souto Ferreira, Sacht, Velho - 2017 - Local Moebius transformations applied to omnidirectional images.pdf:pdf"}}
{"bib_id":"Coors:ECCV:2018","title":"SphereNet: Learning spherical representations for detection and classification in omnidirectional images","author":"Coors, B. and Condurache, A. P. and Geiger, A.","meta_info":{"year":"2018","pages":"525--541","journal":"European Conference on Computer Vision","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Coors, Condurache, Geiger - 2018 - SphereNet Learning spherical representations for detection and classification in omnidirectional imag.pdf:pdf","abstract":"Omnidirectional cameras offer great benefits over classical cameras wherever a wide field of view is essential, such as in virtual reality applications or in autonomous robots. Unfortunately, standard convolutional neural networks are not well suited for this scenario as the natural projection surface is a sphere which cannot be unwrapped to a plane without introducing significant distortions, particularly in the polar regions. In this work, we present SphereNet, a novel deep learning framework which encodes invariance against such distortions explicitly into convolutional neural networks. Towards this goal, SphereNet adapts the sampling locations of the convolutional filters, effectively reversing distortions, and wraps the filters around the sphere. By building on regular convolutions, SphereNet enables the transfer of existing perspective convolutional neural network models to the omnidirectional case. We demonstrate the effectiveness of our method on the tasks of image classification and object detection, exploiting two newly created semi-synthetic and real-world omnidirectional datasets."}}
{"bib_id":"Jung:VR:2019","title":"Deep360Up: A Deep Learning-Based Approach for Automatic VR Image Upright Adjustment","author":"Jung, R. and Lee, A. Ss J. and Ashtari, A. and Bazin, J.-C.","meta_info":{"year":"2019","pages":"1--8","keywords":"computer graphics,computing methodologies,deep learning,image manipulation,image processing,index terms,upright adjustment,vr content","isbn":"978-1-7281-1377-7","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Jung et al. - 2019 - Deep360Up A Deep Learning-Based Approach for Automatic VR Image Upright Adjustment.pdf:pdf","booktitle":"IEEE Conference on Virtual Reality and 3D User Interfaces"}}
{"bib_id":"Deng:WCICA:2008","title":"Automatic spherical panorama generation with two fisheye images","author":"Deng, X. and Wu, F. and Wu, Y. and Wan, C.","meta_info":{"year":"2008","mendeley-groups":"Jung et al.\/References\/Cameras and stitching","isbn":"9781424421145","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Deng et al. - 2008 - Automatic spherical panorama generation with two fisheye images.pdf:pdf","booktitle":"World Congress on Intelligent Control and Automation","abstract":"Panoramas are often used to enhance immersive experiences. Former panorama generation methods with fisheye images either require pre-calibrating the rotation of fisheye cameras with a calibration rig or manually selecting correspondences between images. In this paper, we propose an automatic spherical panorama generation method with two fisheye images. The method contains fisheye camera calibration and panorama stitching. In the calibration, the field of view and circular boundary are used to initialize the focal length, and line perspective constraint on sphere is used to refine the intrinsic parameters. In the panorama stitching, we use scale invariant feature transform (SIFT) features and RANSAC to get the fisheye correspondences. Then the correspondences are used to refine the estimation of rotation and intrinsic parameters. Finally, a spherical panorama is obtained by warping the fisheye images. In the panorama stitching process, no user input is needed. Experimental results on both simulated and real images show that our method is effective."}}
{"bib_id":"Huang:Wiley:2008","title":"Panoramic Imaging: Sensor-Line Cameras and Laser Range-Finders","author":"Huang, F. and Klette, R. and Scheibe, K.","meta_info":{"year":"2008","isbn":"9780470060650","publisher":"Wiley","file":":home\/trugillo\/.local\/share\/data\/Mendeley Ltd.\/Mendeley Desktop\/Downloaded\/Huang, Klette, Scheibe - 2008 - Panoramic Imaging Sensor-Line Cameras and Laser Range-Finders.pdf:pdf","booktitle":"Panoramic Imaging: Sensor-Line Cameras and Laser Range-Finders"}}
{"bib_id":"La-Garanderie:ECCV:2018","title":"Eliminating the Blind Spot: Adapting 3D Object Detection and Monocular Depth Estimation to 360$^∘$ Panoramic Imagery","author":"Payen de La Garanderie, Grégoire\nand Atapour Abarghouei, Amir\nand Breckon, Toby P.","meta_info":{"isbn":"978-3-030-01261-8","pages":"812--830","address":"Cham","publisher":"Springer International Publishing","year":"2018","booktitle":"Computer Vision -- ECCV 2018","editor":"Ferrari, Vittorio\nand Hebert, Martial\nand Sminchisescu, Cristian\nand Weiss, Yair"}}
{"bib_id":"pintore:EG:2020","title":"State-of-the-art in Automatic 3D Reconstruction of Structured Indoor Environments","author":"Pintore, G. and Mura, C. and Ganovelli, F. and Fuentes-Perez, L. and Pajarola, R. and Gobbetti, E.","meta_info":{"year":"2020","number":"2","volume":"39","journal":"Computer Graphics Forum"}}
{"bib_id":"Fatma:MMSP:2020","title":"Sphere Mapping for Feature Extraction\nfrom 360°fish-eye captures","author":"Fatma Hawary, Thomas Maugey, Christine Guillemot","meta_info":{"year":"2020","pages":"1--6","journal":"MMSP 2020 - 22 nd IEEE International Workshop\non Multimedia Signal Processing"}}
{"bib_id":"Pintore:ECCV:2020","title":"AtlantaNet: Inferring the 3D Indoor Layout from a Single 360 Image beyond the Manhattan World Assumption","author":"G. Pintore and M. Agus and E. Gobbetti","meta_info":{"abstract":" We introduce a novel end-to-end approach to predict a 3D room layout from a single panoramic image. Compared to recent state-of-the-art works, our method is not limited to Manhattan World environments, and can reconstruct rooms bounded by vertical walls that do not form right angles or are curved -- i.e., Atlanta World models. In our approach, we project the original gravity-aligned panoramic image on two horizontal planes, one above and one below the camera. This representation encodes all the information needed to recover the \\emphAtlanta World 3D bounding surfaces of the room in the form of a 2D room footprint on the floor plan and a room height. To predict the 3D layout, we propose an encoder-decoder neural network architecture, leveraging Recurrent Neural Networks (RNNs) to capture long-range geometric patterns, and exploiting a customized training strategy based on domain-specific knowledge. The experimental results demonstrate that our method outperforms state-of-the-art solutions in prediction accuracy, in particular in cases of complex wall layouts or curved wall footprints. ","year":"2020","booktitle":"European Conference on Computer Vision"}}
{"bib_id":"Wang:CVPR:2020","title":"BiFuse: Monocular 360 Depth Estimation via Bi-Projection Fusion","author":"Wang, Fu-En and Yeh, Yu-Hsuan and Sun, Min and Chiu, Wei-Chen and Tsai, Yi-Hsuan","meta_info":{"year":"2020","booktitle":"Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"}}
{"bib_id":"Leng:Access:2018","title":"Local Feature Descriptor for Image Matching: A Survey","author":"C. Leng and H. Zhang and B. Li and G. Cai and Z. Pei and L. He","meta_info":{"pages":"6424-6434","number":"","volume":"7","year":"2019","journal":"IEEE Access"}}
{"bib_id":"Rublee:ICCV:2011","title":"ORB: An Efficient Alternative to SIFT or SURF","author":"Rublee, E. and Rabaud, V. and Konolige, K. and Bradski, G.","meta_info":{"numpages":"8","pages":"2564–2571","booktitle":"International Conference on Computer Vision","isbn":"9781457711015","year":"2011"}}
{"bib_id":"DeTone:CVPR:2018","title":"SuperPoint: Self-Supervised Interest Point Detection and Description","author":"D. DeTone and T. Malisiewicz and A. Rabinovich","meta_info":{"pages":"337-33712","number":"","volume":"","year":"2018","booktitle":"2018 IEEE\/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"}}
{"bib_id":"Yi:ECCV:2016","title":"LIFT: Learned Invariant Feature Transform","author":"Yi, Kwang and Trulls, Eduard and Lepetit, Vincent and Fua, Pascal","meta_info":{"booktitle":"European Conference on Computer Vision","isbn":"978-3-319-46465-7","volume":"9910","pages":"467-483","year":"2016"}}
{"bib_id":"Kitamura:ICMA:2015","title":"Spherical FAST corner detector","author":"R. Kitamura and S. Li and I. Nakanishi","meta_info":{"pages":"2597-2602","number":"","volume":"","year":"2015","booktitle":"IEEE International Conference on Mechatronics and Automation"}}
{"bib_id":"James:book:1950","title":"The perception of the visual world.","author":"James J Gibson","meta_info":{"year":"1950","publisher":"Boston, Houghton Mifflin"}}
{"bib_id":"Fortun:CVIU:2015","title":"Optical flow modeling and computation: A survey","author":"D. Fortun and P. Bouthemy and C. Kervrann","meta_info":{"issn":"1077-3142","year":"2015","pages":"1 - 21","volume":"134","journal":"Computer Vision and Image Understanding"}}
{"bib_id":"Leutenegger:ICCV:2011","title":"BRISK: Binary Robust invariant scalable keypoints","author":"S. Leutenegger and M. Chli and R. Y. Siegwart","meta_info":{"pages":"2548-2555","number":"","volume":"","year":"2011","booktitle":"2011 International Conference on Computer Vision"}}
{"bib_id":"Defferrard:ICLR:2020","title":"DeepSphere: a graph-based spherical CNN","author":"M. Defferrard and M. Milani and F. Gusset and N. Perraudin","meta_info":{"year":"2020","booktitle":"International Conference on Learning Representations"}}
{"bib_id":"Zhao:2019:IPMI","title":"Spherical U-Net on Cortical Surfaces: Methods and Applications","author":"Zhao, F. and Xia, S. and Wu, Z. and Duan, D. and Wang, L. and Lin, W. and Gilmore, J. and Li, G.","meta_info":{"booktitle":"International Conference on Information Processing in Medical Imaging","year":"2019"}}
{"bib_id":"Bhowmik:2020:CVPR","title":"Reinforced Feature Points: Optimizing Feature Detection and Description for a High-Level Task","author":"Bhowmik, Aritra and Gumhold, Stefan and Rother, Carsten and Brachmann, Eric","meta_info":{"year":"2020","booktitle":"Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"}}
{"bib_id":"Guan:ISVC:2013","title":"Corner Detection in Spherical Images via the Accelerated Segment Test on a Geodesic Grid","author":"Guan, H.\nand Smith, W. A. P.\nand Ren, P.","meta_info":{"pages":"407--415","year":"2013","booktitle":"International Symposium on Visual Computing"}}
{"bib_id":"Rosten:EECV:2006","title":"Machine Learning for High-Speed Corner Detection","author":"Rosten, E.\nand Drummond, T.","meta_info":{"isbn":"978-3-540-33833-8","pages":"430--443","year":"2006","booktitle":"European Conference on Computer Vision"}}
{"bib_id":"radgui:CVIU:2011","title":"Optical flow estimation from multichannel spherical image decomposition","author":"Radgui, A. and Demonceaux, C. and Mouaddib, E. M. and Rziza, M. and Aboutajdine, D.","meta_info":{"hal_version":"v1","hal_id":"hal-00637423","pdf":"https:\/\/hal.archives-ouvertes.fr\/hal-00637423\/file\/CVIU_Radgui.pdf","keywords":"Omnidirectional images ; optical flow ; spherical wavelets","year":"2011","pages":"1263-1272","number":"9","volume":"115","publisher":"Elsevier","journal":"Computer Vision and Image Understanding"}}
{"bib_id":"Tosic:ESPC:2005","title":"Multiresolution motion estimation for omnidirectional images","author":"I. Tosic and I. Bogdanova and P. Frossard and P. Vandergheynst","meta_info":{"pages":"1-4","number":"","volume":"","year":"2005","booktitle":"European Signal Processing Conference"}}
{"bib_id":"Mochizuki:SIMPAR:2008","title":"Featureless Visual Navigation using Optical Flow of Omnidirectional Image Sequence","author":"Y. Mochizuki and A. Imiya","meta_info":{"year":"2008","pages":"307-318","booktitle":"IEEE International Conference on Simulation, Modeling, and Programming for Autonomous Robots"}}
{"bib_id":"Mochizuki:CAIP:2011","title":"Multiresolution Optical Flow Computation of Spherical Images","author":"Mochizuki, Y.\nand Imiya, A.","meta_info":{"isbn":"978-3-642-23678-5","abstract":"As an application of image analysis on Riemannian manifolds, we develop an accurate algorithm for the computation of optical flow of omni-directional images. To guarantee the accuracy and stability of image processing for spherical images, we introduce the Gaussian pyramid transform, that is, we develop variational optical flow computation with pyramid-transform-based multiresolution analysis for spherical images.","pages":"348--355","year":"2011","booktitle":"Computer Analysis of Images and Patterns"}}
{"bib_id":"Alibouch:SIVP:2014","title":"A phase-based framework for optical flow estimation on omnidirectional images","author":"Alibouch, B. and Radgui, A. and Demonceaux, C. and Rziza, M. and Aboutajdine, D.","meta_info":{"journal":"Signal, Image and Video Processing","volume":"10","pages":"1-8","year":"2014"}}
{"bib_id":"bhandari:Arxiv:2020","title":"Revisiting Optical Flow Estimation in 360 Videos","author":"Bhandari, Keshav and Zong, Ziliang and Yan, Yan","meta_info":{"doi":"10.1109\/ICPR48806.2021.9412035","pages":"8196-8203","number":"","volume":"","year":"2021","booktitle":"IEEE International Conference on Pattern Recognition"}}
{"bib_id":"Kim:ICASSP:2019","title":"E-CNN: Accurate Spherical Camera Rotation Estimation via Uniformization of Distorted Optical Flow Fields","author":"D. Kim and S. Pathak and A. Moro and R. Komatsu and A. Yamashita and H. Asama","meta_info":{"pages":"2232-2236","number":"","volume":"","year":"2019","booktitle":"IEEE International Conference on Acoustics, Speech and Signal Processing"}}
{"bib_id":"artizzu:ICPR:2021","title":"OmniFlowNet: a Perspective Neural Network Adaptation for Optical Flow Estimation in Omnidirectional Images","author":"Artizzu, C.-O. and Zhang, H. and Allibert, G. and Demonceaux, C.","meta_info":{"hal_version":"v1","hal_id":"hal-02968191","pdf":"https:\/\/hal.archives-ouvertes.fr\/hal-02968191\/file\/ICPR_2020_FINAL.pdf","year":"2021","booktitle":"International Conference on Pattern Recognition"}}
{"bib_id":"Conklin:ICA:1997","title":"Multi-resolution motion estimation","author":"G. J. Conklin and S. S. Hemami","meta_info":{"pages":"2873-2876 vol.4","number":"","volume":"4","year":"1997","booktitle":"IEEE International Conference on Acoustics, Speech, and Signal Processing"}}
{"bib_id":"Zach:PR:2007","title":"A Duality Based Approach for Realtime TV-L1 Optical Flow","author":"Zach, C.\nand Pock, T.\nand Bischof, H.","meta_info":{"isbn":"978-3-540-74936-3","pages":"214--223","year":"2007","booktitle":"Pattern Recognition"}}
{"bib_id":"Schubert:IV:2019","title":"Circular convolutional neural networks for panoramic images and laser data","author":"Schubert, Stefan and Neubert, Peer and Pöschmann, Johannes and Protzel, Peter","meta_info":{"organization":"IEEE","year":"2019","pages":"653--660","booktitle":"2019 IEEE Intelligent Vehicles Symposium (IV)"}}
{"bib_id":"Wang:ICRA:2018","title":"Omnidirectional CNN for visual place recognition and navigation","author":"Wang, Tsun-Hsuan and Huang, Hung-Jui and Lin, Juan-Ting and Hu, Chan-Wei and Zeng, Kuo-Hao and Sun, Min","meta_info":{"organization":"IEEE","year":"2018","pages":"2341--2348","booktitle":"2018 IEEE International Conference on Robotics and Automation (ICRA)"}}
{"bib_id":"Horn:AI:1981","title":"Determining optical flow","author":"B. K.P. Horn and B. G. Schunck","meta_info":{"abstract":"Optical flow cannot be computed locally, since only one independent measurement is available from the image sequence at a point, while the flow velocity has two components. A second constraint is needed. A method for finding the optical flow pattern is presented which assumes that the apparent velocity of the brightness pattern varies smoothly almost everywhere in the image. An iterative implementation is shown which successfully computes the optical flow for a number of synthetic image sequences. The algorithm is robust in that it can handle image sequences that are quantized rather coarsely in space and time. It is also insensitive to quantization of brightness levels and additive noise. Examples are included where the assumption of smoothness is violated at singular points or along lines in the image.","issn":"0004-3702","year":"1981","pages":"185 - 203","number":"1","volume":"17","journal":"Artificial Intelligence"}}
{"bib_id":"Dosovitskiy:ICCV:2015","title":"FlowNet: Learning Optical Flow with Convolutional Networks","author":"A. Dosovitskiy and P. Fischer and E. Ilg and P. Häusser and C. Hazirbas and V. Golkov and P. v. d. Smagt and D. Cremers and T. Brox","meta_info":{"pages":"2758-2766","number":"","volume":"","year":"2015","booktitle":"IEEE International Conference on Computer Vision"}}
{"bib_id":"HUI:CVPR:2018","title":"LiteFlowNet: A Lightweight Convolutional Neural Network for Optical Flow Estimation","author":"T. Hui and X. Tang and C. C. Loy","meta_info":{"pages":"8981-8989","number":"","volume":"","year":"2018","booktitle":"IEEEConference on Computer Vision and Pattern Recognition"}}
{"bib_id":"Sun:CVPR:2018","title":"PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume","author":"D. Sun and X. Yang and M. Liu and J. Kautz","meta_info":{"pages":"8934-8943","number":"","volume":"","year":"2018","booktitle":"IEEE\/CVF Conference on Computer Vision and Pattern Recognition"}}
{"bib_id":"ranjan:CVPR:2016","title":"Optical Flow Estimation using a Spatial Pyramid Network","author":"A. Ranjan and M. J. Black","meta_info":{"year":"2016","booktitle":"IEEE\/CVF Conference on Computer Vision and Pattern Recognition"}}
{"bib_id":"Frossard:ICCV:2017","title":"Graph-Based Classification of Omnidirectional Images","author":"P. Frossard and R. Khasanova","meta_info":{"pages":"860-869","number":"","volume":"","year":"2017","booktitle":"IEEE International Conference on Computer Vision Workshops"}}
{"bib_id":"Esteves:ECCV:2018","title":"Learning SO(3) Equivariant Representations with Spherical CNNs","author":"Esteves, C. and Allen-Blanchette, C. and Makadia, A. and Daniilidis, K.","meta_info":{"year":"2018","booktitle":"European Conference on Computer Vision"}}
{"bib_id":"jiang:ICLR:2019","title":"Spherical CNNs on Unstructured Grids","author":"C. \"M.\" Jiang and J. Huang and K. Kashinath and Prabhat and P. Marcus and M. Niessner","meta_info":{"booktitle":"International Conference on Learning Representations","year":"2019"}}
{"bib_id":"Ronneberger:MICCAI:2015","title":"U-Net: Convolutional Networks for Biomedical Image Segmentation","author":"Ronneberger, O.\nand Fischer, P.\nand Brox, T.","meta_info":{"isbn":"978-3-319-24574-4","abstract":"There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http:\/\/lmb.informatik.uni-freiburg.de\/people\/ronneber\/u-net.","pages":"234--241","year":"2015","booktitle":"Medical Image Computing and Computer-Assisted Intervention"}}
{"bib_id":"Redmon:CVPR:2016","title":"You Only Look Once: Unified, Real-Time Object Detection","author":"J. Redmon and S. Divvala and R. Girshick and A. Farhadi","meta_info":{"pages":"779-788","number":"","volume":"","year":"2016","booktitle":"IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"}}
