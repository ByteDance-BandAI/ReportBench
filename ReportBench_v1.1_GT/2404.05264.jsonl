{"bib_id":"ji2023ai","title":"Ai alignment: A comprehensive survey","author":"Ji, Jiaming and Qiu, Tianyi and Chen, Boyuan and Zhang, Borong and Lou, Hantao and Wang, Kaile and Duan, Yawen and He, Zhonghao and Zhou, Jiayi and Zhang, Zhaowei and others","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2310.19852"}}
{"bib_id":"fan2023learning","title":"A Learning-based Incentive Mechanism for Mobile AIGC Service in Decentralized Internet of Vehicles","author":"Fan, Jiani and Xu, Minrui and Liu, Ziyao and Ye, Huanyi and Gu, Chaojie and Niyato, Dusit and Lam, Kwok-Yan","meta_info":{"organization":"IEEE","year":"2023","pages":"1--5","booktitle":"2023 IEEE 98th Vehicular Technology Conference (VTC2023-Fall)"}}
{"bib_id":"shayegani2023jailbreak","title":"Jailbreak in pieces: Compositional adversarial attacks on multi-modal language models","author":"Shayegani, Erfan and Dong, Yue and Abu-Ghazaleh, Nael","meta_info":{"year":"2023","booktitle":"The Twelfth International Conference on Learning Representations"}}
{"bib_id":"li2024images","title":"Images are Achilles' Heel of Alignment: Exploiting Visual Vulnerabilities for Jailbreaking Multimodal Large Language Models","author":"Li, Yifan and Guo, Hangyu and Zhou, Kun and Zhao, Wayne Xin and Wen, Ji-Rong","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2403.09792"}}
{"bib_id":"gong2023figstep","title":"Figstep: Jailbreaking large vision-language models via typographic visual prompts","author":"Gong, Yichen and Ran, Delong and Liu, Jinyuan and Wang, Conglei and Cong, Tianshuo and Wang, Anyu and Duan, Sisi and Wang, Xiaoyun","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2311.05608"}}
{"bib_id":"zhao2024evaluating","title":"On evaluating adversarial robustness of large vision-language models","author":"Zhao, Yunqing and Pang, Tianyu and Du, Chao and Yang, Xiao and Li, Chongxuan and Cheung, Ngai-Man Man and Lin, Min","meta_info":{"year":"2024","volume":"36","journal":"Advances in Neural Information Processing Systems"}}
{"bib_id":"shayegani2023survey","title":"Survey of vulnerabilities in large language models revealed by adversarial attacks. arXiv. doi: 10.48550","author":"Shayegani, E and Mamun, MAA and Fu, Y and Zaree, P and Dong, Y and Abu-Ghazaleh, N","meta_info":{"year":"2023","journal":"arXiv preprint arXiv.2310.10844"}}
{"bib_id":"huang2023survey","title":"A survey of safety and trustworthiness of large language models through the lens of verification and validation","author":"Huang, Xiaowei and Ruan, Wenjie and Huang, Wei and Jin, Gaojie and Dong, Yi and Wu, Changshun and Bensalem, Saddek and Mu, Ronghui and Qi, Yi and Zhao, Xingyu and others","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2305.11391"}}
{"bib_id":"cui2024risk","title":"Risk taxonomy, mitigation, and assessment benchmarks of large language model systems","author":"Cui, Tianyu and Wang, Yanling and Fu, Chuanpu and Xiao, Yong and Li, Sijia and Deng, Xinhao and Liu, Yunpeng and Zhang, Qinglin and Qiu, Ziyi and Li, Peiyang and others","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2401.05778"}}
{"bib_id":"zhang2024mm","title":"Mm-llms: Recent advances in multimodal large language models","author":"Zhang, Duzhen and Yu, Yahan and Li, Chenxing and Dong, Jiahua and Su, Dan and Chu, Chenhui and Yu, Dong","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2401.13601"}}
{"bib_id":"rombach2022high","title":"High-resolution image synthesis with latent diffusion models","author":"Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Björn","meta_info":{"year":"2022","pages":"10684--10695","booktitle":"Proceedings of the IEEE\/CVF conference on computer vision and pattern recognition"}}
{"bib_id":"wang2024essence","title":"On the Essence and Prospect: An Investigation of Alignment Approaches for Big Models","author":"Wang, Xinpeng and Duan, Shitong and Yi, Xiaoyuan and Yao, Jing and Zhou, Shanlin and Wei, Zhihua and Zhang, Peng and Xu, Dongkuan and Sun, Maosong and Xie, Xing","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2403.04204"}}
{"bib_id":"schlarmann2023adversarial","title":"On the adversarial robustness of multi-modal foundation models","author":"Schlarmann, Christian and Hein, Matthias","meta_info":{"year":"2023","pages":"3677--3685","booktitle":"Proceedings of the IEEE\/CVF International Conference on Computer Vision"}}
{"bib_id":"qi2023visual","title":"Visual adversarial examples jailbreak aligned large language models","author":"Qi, Xiangyu and Huang, Kaixuan and Panda, Ashwinee and Wang, Mengdi and Mittal, Prateek","meta_info":{"year":"2023","booktitle":"The Second Workshop on New Frontiers in Adversarial Machine Learning"}}
{"bib_id":"luo2024image","title":"An Image Is Worth 1000 Lies: Adversarial Transferability across Prompts on Vision-Language Models","author":"Luo, Haochen and Gu, Jindong and Liu, Fengyuan and Torr, Philip","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2403.09766"}}
{"bib_id":"bailey2023image","title":"Image hijacks: Adversarial images can control generative models at runtime","author":"Bailey, Luke and Ong, Euan and Russell, Stuart and Emmons, Scott","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2309.00236"}}
{"bib_id":"bagdasaryan2023ab","title":"(Ab) using Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs","author":"Bagdasaryan, Eugene and Hsieh, Tsung-Yin and Nassi, Ben and Shmatikov, Vitaly","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2307.10490"}}
{"bib_id":"wang2024stop","title":"Stop Reasoning! When Multimodal LLMs with Chain-of-Thought Reasoning Meets Adversarial Images","author":"Wang, Zefeng and Han, Zhen and Chen, Shuo and Xue, Fan and Ding, Zifeng and Xiao, Xun and Tresp, Volker and Torr, Philip and Gu, Jindong","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2402.14899"}}
{"bib_id":"lu2024test","title":"Test-Time Backdoor Attacks on Multimodal Large Language Models","author":"Lu, Dong and Pang, Tianyu and Du, Chao and Liu, Qian and Yang, Xianjun and Lin, Min","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2402.08577"}}
{"bib_id":"gu2024agent","title":"Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast","author":"Gu, Xiangming and Zheng, Xiaosen and Pang, Tianyu and Du, Chao and Liu, Qian and Wang, Ye and Jiang, Jing and Lin, Min","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2402.08567"}}
{"bib_id":"tan2024wolf","title":"The Wolf Within: Covert Injection of Malice into MLLM Societies via an MLLM Operative","author":"Tan, Zhen and Zhao, Chengshuai and Moraffah, Raha and Li, Yifan and Kong, Yu and Chen, Tianlong and Liu, Huan","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2402.14859"}}
{"bib_id":"qraitem2024vision","title":"Vision-LLMs Can Fool Themselves with Self-Generated Typographic Attacks","author":"Qraitem, Maan and Tasnim, Nazia and Saenko, Kate and Plummer, Bryan A","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2402.00626"}}
{"bib_id":"wu2023jailbreaking","title":"Jailbreaking gpt-4v via self-adversarial attacks with system prompts","author":"Wu, Yuanwei and Li, Xiang and Liu, Yixin and Zhou, Pan and Sun, Lichao","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2311.09127"}}
{"bib_id":"dong2023robust","title":"How Robust is Google's Bard to Adversarial Image Attacks?","author":"Dong, Yinpeng and Chen, Huanran and Chen, Jiawei and Fang, Zhengwei and Yang, Xiao and Zhang, Yichi and Tian, Yu and Su, Hang and Zhu, Jun","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2309.11751"}}
{"bib_id":"wang2023instructta","title":"InstructTA: Instruction-Tuned Targeted Attack for Large Vision-Language Models","author":"Wang, Xunguang and Ji, Zhenlan and Ma, Pingchuan and Li, Zongjie and Wang, Shuai","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2312.01886"}}
{"bib_id":"bagdasaryan2023ceci","title":"Ceci n'est pas une pomme: Adversarial Illusions in Multi-Modal Embeddings","author":"Bagdasaryan, Eugene and Shmatikov, Vitaly","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2308.11804"}}
{"bib_id":"han2023ot","title":"OT-Attack: Enhancing Adversarial Transferability of Vision-Language Models via Optimal Transport Optimization","author":"Han, Dongchen and Jia, Xiaojun and Bai, Yang and Gu, Jindong and Liu, Yang and Cao, Xiaochun","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2312.04403"}}
{"bib_id":"niu2024jailbreaking","title":"Jailbreaking attack against multimodal large language model","author":"Niu, Zhenxing and Ren, Haodong and Gao, Xinbo and Hua, Gang and Jin, Rong","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2402.02309"}}
{"bib_id":"tao2024imgtrojan","title":"ImgTrojan: Jailbreaking Vision-Language Models with ONE Image","author":"Tao, Xijia and Zhong, Shuai and Li, Lei and Liu, Qi and Kong, Lingpeng","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2403.02910"}}
{"bib_id":"xu2024shadowcast","title":"Shadowcast: Stealthy Data Poisoning Attacks Against Vision-Language Models","author":"Xu, Yuancheng and Yao, Jiarui and Shu, Manli and Sun, Yanchao and Wu, Zichu and Yu, Ning and Goldstein, Tom and Huang, Furong","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2402.06659"}}
{"bib_id":"liang2024vl","title":"VL-Trojan: Multimodal Instruction Backdoor Attacks against Autoregressive Visual Language Models","author":"Liang, Jiawei and Liang, Siyuan and Luo, Man and Liu, Aishan and Han, Dongchen and Chang, Ee-Chien and Cao, Xiaochun","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2402.13851"}}
{"bib_id":"visualinjection","title":"Image to prompt injection with google bard","author":"Johann Rehberger","meta_info":{"year":"2023","howpublished":"r̆lhttps:\/\/embracethered.com\/blog\/posts\/2023\/google-bard-image-to-prompt-injection\/"}}
{"bib_id":"zhang2022towards","title":"Towards adversarial attack on vision-language pre-training models","author":"Zhang, Jiaming and Yi, Qi and Sang, Jitao","meta_info":{"year":"2022","pages":"5005--5013","booktitle":"Proceedings of the 30th ACM International Conference on Multimedia"}}
{"bib_id":"lu2023set","title":"Set-level guidance attack: Boosting adversarial transferability of vision-language pre-training models","author":"Lu, Dong and Wang, Zhiqiang and Wang, Teng and Guan, Weili and Gao, Hongchang and Zheng, Feng","meta_info":{"year":"2023","pages":"102--111","booktitle":"Proceedings of the IEEE\/CVF International Conference on Computer Vision"}}
{"bib_id":"zhou2023advclip","title":"Advclip: Downstream-agnostic adversarial examples in multimodal contrastive learning","author":"Zhou, Ziqi and Hu, Shengshan and Li, Minghui and Zhang, Hangtao and Zhang, Yechao and Jin, Hai","meta_info":{"year":"2023","pages":"6311--6320","booktitle":"Proceedings of the 31st ACM International Conference on Multimedia"}}
{"bib_id":"yin2023vlattack","title":"Vlattack: Multimodal adversarial attacks on vision-language tasks via pre-trained models","author":"Yin, Ziyi and Ye, Muchao and Zhang, Tianrong and Du, Tianyu and Zhu, Jinguo and Liu, Han and Chen, Jinghui and Wang, Ting and Ma, Fenglong","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2310.04655"}}
{"bib_id":"wang2023exploring","title":"Exploring transferability of multimodal adversarial samples for vision-language pre-training models with contrastive learning","author":"Wang, Youze and Hu, Wenbo and Dong, Yinpeng and Hong, Richang","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2308.12636"}}
{"bib_id":"he2023sa","title":"SA-Attack: Improving Adversarial Transferability of Vision-Language Pre-training Models via Self-Augmentation","author":"He, Bangyan and Jia, Xiaojun and Liang, Siyuan and Lou, Tianrui and Liu, Yang and Cao, Xiaochun","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2312.04913"}}
{"bib_id":"nesterov2017random","title":"Random gradient-free minimization of convex functions","author":"Nesterov, Yurii and Spokoiny, Vladimir","meta_info":{"publisher":"Springer","year":"2017","pages":"527--566","number":"2","volume":"17","journal":"Foundations of Computational Mathematics"}}
{"bib_id":"chao2023jailbreaking","title":"Jailbreaking black box large language models in twenty queries","author":"Chao, Patrick and Robey, Alexander and Dobriban, Edgar and Hassani, Hamed and Pappas, George J and Wong, Eric","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2310.08419"}}
{"bib_id":"9186317","title":"Invisible Backdoor Attacks on Deep Neural Networks Via Steganography and Regularization","author":"Li, Shaofeng and Xue, Minhui and Zhao, Benjamin Zi Hao and Zhu, Haojin and Zhang, Xinpeng","meta_info":{"pages":"2088-2105","number":"5","volume":"18","year":"2021","journal":"IEEE Transactions on Dependable and Secure Computing"}}
{"bib_id":"10.1145\/3460120.3484576","title":"Hidden Backdoors in Human-Centric Language Models","author":"Li, Shaofeng and Liu, Hui and Dong, Tian and Zhao, Benjamin Zi Hao and Xue, Minhui and Zhu, Haojin and Lu, Jialiang","meta_info":{"booktitle":"Proceedings of ACM CCS","year":"2021"}}
{"bib_id":"li2024red","title":"Red teaming visual language models","author":"Li, Mukai and Li, Lei and Yin, Yuwei and Ahmed, Masood and Liu, Zhenguang and Liu, Qi","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2401.12915"}}
{"bib_id":"yang2024robust","title":"Robust Contrastive Language-Image Pretraining against Data Poisoning and Backdoor Attacks","author":"Yang, Wenhan and Gao, Jingdong and Mirzasoleiman, Baharan","meta_info":{"year":"2024","volume":"36","journal":"Advances in Neural Information Processing Systems"}}
{"bib_id":"zhang2023adversarial","title":"Adversarial Prompt Tuning for Vision-Language Models","author":"Zhang, Jiaming and Ma, Xingjun and Wang, Xin and Qiu, Lingyu and Wang, Jiaqi and Jiang, Yu-Gang and Sang, Jitao","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2311.11261"}}
{"bib_id":"li2024one","title":"One Prompt Word is Enough to Boost Adversarial Robustness for Pre-trained Vision-Language Models","author":"Li, Lin and Guan, Haoyan and Qiu, Jianing and Spratling, Michael","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2403.01849"}}
{"bib_id":"chen2023dress","title":"Dress: Instructing large vision-language models to align and interact with humans via natural language feedback","author":"Chen, Yangyi and Sikka, Karan and Cogswell, Michael and Ji, Heng and Divakaran, Ajay","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2311.10081"}}
{"bib_id":"zhang2023mutation","title":"A mutation-based method for multi-modal jailbreaking attack detection","author":"Zhang, Xiaoyu and Zhang, Cen and Li, Tianlin and Huang, Yihao and Jia, Xiaojun and Xie, Xiaofei and Liu, Yang and Shen, Chao","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2312.10766"}}
{"bib_id":"pi2024mllm","title":"MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance","author":"Pi, Renjie and Han, Tianyang and Xie, Yueqi and Pan, Rui and Lian, Qing and Dong, Hanze and Zhang, Jipeng and Zhang, Tong","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2401.02906"}}
{"bib_id":"wang2024inferaligner","title":"Inferaligner: Inference-time alignment for harmlessness through cross-model guidance","author":"Wang, Pengyu and Zhang, Dong and Li, Linyang and Tan, Chenkun and Wang, Xinghao and Ren, Ke and Jiang, Botian and Qiu, Xipeng","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2401.11206"}}
{"bib_id":"wang2024adashield","title":"AdaShield: Safeguarding Multimodal Large Language Models from Structure-based Attack via Adaptive Shield Prompting","author":"Wang, Yu and Liu, Xiaogeng and Li, Yu and Chen, Muhao and Xiao, Chaowei","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2403.09513"}}
{"bib_id":"gou2024eyes","title":"Eyes Closed, Safety On: Protecting Multimodal LLMs via Image-to-Text Transformation","author":"Gou, Yunhao and Chen, Kai and Liu, Zhili and Hong, Lanqing and Xu, Hang and Li, Zhenguo and Yeung, Dit-Yan and Kwok, James T and Zhang, Yu","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2403.09572"}}
{"bib_id":"zhang2023right","title":"Right to be forgotten in the era of large language models: Implications, challenges, and solutions","author":"Zhang, Dawen and Finckenberg-Broman, Pamela and Hoang, Thong and Pan, Shidong and Xing, Zhenchang and Staples, Mark and Xu, Xiwei","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2307.03941"}}
{"bib_id":"lynch2024eight","title":"Eight Methods to Evaluate Robust Unlearning in LLMs","author":"Lynch, Aengus and Guo, Phillip and Ewart, Aidan and Casper, Stephen and Hadfield-Menell, Dylan","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2402.16835"}}
{"bib_id":"dwork2006differential","title":"Differential privacy","author":"Dwork, Cynthia","meta_info":{"organization":"Springer","year":"2006","pages":"1--12","booktitle":"International colloquium on automata, languages, and programming"}}
{"bib_id":"liu2023privacy","title":"Privacy-Enhanced Knowledge Transfer with Collaborative Split Learning over Teacher Ensembles","author":"Liu, Ziyao and Guo, Jiale and Yang, Mengmeng and Yang, Wenzhuo and Fan, Jiani and Lam, Kwok-Yan","meta_info":{"year":"2023","pages":"1--13","booktitle":"Proceedings of the 2023 Secure and Trustworthy Deep Learning Systems Workshop"}}
{"bib_id":"zhang2024bounded","title":"Bounded and unbiased composite differential privacy","author":"Zhang, Kai and Zhang, Yanjun and Sun, Ruoxi and Tsai, Pei-Wei and Hassan, Muneeb Ul and Yuan, Xin and Xue, Minhui and Chen, Jinjun","meta_info":{"year":"2024","booktitle":"2024 IEEE Symposium on Security and Privacy (SP)"}}
{"bib_id":"yin2021comprehensive","title":"A comprehensive survey of privacy-preserving federated learning: A taxonomy, review, and future directions","author":"Yin, Xuefei and Zhu, Yanming and Hu, Jiankun","meta_info":{"publisher":"ACM New York, NY, USA","year":"2021","pages":"1--36","number":"6","volume":"54","journal":"ACM Computing Surveys (CSUR)"}}
{"bib_id":"liu2024dynamic","title":"Dynamic User Clustering for Efficient and Privacy-Preserving Federated Learning","author":"Liu, Ziyao and Guo, Jiale and Yang, Wenzhuo and Fan, Jiani and Lam, Kwok-Yan and Zhao, Jun","meta_info":{"publisher":"IEEE","year":"2024","journal":"IEEE Transactions on Dependable and Secure Computing"}}
{"bib_id":"liu2023long","title":"Long-term privacy-preserving aggregation with user-dynamics for federated learning","author":"Liu, Ziyao and Lin, Hsiao-Ying and Liu, Yamin","meta_info":{"publisher":"IEEE","year":"2023","journal":"IEEE Transactions on Information Forensics and Security"}}
{"bib_id":"hu2024duty","title":"A Duty to Forget, a Right to be Assured? Exposing Vulnerabilities in Machine Unlearning Services","author":"Hu, Hongsheng and Wang, Shuo and Chang, Jiamin and Zhong, Haonan and Sun, Ruoxi and Hao, Shuang and Zhu, Haojin and Xue, Minhui","meta_info":{"year":"2024","booktitle":"Proceedings of the Network and Distributed System Security Symposium"}}
{"bib_id":"hu2024learn","title":"Learn What You Want to Unlearn: Unlearning Inversion Attacks against Machine Unlearning","author":"Hu, Hongsheng and Wang, Shuo and Dong, Tian and Xue, Minhui","meta_info":{"year":"2024","booktitle":"2024 IEEE Symposium on Security and Privacy (SP)"}}
{"bib_id":"liu2024threats","title":"Threats, Attacks, and Defenses in Machine Unlearning: A Survey","author":"Liu, Ziyao and Ye, Huanyi and Chen, Chen and Lam, Kwok-Yan","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2403.13682"}}
{"bib_id":"jiang2024towards","title":"Towards Efficient and Certified Recovery from Poisoning Attacks in Federated Learning","author":"Jiang, Yu and Shen, Jiyuan and Liu, Ziyao and Tan, Chee Wei and Lam, Kwok-Yan","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2401.08216"}}
{"bib_id":"liu2023survey","title":"A survey on federated unlearning: Challenges, methods, and future directions","author":"Liu, Ziyao and Jiang, Yu and Shen, Jiyuan and Peng, Minyi and Lam, Kwok-Yan and Yuan, Xingliang","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2310.20448"}}
{"bib_id":"lee2023rlaif","title":"Rlaif: Scaling reinforcement learning from human feedback with ai feedback","author":"Lee, Harrison and Phatale, Samrat and Mansoor, Hassan and Lu, Kellie and Mesnard, Thomas and Bishop, Colton and Carbune, Victor and Rastogi, Abhinav","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2309.00267"}}
{"bib_id":"wang2023knowledge","title":"Knowledge editing for large language models: A survey","author":"Wang, Song and Zhu, Yaochen and Liu, Haochen and Zheng, Zaiyi and Chen, Chen and others","meta_info":{"year":"2023","journal":"arXiv preprint arXiv:2310.16218"}}
{"bib_id":"wang2024editing","title":"Editing Conceptual Knowledge for Large Language Models","author":"Wang, Xiaohan and Mao, Shengyu and Zhang, Ningyu and Deng, Shumin and Yao, Yunzhi and Shen, Yue and Liang, Lei and Gu, Jinjie and Chen, Huajun","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2403.06259"}}
{"bib_id":"wang2024detoxifying","title":"Detoxifying Large Language Models via Knowledge Editing","author":"Wang, Mengru and Zhang, Ningyu and Xu, Ziwen and Xi, Zekun and Deng, Shumin and Yao, Yunzhi and Zhang, Qishen and Yang, Linyi and Wang, Jindong and Chen, Huajun","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2403.14472"}}
{"bib_id":"zhao2024first","title":"The First to Know: How Token Distributions Reveal Hidden Knowledge in Large Vision-Language Models?","author":"Zhao, Qinyu and Xu, Ming and Gupta, Kartik and Asthana, Akshay and Zheng, Liang and Gould, Stephen","meta_info":{"year":"2024","journal":"arXiv preprint arXiv:2403.09037"}}
