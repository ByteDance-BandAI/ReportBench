{"bib_id":"paper1","title":"The risk of racial bias in hate speech detection","author":"Sap, Maarten and Card, Dallas and Gabriel, Saadia and Choi, Yejin and Smith, Noah A","meta_info":{"year":"2019","pages":"1668--1678","booktitle":"Proceedings of the 57th annual meeting of the association for computational linguistics"}}
{"bib_id":"paper2","title":"Examining Racial Bias in an Online Abuse Corpus with Structural Topic Modeling","author":"Davidson, Thomas and Bhattacharya, Debasmita","meta_info":{"year":"2020","journal":"arXiv preprint arXiv:2005.13041"}}
{"bib_id":"paper3","title":"Multilingual Twitter corpus and baselines for evaluating demographic bias in hate speech recognition","author":"Huang, Xiaolei and Xing, Linzi and Dernoncourt, Franck and Paul, Michael J","meta_info":{"year":"2020","journal":"arXiv preprint arXiv:2002.10361"}}
{"bib_id":"paper5","title":"Racial Bias in Hate Speech and Abusive Language Detection Datasets","author":"Davidson, Thomas  and\nBhattacharya, Debasmita  and\nWeber, Ingmar","meta_info":{"abstract":"Technologies for abusive language detection are being developed and applied with little consideration of their potential biases. We examine racial bias in five different sets of Twitter data annotated for hate speech and abusive language. We train classifiers on these datasets and compare the predictions of these classifiers on tweets written in African-American English with those written in Standard American English. The results show evidence of systematic racial bias in all datasets, as classifiers trained on them tend to predict that tweets written in African-American English are abusive at substantially higher rates. If these abusive language detection systems are used in the field they will therefore have a disproportionate negative impact on African-American social media users. Consequently, these systems may discriminate against the groups who are often the targets of the abuse we are trying to detect.","pages":"25--35","doi":"10.18653\/v1\/W19-3504","url":"https:\/\/www.aclweb.org\/anthology\/W19-3504","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"August","booktitle":"Proceedings of the Third Workshop on Abusive Language Online"}}
{"bib_id":"paper6","title":"Hate speech detection and racial bias mitigation in social media based on BERT model","author":"Mozafari, Marzieh and Farahbakhsh, Reza and Crespi, Noël","meta_info":{"publisher":"Public Library of Science San Francisco, CA USA","year":"2020","pages":"e0237861","number":"8","volume":"15","journal":"PloS one"}}
{"bib_id":"paper7","title":"Contextualizing hate speech classifiers with post-hoc explanation","author":"Kennedy, Brendan and Jin, Xisen and Davani, Aida Mostafazadeh and Dehghani, Morteza and Ren, Xiang","meta_info":{"year":"2020","journal":"arXiv preprint arXiv:2005.02439"}}
{"bib_id":"paper8","title":"Comparative evaluation of label agnostic selection bias in multilingual hate speech datasets","author":"Ousidhoum, Nedjma and Song, Yangqiu and Yeung, Dit-Yan","meta_info":{"year":"2020","pages":"2532--2542","booktitle":"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)"}}
{"bib_id":"paper9","title":"Reducing Gender Bias in Abusive Language Detection","author":"Park, Ji Ho  and\nShin, Jamin  and\nFung, Pascale","meta_info":{"abstract":"Abusive language detection models tend to have a problem of being biased toward identity words of a certain group of people because of imbalanced training datasets. For example, ``You are a good woman″ was considered ``sexist″ when trained on an existing dataset. Such model bias is an obstacle for models to be robust enough for practical use. In this work, we measure them on models trained with different datasets, while analyzing the effect of different pre-trained word embeddings and model architectures. We also experiment with three mitigation methods: (1) debiased word embeddings, (2) gender swap data augmentation, and (3) fine-tuning with a larger corpus. These methods can effectively reduce model bias by 90-98% and can be extended to correct model bias in other scenarios.","pages":"2799--2804","doi":"10.18653\/v1\/D18-1302","url":"https:\/\/www.aclweb.org\/anthology\/D18-1302","publisher":"Association for Computational Linguistics","address":"Brussels, Belgium","year":"2018","month":"October-November","booktitle":"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing"}}
{"bib_id":"paper10","title":"Challenges in Automated Debiasing for Toxic Language Detection","author":"Zhou, Xuhui  and\nSap, Maarten  and\nSwayamdipta, Swabha  and\nChoi, Yejin  and\nSmith, Noah","meta_info":{"abstract":"Biased associations have been a challenge in the development of classifiers for detecting toxic language, hindering both fairness and accuracy. As potential solutions, we investigate recently introduced debiasing methods for text classification datasets and models, as applied to toxic language detection. Our focus is on lexical (e.g., swear words, slurs, identity mentions) and dialectal markers (specifically African American English). Our comprehensive experiments establish that existing methods are limited in their ability to prevent biased behavior in current toxicity detectors. We then propose an automatic, dialect-aware data correction method, as a proof-of-concept. Despite the use of synthetic labels, this method reduces dialectal associations with toxicity. Overall, our findings show that debiasing a model trained on biased toxic language data is not as effective as simply relabeling the data to remove existing biases.","pages":"3143--3155","url":"https:\/\/www.aclweb.org\/anthology\/2021.eacl-main.274","publisher":"Association for Computational Linguistics","address":"Online","year":"2021","month":"April","booktitle":"Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume"}}
{"bib_id":"paper11","title":"Demoting Racial Bias in Hate Speech Detection","author":"Xia, Mengzhou  and\nField, Anjalie  and\nTsvetkov, Yulia","meta_info":{"abstract":"In the task of hate speech detection, there exists a high correlation between African American English (AAE) and annotators′ perceptions of toxicity in current datasets. This bias in annotated training data and the tendency of machine learning models to amplify it cause AAE text to often be mislabeled as abusive\/offensive\/hate speech (high false positive rate) by current hate speech classifiers. Here, we use adversarial training to mitigate this bias. Experimental results on one hate speech dataset and one AAE dataset suggest that our method is able to reduce the false positive rate for AAE text with only a minimal compromise on the performance of hate speech classification.","pages":"7--14","doi":"10.18653\/v1\/2020.socialnlp-1.2","url":"https:\/\/www.aclweb.org\/anthology\/2020.socialnlp-1.2","publisher":"Association for Computational Linguistics","address":"Online","year":"2020","month":"July","booktitle":"Proceedings of the Eighth International Workshop on Natural Language Processing for Social Media"}}
{"bib_id":"paper12","title":"Unintended bias in misogyny detection","author":"Nozza, Debora and Volpetti, Claudia and Fersini, Elisabetta","meta_info":{"year":"2019","pages":"149--155","booktitle":"IEEE\/WIC\/ACM International Conference on Web Intelligence"}}
{"bib_id":"paper13","title":"Investigating sampling bias in abusive language detection","author":"Razo, Dante and Kübler, Sandra","meta_info":{"year":"2020","pages":"70--78","booktitle":"Proceedings of the Fourth Workshop on Online Abuse and Harms"}}
{"bib_id":"paper14","title":"Lower Bias, Higher Density Abusive Language Datasets: A Recipe","author":"van Rosendaal, Juliet and Caselli, Tommaso and Nissim, Malvina","meta_info":{"year":"2020","pages":"14--19","booktitle":"Proceedings of the Workshop on Resources and Techniques for User and Author Profiling in Abusive Language"}}
{"bib_id":"paper15","title":"Empirical Analysis of Multi-Task Learning for Reducing Identity Bias in Toxic Comment Detection","author":"Vaidya, Ameya and Mai, Feng and Ning, Yue","meta_info":{"pages":"683-693","month":"May","year":"2020","journal":"Proceedings of the International AAAI Conference on Web and Social Media","number":"1","abstractnote":"&lt;p&gt;With the recent rise of toxicity in online conversations on social media platforms, using modern machine learning algorithms for toxic comment detection has become a central focus of many online applications. Researchers and companies have developed a variety of models to identify toxicity in online conversations, reviews, or comments with mixed successes. However, many existing approaches have learned to incorrectly associate non-toxic comments that have certain trigger-words (e.g. gay, lesbian, black, muslim) as a potential source of toxicity. In this paper, we evaluate several state-of-the-art models with the specific focus of reducing model bias towards these commonly-attacked identity groups. We propose a multi-task learning model with an attention layer that jointly learns to predict the toxicity of a comment as well as the identities present in the comments in order to reduce this bias. We then compare our model to an array of shallow and deep-learning models using metrics designed especially to test for unintended model bias within these identity groups.&lt;\/p&gt;","url":"https:\/\/ojs.aaai.org\/index.php\/ICWSM\/article\/view\/7334","volume":"14"}}
{"bib_id":"paper16","title":"Hate speech detection is not as easy as you may think: A closer look at model validation","author":"Arango, Aymé and Pérez, Jorge and Poblete, Barbara","meta_info":{"year":"2019","pages":"45--54","booktitle":"Proceedings of the 42nd international acm sigir conference on research and development in information retrieval"}}
{"bib_id":"paper17","title":"Are you a racist or am i seeing things? annotator influence on hate speech detection on twitter","author":"Waseem, Zeerak","meta_info":{"year":"2016","pages":"138--142","booktitle":"Proceedings of the first workshop on NLP and computational social science"}}
{"bib_id":"paper18","title":"Measuring the reliability of hate speech annotations: The case of the european refugee crisis","author":"Ross, Björn and Rist, Michael and Carbonell, Guillermo and Cabrera, Benjamin and Kurowsky, Nils and Wojatzki, Michael","meta_info":{"year":"2017","journal":"arXiv preprint arXiv:1701.08118"}}
{"bib_id":"paper19","title":"Detection of abusive language: the problem of biased datasets","author":"Wiegand, Michael and Ruppenhofer, Josef and Kleinbauer, Thomas","meta_info":{"year":"2019","pages":"602--608","booktitle":"Proceedings of the 2019 conference of the North American Chapter of the Association for Computational Linguistics: human language technologies, volume 1 (long and short papers)"}}
{"bib_id":"paper20","title":"Measuring and mitigating unintended bias in text classification","author":"Dixon, Lucas and Li, John and Sorensen, Jeffrey and Thain, Nithum and Vasserman, Lucy","meta_info":{"year":"2018","pages":"67--73","booktitle":"Proceedings of the 2018 AAAI\/ACM Conference on AI, Ethics, and Society"}}
{"bib_id":"paper21","title":"Stereotypical bias removal for hate speech detection task using knowledge-based generalizations","author":"Badjatiya, Pinkesh and Gupta, Manish and Varma, Vasudeva","meta_info":{"year":"2019","pages":"49--59","booktitle":"The World Wide Web Conference"}}
{"bib_id":"paper22","title":"Identifying and measuring annotator bias based on annotators’ demographic characteristics","author":"Al Kuwatly, Hala and Wich, Maximilian and Groh, Georg","meta_info":{"year":"2020","pages":"184--190","booktitle":"Proceedings of the Fourth Workshop on Online Abuse and Harms"}}
{"bib_id":"paper23","title":"Investigating Annotator Bias with a Graph-Based Approach","author":"Wich, Maximilian and Al Kuwatly, Hala and Groh, Georg","meta_info":{"year":"2020","pages":"191--199","booktitle":"Proceedings of the Fourth Workshop on Online Abuse and Harms"}}
{"bib_id":"paper24","title":"Impact of politically biased data on hate speech classification","author":"Wich, Maximilian and Bauer, Jan and Groh, Georg","meta_info":{"year":"2020","pages":"54--64","booktitle":"Proceedings of the Fourth Workshop on Online Abuse and Harms"}}
{"bib_id":"blodgettlm","title":"Demographic Dialectal Variation in Social Media: A Case Study of African-American English","author":"Blodgett, Su Lin  and\nGreen, Lisa  and\nO′Connor, Brendan","meta_info":{"pages":"1119--1130","doi":"10.18653\/v1\/D16-1120","url":"https:\/\/www.aclweb.org\/anthology\/D16-1120","publisher":"Association for Computational Linguistics","address":"Austin, Texas","year":"2016","month":"November","booktitle":"Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing"}}
{"bib_id":"frameworksuresh","title":"A framework for understanding unintended consequences of machine learning","author":"Suresh, Harini and Guttag, John V","meta_info":{"year":"2019","journal":"arXiv preprint arXiv:1901.10002"}}
{"bib_id":"pietrodataset","title":"User-level race and ethnicity predictors from twitter text","author":"Preoţiuc-Pietro, Daniel and Ungar, Lyle","meta_info":{"year":"2018","pages":"1534--1545","booktitle":"Proceedings of the 27th International Conference on Computational Linguistics"}}
{"bib_id":"factverificationmodels","title":"Towards Debiasing Fact Verification Models","author":"Schuster, Tal  and\nShah, Darsh  and\nYeo, Yun Jie Serene  and\nRoberto Filizzola Ortiz, Daniel  and\nSantus, Enrico  and\nBarzilay, Regina","meta_info":{"abstract":"Fact verification requires validating a claim in the context of evidence. We show, however, that in the popular FEVER dataset this might not necessarily be the case. Claim-only classifiers perform competitively with top evidence-aware models. In this paper, we investigate the cause of this phenomenon, identifying strong cues for predicting labels solely based on the claim, without considering any evidence. We create an evaluation set that avoids those idiosyncrasies. The performance of FEVER-trained models significantly drops when evaluated on this test set. Therefore, we introduce a regularization method which alleviates the effect of bias in the training data, obtaining improvements on the newly created test set. This work is a step towards a more sound evaluation of reasoning capabilities in fact verification models.","pages":"3419--3425","doi":"10.18653\/v1\/D19-1341","url":"https:\/\/www.aclweb.org\/anthology\/D19-1341","publisher":"Association for Computational Linguistics","address":"Hong Kong, China","year":"2019","month":"November","booktitle":"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)"}}
{"bib_id":"kumar2019demotinglatentconfounds","title":"Topics to avoid: Demoting latent confounds in text classification","author":"Kumar, Sachin and Wintner, Shuly and Smith, Noah A and Tsvetkov, Yulia","meta_info":{"year":"2019","journal":"arXiv preprint arXiv:1909.00453"}}
{"bib_id":"clarklearnedmixin","title":"Don′t Take the Easy Way Out: Ensemble Based Methods for Avoiding Known Dataset Biases","author":"Clark, Christopher  and\nYatskar, Mark  and\nZettlemoyer, Luke","meta_info":{"abstract":"State-of-the-art models often make use of superficial patterns in the data that do not generalize well to out-of-domain or adversarial settings. For example, textual entailment models often learn that particular key words imply entailment, irrespective of context, and visual question answering models learn to predict prototypical answers, without considering evidence in the image. In this paper, we show that if we have prior knowledge of such biases, we can train a model to be more robust to domain shift. Our method has two stages: we (1) train a naive model that makes predictions exclusively based on dataset biases, and (2) train a robust model as part of an ensemble with the naive one in order to encourage it to focus on other patterns in the data that are more likely to generalize. Experiments on five datasets with out-of-domain test sets show significantly improved robustness in all settings, including a 12 point gain on a changing priors visual question answering dataset and a 9 point gain on an adversarial question answering test set.","pages":"4069--4082","doi":"10.18653\/v1\/D19-1418","url":"https:\/\/www.aclweb.org\/anthology\/D19-1418","publisher":"Association for Computational Linguistics","address":"Hong Kong, China","year":"2019","month":"November","booktitle":"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)"}}
{"bib_id":"borkanmetrics2019","title":"Nuanced metrics for measuring unintended bias with real data for text classification","author":"Borkan, Daniel and Dixon, Lucas and Sorensen, Jeffrey and Thain, Nithum and Vasserman, Lucy","meta_info":{"year":"2019","pages":"491--500","booktitle":"Companion proceedings of the 2019 world wide web conference"}}
{"bib_id":"bolukbasi","title":"Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings","author":"Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James and Saligrama, Venkatesh and Kalai, Adam","meta_info":{"series":"NIPS'16","location":"Barcelona, Spain","numpages":"9","pages":"4356–4364","booktitle":"Proceedings of the 30th International Conference on Neural Information Processing Systems","abstract":"The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female\/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.","address":"Red Hook, NY, USA","publisher":"Curran Associates Inc.","isbn":"9781510838819","year":"2016"}}
{"bib_id":"pauc-limitations","title":"Limitations of pinned auc for measuring unintended bias","author":"Borkan, Daniel and Dixon, Lucas and Li, John and Sorensen, Jeffrey and Thain, Nithum and Vasserman, Lucy","meta_info":{"year":"2019","journal":"arXiv preprint arXiv:1903.02088"}}
{"bib_id":"portuguesedatasetinpaper8","title":"A hierarchically-labeled portuguese hate speech dataset","author":"Fortuna, Paula and da Silva, Joao Rocha and Wanner, Leo and Nunes, Sérgio and others","meta_info":{"year":"2019","pages":"94--104","booktitle":"Proceedings of the Third Workshop on Abusive Language Online"}}
{"bib_id":"albadiarabicdataset2018","title":"Are they our brothers? Analysis and detection of religious hate speech in the Arabic Twittersphere","author":"Albadi, Nuha and Kurdi, Maram and Mishra, Shivakant","meta_info":{"organization":"IEEE","year":"2018","pages":"69--76","booktitle":"2018 IEEE\/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)"}}
{"bib_id":"qianwaseemauthorbias","title":"Leveraging Intra-User and Inter-User Representation Learning for Automated Hate Speech Detection","author":"Qian, Jing  and\nElSherief, Mai  and\nBelding, Elizabeth  and\nWang, William Yang","meta_info":{"abstract":"Hate speech detection is a critical, yet challenging problem in Natural Language Processing (NLP). Despite the existence of numerous studies dedicated to the development of NLP hate speech detection approaches, the accuracy is still poor. The central problem is that social media posts are short and noisy, and most existing hate speech detection solutions take each post as an isolated input instance, which is likely to yield high false positive and negative rates. In this paper, we radically improve automated hate speech detection by presenting a novel model that leverages intra-user and inter-user representation learning for robust hate speech detection on Twitter. In addition to the target Tweet, we collect and analyze the user′s historical posts to model intra-user Tweet representations. To suppress the noise in a single Tweet, we also model the similar Tweets posted by all other users with reinforced inter-user representation learning techniques. Experimentally, we show that leveraging these two representations can significantly improve the f-score of a strong bidirectional LSTM baseline model by 10.1%.","pages":"118--123","doi":"10.18653\/v1\/N18-2019","url":"https:\/\/www.aclweb.org\/anthology\/N18-2019","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)"}}
{"bib_id":"mishrawaseemauthorbias","title":"Author profiling for abuse detection","author":"Mishra, Pushkar and Del Tredici, Marco and Yannakoudakis, Helen and Shutova, Ekaterina","meta_info":{"year":"2018","pages":"1088--1098","booktitle":"Proceedings of the 27th international conference on computational linguistics"}}
{"bib_id":"graumas2019","title":"Twitter-based polarised embeddings for abusive language detection","author":"Graumas, Leon and David, Roy and Caselli, Tommaso","meta_info":{"organization":"IEEE","year":"2019","pages":"1--7","booktitle":"2019 8th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW)"}}
{"bib_id":"aflite","title":"Adversarial filters of dataset biases","author":"Le Bras, Ronan and Swayamdipta, Swabha and Bhagavatula, Chandra and Zellers, Rowan and Peters, Matthew and Sabharwal, Ashish and Choi, Yejin","meta_info":{"organization":"PMLR","year":"2020","pages":"1078--1088","booktitle":"International Conference on Machine Learning"}}
{"bib_id":"datamaps","title":"Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics","author":"Swayamdipta, Swabha  and\nSchwartz, Roy  and\nLourie, Nicholas  and\nWang, Yizhong  and\nHajishirzi, Hannaneh  and\nSmith, Noah A.  and\nChoi, Yejin","meta_info":{"abstract":"Large datasets have become commonplace in NLP research. However, the increased emphasis on data quantity has made it challenging to assess the quality of data. We introduce Data Maps---a model-based tool to characterize and diagnose datasets. We leverage a largely ignored source of information: the behavior of the model on individual instances during training (training dynamics) for building data maps. This yields two intuitive measures for each example---the model′s confidence in the true class, and the variability of this confidence across epochs---obtained in a single run of training. Experiments on four datasets show that these model-dependent measures reveal three distinct regions in the data map, each with pronounced characteristics. First, our data maps show the presence of ``ambiguous″ regions with respect to the model, which contribute the most towards out-of-distribution generalization. Second, the most populous regions in the data are ``easy to learn″ for the model, and play an important role in model optimization. Finally, data maps uncover a region with instances that the model finds ``hard to learn″; these often correspond to labeling errors. Our results indicate that a shift in focus from quantity to quality of data could lead to robust models and improved out-of-distribution generalization.","pages":"9275--9293","doi":"10.18653\/v1\/2020.emnlp-main.746","url":"https:\/\/www.aclweb.org\/anthology\/2020.emnlp-main.746","publisher":"Association for Computational Linguistics","address":"Online","year":"2020","month":"November","booktitle":"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)"}}
{"bib_id":"louvaincommunitydetection","title":"Fast unfolding of communities in large networks","author":"Blondel, Vincent D and Guillaume, Jean-Loup and Lambiotte, Renaud and Lefebvre, Etienne","meta_info":{"publisher":"IOP Publishing","year":"2008","pages":"P10008","number":"10","volume":"2008","journal":"Journal of statistical mechanics: theory and experiment"}}
{"bib_id":"stm","title":"Structural topic models for open-ended survey responses","author":"Roberts, Margaret E and Stewart, Brandon M and Tingley, Dustin and Lucas, Christopher and Leder-Luis, Jetson and Gadarian, Shana Kushner and Albertson, Bethany and Rand, David G","meta_info":{"publisher":"Wiley Online Library","year":"2014","pages":"1064--1082","number":"4","volume":"58","journal":"American Journal of Political Science"}}
{"bib_id":"aaelanguageuse","title":"African-American language use: Ideology and so-called obscenity","author":"Spears, Arthur K and others","meta_info":{"publisher":"Routledge London,, UK","year":"1998","pages":"226--250","journal":"African-American English: Structure, history, and use"}}
{"bib_id":"gpt3","title":"Language models are few-shot learners","author":"Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others","meta_info":{"year":"2020","journal":"arXiv preprint arXiv:2005.14165"}}
{"bib_id":"saeavoidrosa","title":"Looking like a language, sounding like a race","author":"Rosa, Jonathan","meta_info":{"publisher":"Oxf Studies in Anthropology of","year":"2019"}}
{"bib_id":"glove","title":"Glove: Global vectors for word representation","author":"Pennington, Jeffrey and Socher, Richard and Manning, Christopher D","meta_info":{"year":"2014","pages":"1532--1543","booktitle":"Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)"}}
{"bib_id":"w2v","title":"Efficient Estimation of Word Representations in Vector Space","author":"Tomás Mikolov and\nKai Chen and\nGreg Corrado and\nJeffrey Dean","meta_info":{"bibsource":"dblp computer science bibliography, https:\/\/dblp.org","biburl":"https:\/\/dblp.org\/rec\/journals\/corr\/abs-1301-3781.bib","timestamp":"Mon, 28 Dec 2020 11:31:01 +0100","url":"http:\/\/arxiv.org\/abs\/1301.3781","year":"2013","booktitle":"1st International Conference on Learning Representations, ICLR 2013,\nScottsdale, Arizona, USA, May 2-4, 2013, Workshop Track Proceedings","editor":"Yoshua Bengio and\nYann LeCun"}}
{"bib_id":"caliskan","title":"Semantics derived automatically from language corpora contain human-like biases","author":"Caliskan, Aylin and Bryson, Joanna J and Narayanan, Arvind","meta_info":{"publisher":"American Association for the Advancement of Science","year":"2017","pages":"183--186","number":"6334","volume":"356","journal":"Science"}}
{"bib_id":"bertweet","title":"BERTweet: A pre-trained language model for English Tweets","author":"Nguyen, Dat Quoc  and\nVu, Thanh  and\nTuan Nguyen, Anh","meta_info":{"abstract":"We present BERTweet, the first public large-scale pre-trained language model for English Tweets. Our BERTweet, having the same architecture as BERT-base (Devlin et al., 2019), is trained using the RoBERTa pre-training procedure (Liu et al., 2019). Experiments show that BERTweet outperforms strong baselines RoBERTa-base and XLM-R-base (Conneau et al., 2020), producing better performance results than the previous state-of-the-art models on three Tweet NLP tasks: Part-of-speech tagging, Named-entity recognition and text classification. We release BERTweet under the MIT License to facilitate future research and applications on Tweet data. Our BERTweet is available at https:\/\/github.com\/VinAIResearch\/BERTweet","pages":"9--14","doi":"10.18653\/v1\/2020.emnlp-demos.2","url":"https:\/\/www.aclweb.org\/anthology\/2020.emnlp-demos.2","publisher":"Association for Computational Linguistics","address":"Online","year":"2020","month":"October","booktitle":"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations"}}
{"bib_id":"founta","title":"Large scale crowdsourcing and characterization of twitter abusive behavior","author":"Founta, Antigoni and Djouvas, Constantinos and Chatzakou, Despoina and Leontiadis, Ilias and Blackburn, Jeremy and Stringhini, Gianluca and Vakali, Athena and Sirivianos, Michael and Kourtellis, Nicolas","meta_info":{"year":"2018","number":"1","volume":"12","booktitle":"Proceedings of the International AAAI Conference on Web and Social Media"}}
{"bib_id":"fortunalabelstudy","title":"Toxic, hateful, offensive or abusive? what are we really classifying? an empirical analysis of hate speech datasets","author":"Fortuna, Paula and Soler, Juan and Wanner, Leo","meta_info":{"year":"2020","pages":"6786--6794","booktitle":"Proceedings of the 12th Language Resources and Evaluation Conference"}}
{"bib_id":"zhao-etal-2018-gender","title":"Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods","author":"Zhao, Jieyu  and\nWang, Tianlu  and\nYatskar, Mark  and\nOrdonez, Vicente  and\nChang, Kai-Wei","meta_info":{"abstract":"In this paper, we introduce a new benchmark for co-reference resolution focused on gender bias, WinoBias. Our corpus contains Winograd-schema style sentences with entities corresponding to people referred by their occupation (e.g. the nurse, the doctor, the carpenter). We demonstrate that a rule-based, a feature-rich, and a neural coreference system all link gendered pronouns to pro-stereotypical entities with higher accuracy than anti-stereotypical entities, by an average difference of 21.1 in F1 score. Finally, we demonstrate a data-augmentation approach that, in combination with existing word-embedding debiasing techniques, removes the bias demonstrated by these systems in WinoBias without significantly affecting their performance on existing datasets.","pages":"15--20","doi":"10.18653\/v1\/N18-2003","url":"https:\/\/www.aclweb.org\/anthology\/N18-2003","publisher":"Association for Computational Linguistics","address":"New Orleans, Louisiana","year":"2018","month":"June","booktitle":"Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)"}}
{"bib_id":"whitesupremacystormfrontdataset","title":"Hate speech dataset from a white supremacy forum","author":"de Gibert, Ona and Perez, Naiara and Garcı́a-Pablos, Aitor and Cuadros, Montse","meta_info":{"year":"2018","journal":"arXiv preprint arXiv:1809.04444"}}
{"bib_id":"cvbiasbenchmark","title":"Towards fairness in visual recognition: Effective strategies for bias mitigation","author":"Wang, Zeyu and Qinami, Klint and Karakozis, Ioannis Christos and Genova, Kyle and Nair, Prem and Hata, Kenji and Russakovsky, Olga","meta_info":{"year":"2020","pages":"8919--8928","booktitle":"Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern Recognition"}}
{"bib_id":"wordnet","title":"WordNet: a lexical database for English","author":"Miller, George A","meta_info":{"publisher":"ACM New York, NY, USA","year":"1995","pages":"39--41","number":"11","volume":"38","journal":"Communications of the ACM"}}
{"bib_id":"sentdebias","title":"Towards Debiasing Sentence Representations","author":"Liang, Paul Pu  and\nLi, Irene Mengze  and\nZheng, Emily  and\nLim, Yao Chong  and\nSalakhutdinov, Ruslan  and\nMorency, Louis-Philippe","meta_info":{"abstract":"As natural language processing methods are increasingly deployed in real-world scenarios such as healthcare, legal systems, and social science, it becomes necessary to recognize the role they potentially play in shaping social biases and stereotypes. Previous work has revealed the presence of social biases in widely used word embeddings involving gender, race, religion, and other social constructs. While some methods were proposed to debias these word-level embeddings, there is a need to perform debiasing at the sentence-level given the recent shift towards new contextualized sentence representations such as ELMo and BERT. In this paper, we investigate the presence of social biases in sentence-level representations and propose a new method, Sent-Debias, to reduce these biases. We show that Sent-Debias is effective in removing biases, and at the same time, preserves performance on sentence-level downstream tasks such as sentiment analysis, linguistic acceptability, and natural language understanding. We hope that our work will inspire future research on characterizing and removing social biases from widely adopted sentence representations for fairer NLP.","pages":"5502--5515","doi":"10.18653\/v1\/2020.acl-main.488","url":"https:\/\/www.aclweb.org\/anthology\/2020.acl-main.488","publisher":"Association for Computational Linguistics","address":"Online","year":"2020","month":"July","booktitle":"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"}}
{"bib_id":"saplexica","title":"Developing age and gender predictive lexica over social media","author":"Sap, Maarten and Park, Gregory and Eichstaedt, Johannes and Kern, Margaret and Stillwell, David and Kosinski, Michal and Ungar, Lyle and Schwartz, H Andrew","meta_info":{"year":"2014","pages":"1146--1151","booktitle":"Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)"}}
{"bib_id":"waseemandhovy","title":"Hateful symbols or hateful people? predictive features for hate speech detection on twitter","author":"Waseem, Zeerak and Hovy, Dirk","meta_info":{"year":"2016","pages":"88--93","booktitle":"Proceedings of the NAACL student research workshop"}}
{"bib_id":"davidsondataset","title":"Automated hate speech detection and the problem of offensive language","author":"Davidson, Thomas and Warmsley, Dana and Macy, Michael and Weber, Ingmar","meta_info":{"year":"2017","number":"1","volume":"11","booktitle":"Proceedings of the International AAAI Conference on Web and Social Media"}}
{"bib_id":"hatevaldataset","title":"Multilingual detection of hate speech against immigrants and women in Twitter at SemEval-2019 task 5: Frequency analysis interpolation for hate in speech detection","author":"i Orts, Òscar Garibo","meta_info":{"year":"2019","pages":"460--463","booktitle":"Proceedings of the 13th International Workshop on Semantic Evaluation"}}
{"bib_id":"wulczyndataset","title":"Ex machina: Personal attacks seen at scale","author":"Wulczyn, Ellery and Thain, Nithum and Dixon, Lucas","meta_info":{"year":"2017","pages":"1391--1399","booktitle":"Proceedings of the 26th international conference on world wide web"}}
{"bib_id":"ghcdataset","title":"The Gab Hate Corpus: A collection of 27k posts annotated for hate speech","author":"Kennedy, Brendan and Atari, Mohammad and Davani, Aida Mostafazadeh and Yeh, Leigh and Omrani, Ali and Kim, Yehsong and Coombs, Kris and Havaldar, Shreya and Portillo-Wightman, Gwenyth and Gonzalez, Elaine and others","meta_info":{"doi":"doi:10.31234\/osf.io\/hqjxn","journal":"PsyArXiv","year":"2018"}}
{"bib_id":"evalitadataset","title":"Overview of the evalita 2018 task on automatic misogyny identification (ami)","author":"Fersini, Elisabetta and Nozza, Debora and Rosso, Paolo","meta_info":{"year":"2018","pages":"59","volume":"12","journal":"EVALITA Evaluation of NLP and Speech Tools for Italian"}}
{"bib_id":"germeval2018","title":"Overview of the germeval 2018 shared task on the identification of offensive language","author":"Wiegand, Michael and Siegel, Melanie and Ruppenhofer, Josef","meta_info":{"year":"2018"}}
{"bib_id":"germeval2019","title":"Overview of germeval task 2, 2019 shared task on the identification of offensive language","author":"Struß, Julia Maria and Siegel, Melanie and Ruppenhofer, Josef and Wiegand, Michael and Klenner, Manfred and others","meta_info":{"publisher":"sa","year":"2019"}}
{"bib_id":"toxicspeechsurvey1","title":"A survey on hate speech detection using natural language processing","author":"Schmidt, Anna and Wiegand, Michael","meta_info":{"year":"2017","pages":"1--10","booktitle":"Proceedings of the fifth international workshop on natural language processing for social media"}}
{"bib_id":"toxicspeechsurvey2","title":"A survey on automatic detection of hate speech in text","author":"Fortuna, Paula and Nunes, Sérgio","meta_info":{"publisher":"ACM New York, NY, USA","year":"2018","pages":"1--30","number":"4","volume":"51","journal":"ACM Computing Surveys (CSUR)"}}
{"bib_id":"hatespeechelectoralviolence","title":"Hate speech and election violence in Nigeria","author":"Ezeibe, Christian","meta_info":{"publisher":"SAGE Publications Sage UK: London, England","year":"2020","pages":"0021909620951208","journal":"Journal of Asian and African Studies"}}
{"bib_id":"hatespeechviolence","title":"Spread of hate speech in online social media","author":"Mathew, Binny and Dutt, Ritam and Goyal, Pawan and Mukherjee, Animesh","meta_info":{"year":"2019","pages":"173--182","booktitle":"Proceedings of the 10th ACM conference on web science"}}
{"bib_id":"textualentailmentbias","title":"Investigating biases in textual entailment datasets","author":"Tan, Shawn and Shen, Yikang and Huang, Chin-wei and Courville, Aaron","meta_info":{"year":"2019","journal":"arXiv preprint arXiv:1906.09635"}}
{"bib_id":"readingcomprehensionbias","title":"On making reading comprehension more comprehensive","author":"Gardner, Matt and Berant, Jonathan and Hajishirzi, Hannaneh and Talmor, Alon and Min, Sewon","meta_info":{"year":"2019","pages":"105--112","booktitle":"Proceedings of the 2nd Workshop on Machine Reading for Question Answering"}}
{"bib_id":"genderbiasinnlpsurvey","title":"Mitigating gender bias in natural language processing: Literature review","author":"Sun, Tony and Gaut, Andrew and Tang, Shirlyn and Huang, Yuxin and ElSherief, Mai and Zhao, Jieyu and Mirza, Diba and Belding, Elizabeth and Chang, Kai-Wei and Wang, William Yang","meta_info":{"year":"2019","journal":"arXiv preprint arXiv:1906.08976"}}
{"bib_id":"kiela2020hateful","title":"The hateful memes challenge: Detecting hate speech in multimodal memes","author":"Kiela, Douwe and Firooz, Hamed and Mohan, Aravind and Goswami, Vedanuj and Singh, Amanpreet and Ringshia, Pratik and Testuggine, Davide","meta_info":{"year":"2020","journal":"arXiv preprint arXiv:2005.04790"}}
{"bib_id":"generalisable-hs-survey","title":"Towards generalisable hate speech detection: a review on obstacles and solutions","author":"Yin, Wenjie and Zubiaga, Arkaitz","meta_info":{"publisher":"PeerJ Inc.","year":"2021","pages":"e598","volume":"7","journal":"PeerJ Computer Science"}}
{"bib_id":"mitigating-implicit-bias-2021","title":"The Authors Matter: Understanding and Mitigating Implicit Bias in Deep Text Classification","author":"Liu, Haochen  and\nJin, Wei  and\nKarimi, Hamid  and\nLiu, Zitao  and\nTang, Jiliang","meta_info":{"pages":"74--85","doi":"10.18653\/v1\/2021.findings-acl.7","url":"https:\/\/aclanthology.org\/2021.findings-acl.7","publisher":"Association for Computational Linguistics","address":"Online","year":"2021","month":"August","booktitle":"Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021"}}
{"bib_id":"blodgett-bias-in-nlp-survey","title":"Language (Technology) is Power: A Critical Survey of “Bias” in NLP","author":"Blodgett, Su Lin and Barocas, Solon and Daumé III, Hal and Wallach, Hanna","meta_info":{"year":"2020","pages":"5454--5476","booktitle":"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"}}
{"bib_id":"GarridoMuoz2021","title":"A Survey on Bias in Deep NLP","author":"Ismael Garrido-Muñoz~ and Arturo Montejo-Ráez~ and Fernando Martínez-Santiago~ and L. Alfonso Ureña-López~","meta_info":{"journal":"Applied Sciences","pages":"3184","number":"7","volume":"11","publisher":"MDPI AG","month":"April","year":"2021","url":"https:\/\/doi.org\/10.3390\/app11073184","doi":"10.3390\/app11073184"}}
{"bib_id":"Greenwald1998","title":"Measuring individual differences in implicit cognition: The implicit association test.","author":"Anthony G. Greenwald and Debbie E. McGhee and Jordan L. K. Schwartz","meta_info":{"journal":"Journal of Personality and Social Psychology","pages":"1464--1480","number":"6","volume":"74","publisher":"American Psychological Association (APA)","year":"1998","url":"https:\/\/doi.org\/10.1037\/0022-3514.74.6.1464","doi":"10.1037\/0022-3514.74.6.1464"}}
{"bib_id":"may-etal-2019-measuring","title":"On Measuring Social Biases in Sentence Encoders","author":"May, Chandler  and\nWang, Alex  and\nBordia, Shikha  and\nBowman, Samuel R.  and\nRudinger, Rachel","meta_info":{"abstract":"The Word Embedding Association Test shows that GloVe and word2vec word embeddings exhibit human-like implicit biases based on gender, race, and other social constructs (Caliskan et al., 2017). Meanwhile, research on learning reusable text representations has begun to explore sentence-level texts, with some sentence encoders seeing enthusiastic adoption. Accordingly, we extend the Word Embedding Association Test to measure bias in sentence encoders. We then test several sentence encoders, including state-of-the-art methods such as ELMo and BERT, for the social biases studied in prior work and two important biases that are difficult or impossible to test at the word level. We observe mixed results including suspicious patterns of sensitivity that suggest the test′s assumptions may not hold in general. We conclude by proposing directions for future work on measuring bias in sentence encoders.","pages":"622--628","doi":"10.18653\/v1\/N19-1063","url":"https:\/\/aclanthology.org\/N19-1063","publisher":"Association for Computational Linguistics","address":"Minneapolis, Minnesota","year":"2019","month":"June","booktitle":"Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)"}}
{"bib_id":"Huszr2021","title":"Algorithmic amplification of politics on Twitter","author":"Ferenc Huszár and Sofia Ira Ktena and Conor O'Brien and Luca Belli and Andrew Schlaikjer and Moritz Hardt","meta_info":{"journal":"Proceedings of the National Academy of Sciences","pages":"e2025334119","number":"1","volume":"119","publisher":"Proceedings of the National Academy of Sciences","month":"December","year":"2021","url":"https:\/\/doi.org\/10.1073\/pnas.2025334119","doi":"10.1073\/pnas.2025334119"}}
{"bib_id":"Munn2020","title":"Angry by design: toxic communication and technical architectures","author":"Luke Munn","meta_info":{"journal":"Humanities and Social Sciences Communications","number":"1","volume":"7","publisher":"Springer Science and Business Media LLC","month":"July","year":"2020","url":"https:\/\/doi.org\/10.1057\/s41599-020-00550-7","doi":"10.1057\/s41599-020-00550-7"}}
{"bib_id":"Salminen2020","title":"Topic-driven toxicity: Exploring the relationship between online toxicity and news topics","author":"Joni Salminen and Sercan Sengün and Juan Corporan and Soon-gyo Jung and Bernard J. Jansen","meta_info":{"journal":"PLOS ONE","editor":"Pin-Yu Chen","pages":"e0228723","number":"2","volume":"15","publisher":"Public Library of Science (PLoS)","month":"February","year":"2020","url":"https:\/\/doi.org\/10.1371\/journal.pone.0228723","doi":"10.1371\/journal.pone.0228723"}}
{"bib_id":"ghosh-etal-2021-detecting","title":"Detecting Cross-Geographic Biases in Toxicity Modeling on Social Media","author":"Ghosh, Sayan  and\nBaker, Dylan  and\nJurgens, David  and\nPrabhakaran, Vinodkumar","meta_info":{"abstract":"Online social media platforms increasingly rely on Natural Language Processing (NLP) techniques to detect abusive content at scale in order to mitigate the harms it causes to their users. However, these techniques suffer from various sampling and association biases present in training data, often resulting in sub-par performance on content relevant to marginalized groups, potentially furthering disproportionate harms towards them. Studies on such biases so far have focused on only a handful of axes of disparities and subgroups that have annotations\/lexicons available. Consequently, biases concerning non-Western contexts are largely ignored in the literature. In this paper, we introduce a weakly supervised method to robustly detect lexical biases in broader geo-cultural contexts. Through a case study on a publicly available toxicity detection model, we demonstrate that our method identifies salient groups of cross-geographic errors, and, in a follow up, demonstrate that these groupings reflect human judgments of offensive and inoffensive language in those geographic contexts. We also conduct analysis of a model trained on a dataset with ground truth labels to better understand these biases, and present preliminary mitigation experiments.","pages":"313--328","doi":"10.18653\/v1\/2021.wnut-1.35","url":"https:\/\/aclanthology.org\/2021.wnut-1.35","publisher":"Association for Computational Linguistics","address":"Online","year":"2021","month":"November","booktitle":"Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)"}}
{"bib_id":"may-etal-2019-measuring","title":"On Measuring Social Biases in Sentence Encoders","author":"May, Chandler  and\nWang, Alex  and\nBordia, Shikha  and\nBowman, Samuel R.  and\nRudinger, Rachel","meta_info":{"abstract":"The Word Embedding Association Test shows that GloVe and word2vec word embeddings exhibit human-like implicit biases based on gender, race, and other social constructs (Caliskan et al., 2017). Meanwhile, research on learning reusable text representations has begun to explore sentence-level texts, with some sentence encoders seeing enthusiastic adoption. Accordingly, we extend the Word Embedding Association Test to measure bias in sentence encoders. We then test several sentence encoders, including state-of-the-art methods such as ELMo and BERT, for the social biases studied in prior work and two important biases that are difficult or impossible to test at the word level. We observe mixed results including suspicious patterns of sensitivity that suggest the test′s assumptions may not hold in general. We conclude by proposing directions for future work on measuring bias in sentence encoders.","pages":"622--628","doi":"10.18653\/v1\/N19-1063","url":"https:\/\/aclanthology.org\/N19-1063","publisher":"Association for Computational Linguistics","address":"Minneapolis, Minnesota","year":"2019","month":"June","booktitle":"Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)"}}
{"bib_id":"kim2020intersectional","title":"Intersectional Bias in Hate Speech and Abusive Language Datasets","author":"Jae Yeon Kim and Carlos Ortiz and Sarah Nam and Sarah Santiago and Vivek Datta","meta_info":{"primaryclass":"cs.CL","archiveprefix":"arXiv","eprint":"2005.05921","year":"2020"}}
{"bib_id":"rahman2021an","title":"An Information Retrieval Approach to Building Datasets for Hate Speech Detection","author":"Md Mustafizur Rahman and Dinesh Balakrishnan and Dhiraj Murthy and Mucahid Kutlu and Matthew Lease","meta_info":{"url":"https:\/\/openreview.net\/forum?id=jI_BbL-qjJN","year":"2021","booktitle":"Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)"}}
{"bib_id":"dev-etal-2021-harms","title":"Harms of Gender Exclusivity and Challenges in Non-Binary Representation in Language Technologies","author":"Dev, Sunipa  and\nMonajatipoor, Masoud  and\nOvalle, Anaelia  and\nSubramonian, Arjun  and\nPhillips, Jeff  and\nChang, Kai-Wei","meta_info":{"abstract":"Gender is widely discussed in the context of language tasks and when examining the stereotypes propagated by language models. However, current discussions primarily treat gender as binary, which can perpetuate harms such as the cyclical erasure of non-binary gender identities. These harms are driven by model and dataset biases, which are consequences of the non-recognition and lack of understanding of non-binary genders in society. In this paper, we explain the complexity of gender and language around it, and survey non-binary persons to understand harms associated with the treatment of gender as binary in English language technologies. We also detail how current language representations (e.g., GloVe, BERT) capture and perpetuate these harms and related challenges that need to be acknowledged and addressed for representations to equitably encode gender information.","pages":"1968--1994","doi":"10.18653\/v1\/2021.emnlp-main.150","url":"https:\/\/aclanthology.org\/2021.emnlp-main.150","publisher":"Association for Computational Linguistics","address":"Online and Punta Cana, Dominican Republic","year":"2021","month":"November","booktitle":"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing"}}
{"bib_id":"Steels2016","title":"Human language is a culturally evolving system","author":"Luc Steels","meta_info":{"journal":"Psychonomic Bulletin & Review","pages":"190--193","number":"1","volume":"24","publisher":"Springer Science and Business Media LLC","month":"July","year":"2016","url":"https:\/\/doi.org\/10.3758\/s13423-016-1086-6","doi":"10.3758\/s13423-016-1086-6"}}
{"bib_id":"qian-etal-2021-lifelong","title":"Lifelong Learning of Hate Speech Classification on Social Media","author":"Qian, Jing  and\nWang, Hong  and\nElSherief, Mai  and\nYan, Xifeng","meta_info":{"abstract":"Existing work on automated hate speech classification assumes that the dataset is fixed and the classes are pre-defined. However, the amount of data in social media increases every day, and the hot topics changes rapidly, requiring the classifiers to be able to continuously adapt to new data without forgetting the previously learned knowledge. This ability, referred to as lifelong learning, is crucial for the real-word application of hate speech classifiers in social media. In this work, we propose lifelong learning of hate speech classification on social media. To alleviate catastrophic forgetting, we propose to use Variational Representation Learning (VRL) along with a memory module based on LB-SOINN (Load-Balancing Self-Organizing Incremental Neural Network). Experimentally, we show that combining variational representation learning and the LB-SOINN memory module achieves better performance than the commonly-used lifelong learning techniques.","pages":"2304--2314","doi":"10.18653\/v1\/2021.naacl-main.183","url":"https:\/\/aclanthology.org\/2021.naacl-main.183","publisher":"Association for Computational Linguistics","address":"Online","year":"2021","month":"June","booktitle":"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"}}
{"bib_id":"Jin2020Towards","title":"Towards Hierarchical Importance Attribution: Explaining Compositional Semantics for Neural Sequence Models","author":"Xisen Jin and Zhongyu Wei and Junyi Du and Xiangyang Xue and Xiang Ren","meta_info":{"url":"https:\/\/openreview.net\/forum?id=BkxRRkSKwr","year":"2020","booktitle":"International Conference on Learning Representations"}}
{"bib_id":"pramanick-etal-2021-detecting","title":"Detecting Harmful Memes and Their Targets","author":"Pramanick, Shraman  and\nDimitrov, Dimitar  and\nMukherjee, Rituparna  and\nSharma, Shivam  and\nAkhtar, Md. Shad  and\nNakov, Preslav  and\nChakraborty, Tanmoy","meta_info":{"pages":"2783--2796","doi":"10.18653\/v1\/2021.findings-acl.246","url":"https:\/\/aclanthology.org\/2021.findings-acl.246","publisher":"Association for Computational Linguistics","address":"Online","year":"2021","month":"August","booktitle":"Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021"}}
{"bib_id":"pramanick-etal-2021-momenta-multimodal","title":"MOMENTA: A Multimodal Framework for Detecting Harmful Memes and Their Targets","author":"Pramanick, Shraman  and\nSharma, Shivam  and\nDimitrov, Dimitar  and\nAkhtar, Md. Shad  and\nNakov, Preslav  and\nChakraborty, Tanmoy","meta_info":{"pages":"4439--4455","url":"https:\/\/aclanthology.org\/2021.findings-emnlp.379","publisher":"Association for Computational Linguistics","address":"Punta Cana, Dominican Republic","year":"2021","month":"November","booktitle":"Findings of the Association for Computational Linguistics: EMNLP 2021"}}
{"bib_id":"chakraborty2022nipping","title":"Nipping in the Bud: Detection, Diffusion and Mitigation of Hate Speech on Social Media","author":"Tanmoy Chakraborty and Sarah Masud","meta_info":{"primaryclass":"cs.SI","archiveprefix":"arXiv","eprint":"2201.00961","year":"2022"}}
{"bib_id":"weidinger2021ethical","title":"Ethical and social risks of harm from Language Models","author":"Laura Weidinger and John Mellor and Maribeth Rauh and Conor Griffin and Jonathan Uesato and Po-Sen Huang and Myra Cheng and Mia Glaese and Borja Balle and Atoosa Kasirzadeh and Zac Kenton and Sasha Brown and Will Hawkins and Tom Stepleton and Courtney Biles and Abeba Birhane and Julia Haas and Laura Rimell and Lisa Anne Hendricks and William Isaac and Sean Legassick and Geoffrey Irving and Iason Gabriel","meta_info":{"primaryclass":"cs.CL","archiveprefix":"arXiv","eprint":"2112.04359","year":"2021"}}
{"bib_id":"rottger-etal-2022-two","title":"Two Contrasting Data Annotation Paradigms for Subjective NLP Tasks","author":"Rottger, Paul  and\nVidgen, Bertie  and\nHovy, Dirk  and\nPierrehumbert, Janet","meta_info":{"abstract":"Labelled data is the foundation of most natural language processing tasks. However, labelling data is difficult and there often are diverse valid beliefs about what the correct data labels should be. So far, dataset creators have acknowledged annotator subjectivity, but rarely actively managed it in the annotation process. This has led to partly-subjective datasets that fail to serve a clear downstream use. To address this issue, we propose two contrasting paradigms for data annotation. The descriptive paradigm encourages annotator subjectivity, whereas the prescriptive paradigm discourages it. Descriptive annotation allows for the surveying and modelling of different beliefs, whereas prescriptive annotation enables the training of models that consistently apply one belief. We discuss benefits and challenges in implementing both paradigms, and argue that dataset creators should explicitly aim for one or the other to facilitate the intended use of their dataset. Lastly, we conduct an annotation experiment using hate speech data that illustrates the contrast between the two paradigms.","pages":"175--190","doi":"10.18653\/v1\/2022.naacl-main.13","url":"https:\/\/aclanthology.org\/2022.naacl-main.13","publisher":"Association for Computational Linguistics","address":"Seattle, United States","year":"2022","month":"July","booktitle":"Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"}}
{"bib_id":"disagreement_deconvolution","title":"The Disagreement Deconvolution: Bringing Machine Learning Performance Metrics In Line With Reality","author":"Gordon, Mitchell L. and Zhou, Kaitlyn and Patel, Kayur and Hashimoto, Tatsunori and Bernstein, Michael S.","meta_info":{"series":"CHI '21","location":"Yokohama, Japan","numpages":"14","articleno":"388","booktitle":"Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems","abstract":"Machine learning classifiers for human-facing tasks such as comment toxicity and misinformation often score highly on metrics such as ROC AUC but are received poorly in practice. Why this gap? Today, metrics such as ROC AUC, precision, and recall are used to measure technical performance; however, human-computer interaction observes that evaluation of human-facing systems should account for people’s reactions to the system. In this paper, we introduce a transformation that more closely aligns machine learning classification metrics with the values and methods of user-facing performance measures. The disagreement deconvolution takes in any multi-annotator (e.g., crowdsourced) dataset, disentangles stable opinions from noise by estimating intra-annotator consistency, and compares each test set prediction to the individual stable opinions from each annotator. Applying the disagreement deconvolution to existing social computing datasets, we find that current metrics dramatically overstate the performance of many human-facing machine learning tasks: for example, performance on a comment toxicity task is corrected from .95 to .73 ROC AUC.","doi":"10.1145\/3411764.3445423","url":"https:\/\/doi.org\/10.1145\/3411764.3445423","address":"New York, NY, USA","publisher":"Association for Computing Machinery","isbn":"9781450380966","year":"2021"}}
{"bib_id":"Schimmack2021","title":"Invalid Claims About the Validity of Implicit Association Tests by Prisoners of the Implicit Social-Cognition Paradigm","author":"Ulrich Schimmack","meta_info":{"journal":"Perspectives on Psychological Science","pages":"435--442","number":"2","volume":"16","publisher":"SAGE Publications","month":"March","year":"2021","url":"https:\/\/doi.org\/10.1177\/1745691621991860","doi":"10.1177\/1745691621991860"}}
{"bib_id":"latent_hate","title":"Latent Hatred: A Benchmark for Understanding Implicit Hate Speech","author":"ElSherief, Mai  and\nZiems, Caleb  and\nMuchlinski, David  and\nAnupindi, Vaishnavi  and\nSeybolt, Jordyn  and\nDe Choudhury, Munmun  and\nYang, Diyi","meta_info":{"abstract":"Hate speech has grown significantly on social media, causing serious consequences for victims of all demographics. Despite much attention being paid to characterize and detect discriminatory speech, most work has focused on explicit or overt hate speech, failing to address a more pervasive form based on coded or indirect language. To fill this gap, this work introduces a theoretically-justified taxonomy of implicit hate speech and a benchmark corpus with fine-grained labels for each message and its implication. We present systematic analyses of our dataset using contemporary baselines to detect and explain implicit hate speech, and we discuss key features that challenge existing models. This dataset will continue to serve as a useful benchmark for understanding this multifaceted issue.","pages":"345--363","doi":"10.18653\/v1\/2021.emnlp-main.29","url":"https:\/\/aclanthology.org\/2021.emnlp-main.29","publisher":"Association for Computational Linguistics","address":"Online and Punta Cana, Dominican Republic","year":"2021","month":"November","booktitle":"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing"}}
{"bib_id":"toxigen","title":"ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection","author":"Hartvigsen, Thomas  and\nGabriel, Saadia  and\nPalangi, Hamid  and\nSap, Maarten  and\nRay, Dipankar  and\nKamar, Ece","meta_info":{"abstract":"Toxic language detection systems often falsely flag text that contains minority group mentions as toxic, as those groups are often the targets of online hate. Such over-reliance on spurious correlations also causes systems to struggle with detecting implicitly toxic language.To help mitigate these issues, we create ToxiGen, a new large-scale and machine-generated dataset of 274k toxic and benign statements about 13 minority groups. We develop a demonstration-based prompting framework and an adversarial classifier-in-the-loop decoding method to generate subtly toxic and benign text with a massive pretrained language model. Controlling machine generation in this way allows ToxiGen to cover implicitly toxic text at a larger scale, and about more demographic groups, than previous resources of human-written text. We conduct a human evaluation on a challenging subset of ToxiGen and find that annotators struggle to distinguish machine-generated text from human-written language. We also find that 94.5% of toxic examples are labeled as hate speech by human annotators. Using three publicly-available datasets, we show that finetuning a toxicity classifier on our data improves its performance on human-written data substantially. We also demonstrate that ToxiGen can be used to fight machine-generated toxicity as finetuning improves the classifier significantly on our evaluation subset.","pages":"3309--3326","doi":"10.18653\/v1\/2022.acl-long.234","url":"https:\/\/aclanthology.org\/2022.acl-long.234","publisher":"Association for Computational Linguistics","address":"Dublin, Ireland","year":"2022","month":"May","booktitle":"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"}}
{"bib_id":"Suler","title":"The Online Disinhibition Effect","author":"J. Suler","meta_info":{"pages":"\n321-326\n","volume":"7 3","year":"2004","journal":"Cyberpsychology & behavior : the impact of the Internet, multimedia and virtual reality on behavior and society"}}
{"bib_id":"agneta","title":"Why We Hate","author":"Agneta Fischer and Eran Halperin and Daphna Canetti and Alba Jasini","meta_info":{"year":"2018","pages":"309-320","number":"4","volume":"10","journal":"Emotion Review"}}
{"bib_id":"Navarro","title":"The Psychology of Hatred","author":"Jose I. Navarro","meta_info":{"journal":"The Open Criminology Journal","pages":"10--17","number":"1","volume":"6","publisher":"Bentham Science Publishers Ltd.","month":"April","year":"2013","url":"https:\/\/doi.org\/10.2174\/1874917801306010010","doi":"10.2174\/1874917801306010010"}}
{"bib_id":"10.1145\/3419249.3420142","title":"Four Types of Toxic People: Characterizing Online Users’ Toxicity over Time","author":"Mall, Raghvendra and Nagpal, Mridul and Salminen, Joni and Almerekhi, Hind and Jung, Soon-Gyo and Jansen, Bernard J.","meta_info":{"series":"NordiCHI '20","location":"Tallinn, Estonia","keywords":"social media behavior, online toxicity, user analysis, Reddit","numpages":"11","articleno":"37","booktitle":"Proceedings of the 11th Nordic Conference on Human-Computer Interaction: Shaping Experiences, Shaping Society","abstract":"Identifying types of online users’ toxic behavior reveals important insights from social media interactions, including whether a user becomes “radicalized” (more toxic) or “pacified” (less toxic) over time. In this research, we design two metrics to identify toxic user types: F score that captures the changes in a user’s toxicity, and G score that captures the direction of the shift taking place in the user’s toxicity pattern. We apply these metrics to a dataset of 4M user comments from Reddit by defining four toxic user types based on the toxicity scores of a user’s comments: (a) Steady Users whose toxicity scores are steady over time, (b) Fickle-Minded Users that switch between toxic and non-toxic commenting, (c) Pacified Users whose commenting becomes less toxic in time, and (d) Radicalized Users that become gradually toxic. Findings from the Reddit dataset indicate that fickle-minded users form the largest group (31.2%), followed by pacified (25.8%), radicalized (25.4%), and steadily toxic users (17.6%). The results suggest that the most typical behavior type of toxicity is switching between toxic and non-toxic commenting. This research has implications for preserving the user-friendliness of online communities by identifying continuously toxic users and users in danger of becoming radicalized (in terms of their toxic behavior), and designing interventions to mitigate these behavior types. Using the metrics we have defined, identifying these user types becomes possible. More research is needed to understand why these patterns take place and how they could be mitigated.","doi":"10.1145\/3419249.3420142","url":"https:\/\/doi.org\/10.1145\/3419249.3420142","address":"New York, NY, USA","publisher":"Association for Computing Machinery","isbn":"9781450375795","year":"2020"}}
{"bib_id":"doi:10.1080\/17440572.2019.1591952","title":"Denunciation and doxing: towards a conceptual model of digital vigilantism","author":"Daniel Trottier","meta_info":{"eprint":" \nhttps:\/\/doi.org\/10.1080\/17440572.2019.1591952","url":" \nhttps:\/\/doi.org\/10.1080\/17440572.2019.1591952","doi":"10.1080\/17440572.2019.1591952","publisher":"Routledge","year":"2020","pages":"196-212","number":"3-4","volume":"21","journal":"Global Crime"}}
{"bib_id":"saha2019prevalence","title":"Prevalence and Psychological Effects of Hateful Speech in Online College Communities","author":"Saha, Koustuv and Chandrasekharan, Eshwar and De Choudhury, Munmun","meta_info":{"series":"WebSci '19","location":"Boston, Massachusetts, USA","keywords":"natural language analysis, college subreddits, hateful speech, stress, psychological endurance, reddit, social media, mental health","numpages":"10","pages":"255–264","booktitle":"Proceedings of the 10th ACM Conference on Web Science","abstract":"Background. Hateful speech bears negative repercussions and is particularly damaging in college communities. The efforts to regulate hateful speech on college campuses pose vexing socio-political problems, and the interventions to mitigate the effects require evaluating the pervasiveness of the phenomenon on campuses as well the impacts on students' psychological state. Data and Methods. Given the growing use of social media among college students, we target the above issues by studying the online aspect of hateful speech in a dataset of 6 million Reddit comments shared in 174 college communities. To quantify the prevelence of hateful speech in an online college community, we devise College Hate Index (CHX). Next, we examine its distribution across the categories of hateful speech,behavior, class, disability, ethnicity, gender, physical appearance, race, religion, andsexual orientation. We then employ a causal-inference framework to study the psychological effects of hateful speech, particularly in the form of individuals' online stress expression. Finally, we characterize their psychological endurance to hateful speech by analyzing their language -- their discriminatory keyword use, and their personality traits. Results. We find that hateful speech is prevalent in college subreddits, and 25% of them show greater hateful speech than non-college subreddits. We also find that the exposure to hate leads to greater stress expression. However, everybody exposed is not equally affected; some show lower psychological endurance than others. Low endurance individuals are more vulnerable to emotional outbursts, and are more neurotic than those with higher endurance. Discussion. Our work bears implications for policy-making and intervention efforts to tackle the damaging effects of online hateful speech in colleges. From technological perspective, our work caters to mental health support provisions on college campuses, and to moderation efforts in online college communities. In addition, given the charged aspect of speech dilemma, we highlight the ethical implications of our work. Our work lays the foundation for studying the psychological impacts of hateful speech in online communities in general, and situated communities in particular (the ones that have both an offline and an online analog).","doi":"10.1145\/3292522.3326032","url":"https:\/\/doi.org\/10.1145\/3292522.3326032","address":"New York, NY, USA","publisher":"Association for Computing Machinery","isbn":"9781450362023","year":"2019"}}
{"bib_id":"doi:10.1089\/cyber.2022.0009","title":"Online Hate Speech Victimization and Depressive Symptoms Among Adolescents: The Protective Role of Resilience","author":"Wachs, Sebastian and Gámez-Guadix, Manuel and Wright, Michelle F.","meta_info":{"abstract":" Online hate speech has become a widespread problem in the daily life of adolescents. Despite growing societal and academic interest in this online risk, not much is known about the relationship between online hate speech victimization (OHSV) and adolescents' mental well-being. In addition, potential factors influencing the magnitude of this relationship remain unclear. To address these gaps in the literature, this study investigated the relationship between OHSV and depressive symptoms and the buffering effects of resilience in this relationship. The sample consists of 1,632 adolescents (49.1% girls) between 12 and 18 years old (Mage = 13.83, SDage = 1.23), recruited from nine schools across Spain. Self-report questionnaires were administered to assess OHSV, depressive symptoms, and resilience. Regression analyses revealed that OHSV was positively linked to depressive symptoms. In addition, victims of online hate speech were less likely to report depressive symptoms when they reported average or high levels of resilience (i.e., social competence, personal competence, structured style, social resources, and family cohesion) compared with those with low levels of resilience. Our findings highlight the need for the development of intervention programs and the relevance of focusing on internal and external developmental assets to mitigate negative outcomes for victims of online hate speech. ","eprint":" \nhttps:\/\/doi.org\/10.1089\/cyber.2022.0009\n","url":" \nhttps:\/\/doi.org\/10.1089\/cyber.2022.0009\n","note":"PMID: 35639126","doi":"10.1089\/cyber.2022.0009","year":"2022","pages":"416-423","number":"7","volume":"25","journal":"Cyberpsychology, Behavior, and Social Networking"}}
{"bib_id":"pmlr-v81-buolamwini18a","title":"Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification","author":"Buolamwini, Joy and Gebru, Timnit","meta_info":{"abstract":"Recent studies demonstrate that machine learning algorithms can discriminate based on classes like race and gender. In this work, we present an approach to evaluate bias present in automated facial analysis algorithms and datasets with respect to phenotypic subgroups. Using the dermatologist  approved Fitzpatrick Skin Type classification system, we characterize the gender and skin type distribution of two facial analysis benchmarks, IJB-A and Adience. We find that these datasets are overwhelmingly composed of lighter-skinned subjects (79.6% for IJB-A and 86.2% for Adience) and introduce a new facial analysis dataset which is balanced by gender and skin type. We evaluate 3 commercial gender classification systems using our dataset and show that darker-skinned females are the most misclassified group (with error rates of up to 34.7%). The maximum error rate for lighter-skinned males is 0.8%. The substantial disparities in the accuracy of classifying darker females, lighter females, darker males, and lighter males in gender classification systems require urgent attention if commercial companies are to build genuinely fair, transparent and accountable facial analysis algorithms.","url":"https:\/\/proceedings.mlr.press\/v81\/buolamwini18a.html","pdf":"http:\/\/proceedings.mlr.press\/v81\/buolamwini18a\/buolamwini18a.pdf","publisher":"PMLR","month":"23--24 Feb","series":"Proceedings of Machine Learning Research","volume":"81","editor":"Friedler, Sorelle A. and Wilson, Christo","year":"2018","pages":"77--91","booktitle":"Proceedings of the 1st Conference on Fairness, Accountability and Transparency"}}
{"bib_id":"tatman-2017-gender","title":"Gender and Dialect Bias in YouTube′s Automatic Captions","author":"Tatman, Rachael","meta_info":{"abstract":"This project evaluates the accuracy of YouTube′s automatically-generated captions across two genders and five dialect groups. Speakers′ dialect and gender was controlled for by using videos uploaded as part of the ``accent tag challenge″, where speakers explicitly identify their language background. The results show robust differences in accuracy across both gender and dialect, with lower accuracy for 1) women and 2) speakers from Scotland. This finding builds on earlier research finding that speaker′s sociolinguistic identity may negatively impact their ability to use automatic speech recognition, and demonstrates the need for sociolinguistically-stratified validation of systems.","pages":"53--59","doi":"10.18653\/v1\/W17-1606","url":"https:\/\/aclanthology.org\/W17-1606","publisher":"Association for Computational Linguistics","address":"Valencia, Spain","year":"2017","month":"April","booktitle":"Proceedings of the First ACL Workshop on Ethics in Natural Language Processing"}}
{"bib_id":"doi:10.1126\/science.aal4230","title":"Semantics derived automatically from language corpora contain human-like biases","author":"Aylin Caliskan  and Joanna J. Bryson  and Arvind Narayanan ","meta_info":{"eprint":"https:\/\/www.science.org\/doi\/pdf\/10.1126\/science.aal4230","url":"https:\/\/www.science.org\/doi\/abs\/10.1126\/science.aal4230","doi":"10.1126\/science.aal4230","year":"2017","pages":"183-186","number":"6334","volume":"356","journal":"Science"}}
{"bib_id":"sun-etal-2019-mitigating","title":"Mitigating Gender Bias in Natural Language Processing: Literature Review","author":"Sun, Tony  and\nGaut, Andrew  and\nTang, Shirlyn  and\nHuang, Yuxin  and\nElSherief, Mai  and\nZhao, Jieyu  and\nMirza, Diba  and\nBelding, Elizabeth  and\nChang, Kai-Wei  and\nWang, William Yang","meta_info":{"abstract":"As Natural Language Processing (NLP) and Machine Learning (ML) tools rise in popularity, it becomes increasingly vital to recognize the role they play in shaping societal biases and stereotypes. Although NLP models have shown success in modeling various applications, they propagate and may even amplify gender bias found in text corpora. While the study of bias in artificial intelligence is not new, methods to mitigate gender bias in NLP are relatively nascent. In this paper, we review contemporary studies on recognizing and mitigating gender bias in NLP. We discuss gender bias based on four forms of representation bias and analyze methods recognizing gender bias. Furthermore, we discuss the advantages and drawbacks of existing gender debiasing methods. Finally, we discuss future studies for recognizing and mitigating gender bias in NLP.","pages":"1630--1640","doi":"10.18653\/v1\/P19-1159","url":"https:\/\/aclanthology.org\/P19-1159","publisher":"Association for Computational Linguistics","address":"Florence, Italy","year":"2019","month":"July","booktitle":"Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"}}
{"bib_id":"zhang-etal-2020-demographics","title":"Demographics Should Not Be the Reason of Toxicity: Mitigating Discrimination in Text Classifications with Instance Weighting","author":"Zhang, Guanhua  and\nBai, Bing  and\nZhang, Junqi  and\nBai, Kun  and\nZhu, Conghui  and\nZhao, Tiejun","meta_info":{"abstract":"With the recent proliferation of the use of text classifications, researchers have found that there are certain unintended biases in text classification datasets. For example, texts containing some demographic identity-terms (e.g., ``gay″, ``black″) are more likely to be abusive in existing abusive language detection datasets. As a result, models trained with these datasets may consider sentences like ``She makes me happy to be gay″ as abusive simply because of the word ``gay.″ In this paper, we formalize the unintended biases in text classification datasets as a kind of selection bias from the non-discrimination distribution to the discrimination distribution. Based on this formalization, we further propose a model-agnostic debiasing training framework by recovering the non-discrimination distribution using instance weighting, which does not require any extra resources or annotations apart from a pre-defined set of demographic identity-terms. Experiments demonstrate that our method can effectively alleviate the impacts of the unintended biases without significantly hurting models′ generalization ability.","pages":"4134--4145","doi":"10.18653\/v1\/2020.acl-main.380","url":"https:\/\/aclanthology.org\/2020.acl-main.380","publisher":"Association for Computational Linguistics","address":"Online","year":"2020","month":"July","booktitle":"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"}}
{"bib_id":"van-der-wal-etal-2022-birth","title":"The Birth of Bias: A case study on the evolution of gender bias in an English language model","author":"Van Der Wal, Oskar  and\nJumelet, Jaap  and\nSchulz, Katrin  and\nZuidema, Willem","meta_info":{"abstract":"Detecting and mitigating harmful biases in modern language models are widely recognized as crucial, open problems. In this paper, we take a step back and investigate how language models come to be biased in the first place.We use a relatively small language model, using the LSTM architecture trained on an English Wikipedia corpus. With full access to the data and to the model parameters as they change during every step while training, we can map in detail how the representation of gender develops, what patterns in the dataset drive this, and how the model′s internal state relates to the bias in a downstream task (semantic textual similarity).We find that the representation of gender is dynamic and identify different phases during training.Furthermore, we show that gender information is represented increasingly locally in the input embeddings of the model and that, as a consequence, debiasing these can be effective in reducing the downstream bias.Monitoring the training dynamics, allows us to detect an asymmetry in how the female and male gender are represented in the input embeddings. This is important, as it may cause naive mitigation strategies to introduce new undesirable biases.We discuss the relevance of the findings for mitigation strategies more generally and the prospects of generalizing our methods to larger language models, the Transformer architecture, other languages and other undesirable biases.","pages":"75--75","doi":"10.18653\/v1\/2022.gebnlp-1.8","url":"https:\/\/aclanthology.org\/2022.gebnlp-1.8","publisher":"Association for Computational Linguistics","address":"Seattle, Washington","year":"2022","month":"July","booktitle":"Proceedings of the 4th Workshop on Gender Bias in Natural Language Processing (GeBNLP)"}}
{"bib_id":"10.1145\/3461702.3462557","title":"Measuring Model Biases in the Absence of Ground Truth","author":"Aka, Osman and Burke, Ken and Bauerle, Alex and Greer, Christina and Mitchell, Margaret","meta_info":{"series":"AIES '21","location":"Virtual Event, USA","keywords":"datasets, fairness, image tagging, stereotypes, information extraction, model analysis, bias","numpages":"9","pages":"327–335","booktitle":"Proceedings of the 2021 AAAI\/ACM Conference on AI, Ethics, and Society","abstract":"The measurement of bias in machine learning often focuses on model performance across identity subgroups (such as man and woman) with respect to groundtruth labels. However, these methods do not directly measure the associations that a model may have learned, for example between labels and identity subgroups. Further, measuring a model's bias requires a fully annotated evaluation dataset which may not be easily available in practice.We present an elegant mathematical solution that tackles both issues simultaneously, using image classification as a working example. By treating a classification model's predictions for a given image as a set of labels analogous to a \"bag of words\", we rank the biases that a model has learned with respect to different identity labels. We use man, woman as a concrete example of an identity label set (although this set need not be binary), and present rankings for the labels that are most biased towards one identity or the other. We demonstrate how the statistical properties of different association metrics can lead to different rankings of the most \"gender biased\" labels, and conclude that normalized pointwise mutual information (nPMI) is most useful in practice. Finally, we announce an open-sourced nPMI visualization tool using TensorBoard.","doi":"10.1145\/3461702.3462557","url":"https:\/\/doi.org\/10.1145\/3461702.3462557","address":"New York, NY, USA","publisher":"Association for Computing Machinery","isbn":"9781450384735","year":"2021"}}
{"bib_id":"golbeck_harrase","title":"A Large Labeled Corpus for Online Harassment Research","author":"Golbeck, Jennifer and Ashktorab, Zahra and Banjo, Rashad O. and Berlinger, Alexandra and Bhagwan, Siddharth and Buntain, Cody and Cheakalos, Paul and Geller, Alicia A. and Gergory, Quint and Gnanasekaran, Rajesh Kumar and Gunasekaran, Raja Rajan and Hoffman, Kelly M. and Hottle, Jenny and Jienjitlert, Vichita and Khare, Shivika and Lau, Ryan and Martindale, Marianna J. and Naik, Shalmali and Nixon, Heather L. and Ramachandran, Piyush and Rogers, Kristine M. and Rogers, Lisa and Sarin, Meghna Sardana and Shahane, Gaurav and Thanki, Jayanee and Vengataraman, Priyanka and Wan, Zijian and Wu, Derek Michael","meta_info":{"series":"WebSci '17","location":"Troy, New York, USA","keywords":"online harassment, datasets","numpages":"5","pages":"229–233","booktitle":"Proceedings of the 2017 ACM on Web Science Conference","abstract":"A fundamental part of conducting cross-disciplinary web science research is having useful, high-quality datasets that provide value to studies across disciplines. In this paper, we introduce a large, hand-coded corpus of online harassment data. A team of researchers collaboratively developed a codebook using grounded theory and labeled 35,000 tweets. Our resulting dataset has roughly 15% positive harassment examples and 85% negative examples. This data is useful for training machine learning models, identifying textual and linguistic features of online harassment, and for studying the nature of harassing comments and the culture of trolling.","doi":"10.1145\/3091478.3091509","url":"https:\/\/doi.org\/10.1145\/3091478.3091509","address":"New York, NY, USA","publisher":"Association for Computing Machinery","isbn":"9781450348966","year":"2017"}}
{"bib_id":"Sarwar_Murdock_2022","title":"Unsupervised Domain Adaptation for Hate Speech Detection Using a Data Augmentation Approach","author":"Sarwar, Sheikh Muhammad and Murdock, Vanessa","meta_info":{"pages":"852-862","month":"May","year":"2022","journal":"Proceedings of the International AAAI Conference on Web and Social Media","number":"1","abstractnote":"Online harassment in the form of hate speech has been on the rise in recent years. Addressing the issue requires a combination of content moderation by people, aided by automatic detection methods. As content moderation is itself harmful to the people doing it, we desire to reduce the burden by improving the automatic detection of hate speech. Hate speech presents a challenge as it is directed at different target groups using a completely different vocabulary. Further the authors of the hate speech are incentivized to disguise their behavior to avoid being removed from a platform. This makes it difficult to develop a comprehensive data set for training and evaluating hate speech detection models because the examples that represent one hate speech domain do not typically represent others, even within the same language or culture. We propose an unsupervised domain adaptation approach to augment labeled data for hate speech detection. We evaluate the approach with three different models (character CNNs, BiLSTMs and BERT) on three different collections. We show our approach improves Area under the Precision\/Recall curve by as much as 42% and recall by as much as 278%, with no loss (and in some cases a significant gain) in precision.","url":"https:\/\/ojs.aaai.org\/index.php\/ICWSM\/article\/view\/19340","volume":"16"}}
{"bib_id":"ludwig-etal-2022-improving","title":"Improving Generalization of Hate Speech Detection Systems to Novel Target Groups via Domain Adaptation","author":"Ludwig, Florian  and\nDolos, Klara  and\nZesch, Torsten  and\nHobley, Eleanor","meta_info":{"abstract":"Despite recent advances in machine learning based hate speech detection, classifiers still struggle with generalizing knowledge to out-of-domain data samples. In this paper, we investigate the generalization capabilities of deep learning models to different target groups of hate speech under clean experimental settings. Furthermore, we assess the efficacy of three different strategies of unsupervised domain adaptation to improve these capabilities. Given the diversity of hate and its rapid dynamics in the online world (e.g. the evolution of new target groups like virologists during the COVID-19 pandemic), robustly detecting hate aimed at newly identified target groups is a highly relevant research question. We show that naively trained models suffer from a target group specific bias, which can be reduced via domain adaptation. We were able to achieve a relative improvement of the F1-score between 5.8% and 10.7% for out-of-domain target groups of hate speech compared to baseline approaches by utilizing domain adaptation.","pages":"29--39","doi":"10.18653\/v1\/2022.woah-1.4","url":"https:\/\/aclanthology.org\/2022.woah-1.4","publisher":"Association for Computational Linguistics","address":"Seattle, Washington (Hybrid)","year":"2022","month":"July","booktitle":"Proceedings of the Sixth Workshop on Online Abuse and Harms (WOAH)"}}
{"bib_id":"Haselton2015","title":"The Evolution of Cognitive Bias","author":"Martie G. Haselton and Daniel Nettle and Paul W. Andrews","meta_info":{"booktitle":"The Handbook of Evolutionary Psychology","pages":"724--746","publisher":"John Wiley & Sons,  Inc.","month":"September","year":"2015","url":"https:\/\/doi.org\/10.1002\/9780470939376.ch25","doi":"10.1002\/9780470939376.ch25"}}
{"bib_id":"10.1145\/3383313.3418435","title":"Bias in Search and Recommender Systems","author":"Baeza-Yates, Ricardo","meta_info":{"series":"RecSys '20","location":"Virtual Event, Brazil","numpages":"1","pages":"2","booktitle":"Proceedings of the 14th ACM Conference on Recommender Systems","abstract":"We explore the vicious cycle of bias on the Web related to search and recommender systems. The first bias is activity bias [1], called by Nielsen participation inequality in Internet [3]. This means that when sampling content, data will have many different types of bias such as gender, language, topic, etc. During the design and implementation of the system, biases might be added, and we call those the true algorithmic bias. In addition, when evaluating the system we may have been biased and hence that may also be reflected in algorithmic bias. The design of the interaction also matters, adding several other biases. All of them affect the data that is gathered for optimizing and personalizing the system [1]. Finally, our own cognitive biases also taint the interaction data, including confirmation bias and other behavioral biases. In all these cases we may need to debias the input data, to use techniques that can handle biased data (e.g., machine learning algorithms tailored for that), or debias the output (when we have already lost information). Most systems are optimized by using implicit user feedback, that is, clicks or other trackable user interactions. However, those interactions are biased to the choices that such systems offer to their users, as clicks can only be done on things that are shown to us. Hence, these feedback loops are tainted by presentation or exposure bias [1]. The most well-known problem related to this bias is called the filter bubble [4] or the echo chamber effect. Solutions to this problem include the explore and exploit paradigm (e.g., learning the world), diversity, novelty, and serendipity. Depending on the exploration technique, the bubble is smaller or bigger [2]. On the other hand, too much exploration also reduces short-term revenue and hence is usually bounded. However, we believe that recommender systems could improve their long-term revenue if significantly more exploration is performed, probably diminishing at the same time the tension between user experience and monetization. This is good for the recommender system but also creates more fair and healthy digital markets for everyone, users and publishers\/sellers.","doi":"10.1145\/3383313.3418435","url":"https:\/\/doi.org\/10.1145\/3383313.3418435","address":"New York, NY, USA","publisher":"Association for Computing Machinery","isbn":"9781450375832","year":"2020"}}
{"bib_id":"shah-etal-2020-predictive","title":"Predictive Biases in Natural Language Processing Models: A Conceptual Framework and Overview","author":"Shah, Deven Santosh  and\nSchwartz, H. Andrew  and\nHovy, Dirk","meta_info":{"abstract":"An increasing number of natural language processing papers address the effect of bias on predictions, introducing mitigation techniques at different parts of the standard NLP pipeline (data and models). However, these works have been conducted individually, without a unifying framework to organize efforts within the field. This situation leads to repetitive approaches, and focuses overly on bias symptoms\/effects, rather than on their origins, which could limit the development of effective countermeasures. In this paper, we propose a unifying predictive bias framework for NLP. We summarize the NLP literature and suggest general mathematical definitions of predictive bias. We differentiate two consequences of bias: outcome disparities and error disparities, as well as four potential origins of biases: label bias, selection bias, model overamplification, and semantic bias. Our framework serves as an overview of predictive bias in NLP, integrating existing work into a single structure, and providing a conceptual baseline for improved frameworks.","pages":"5248--5264","doi":"10.18653\/v1\/2020.acl-main.468","url":"https:\/\/aclanthology.org\/2020.acl-main.468","publisher":"Association for Computational Linguistics","address":"Online","year":"2020","month":"July","booktitle":"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"}}
{"bib_id":"b-etal-2021-overview","title":"An Overview of Fairness in Data -- Illuminating the Bias in Data Pipeline","author":"Kumar, Senthil   and\nChandrabose, Aravindan  and\nChakravarthi, Bharathi Raja","meta_info":{"abstract":"Data in general encodes human biases by default; being aware of this is a good start, and the research around how to handle it is ongoing. The term `bias′ is extensively used in various contexts in NLP systems. In our research the focus is specific to biases such as gender, racism, religion, demographic and other intersectional views on biases that prevail in text processing systems responsible for systematically discriminating specific population, which is not ethical in NLP. These biases exacerbate the lack of equality, diversity and inclusion of specific population while utilizing the NLP applications. The tools and technology at the intermediate level utilize biased data, and transfer or amplify this bias to the downstream applications. However, it is not enough to be colourblind, gender-neutral alone when designing a unbiased technology -- instead, we should take a conscious effort by designing a unified framework to measure and benchmark the bias. In this paper, we recommend six measures and one augment measure based on the observations of the bias in data, annotations, text representations and debiasing techniques.","pages":"34--45","url":"https:\/\/aclanthology.org\/2021.ltedi-1.5","publisher":"Association for Computational Linguistics","address":"Kyiv","year":"2021","month":"April","booktitle":"Proceedings of the First Workshop on Language Technology for Equality, Diversity and Inclusion"}}
{"bib_id":"eisenstein-etal-2011-discovering","title":"Discovering Sociolinguistic Associations with Structured Sparsity","author":"Eisenstein, Jacob  and\nSmith, Noah A.  and\nXing, Eric P.","meta_info":{"pages":"1365--1374","url":"https:\/\/aclanthology.org\/P11-1137","publisher":"Association for Computational Linguistics","address":"Portland, Oregon, USA","year":"2011","month":"June","booktitle":"Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies"}}
