# Copyright (c) 2025 Bytedance Ltd. and/or its affiliates
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import json
import os
from pathlib import Path
from typing import Dict, Any
from datetime import datetime
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from tqdm import tqdm

class FileLogger:
    """ä¸ºæ¯ä¸ªæ–‡ä»¶åˆ›å»ºç‹¬ç«‹çš„æ—¥å¿—è®°å½•å™¨"""
    
    def __init__(self, log_dir: str = "logs"):
        self.log_dir = log_dir
        os.makedirs(log_dir, exist_ok=True)
        self._loggers = {}
        self._lock = threading.Lock()
    
    def get_logger(self, file_path: str):
        """è·å–æŒ‡å®šæ–‡ä»¶çš„æ—¥å¿—è®°å½•å™¨"""
        base_name = os.path.splitext(os.path.basename(file_path))[0]
        log_file = os.path.join(self.log_dir, f"{base_name}.log")
        
        with self._lock:
            if log_file not in self._loggers:
                self._loggers[log_file] = open(log_file, 'w', encoding='utf-8')
                # å†™å…¥æ—¥å¿—å¤´éƒ¨ä¿¡æ¯
                self._loggers[log_file].write(f"Processing log for: {os.path.basename(file_path)}\n")
                self._loggers[log_file].write(f"Started at: {datetime.now().isoformat()}\n")
                self._loggers[log_file].write("=" * 60 + "\n\n")
                self._loggers[log_file].flush()
            
            return self._loggers[log_file]
    
    def log(self, file_path: str, message: str):
        """è®°å½•æ—¥å¿—æ¶ˆæ¯"""
        logger = self.get_logger(file_path)
        timestamp = datetime.now().strftime("%H:%M:%S")
        logger.write(f"[{timestamp}] {message}\n")
        logger.flush()
    
    def close_all(self):
        """å…³é—­æ‰€æœ‰æ—¥å¿—æ–‡ä»¶"""
        with self._lock:
            for logger in self._loggers.values():
                logger.close()
            self._loggers.clear()

# å…¨å±€æ—¥å¿—è®°å½•å™¨
file_logger = FileLogger()

def clean_markdown_block(content: str) -> str:
    """æ¸…ç†markdownå†…å®¹ä¸­çš„ä»£ç å—æ ‡è®°"""
    if not content:
        return content
    
    # å»æ‰å‰é¢çš„markdownä»£ç å—æ ‡è®°
    content = re.sub(r'^```(?:markdown|md)?\s*\n?', '', content, flags=re.IGNORECASE | re.MULTILINE)
    
    # å»æ‰åé¢çš„ä»£ç å—ç»“æŸæ ‡è®°
    content = re.sub(r'\n?```\s*$', '', content, flags=re.MULTILINE)
    
    # æ¸…ç†å‰åå¤šä½™çš„ç©ºæ ¼å’Œæ¢è¡Œ
    content = content.strip()
    
    return content

def split_html_by_headers(html_content: str, max_chunk_size: int, log=None) -> list:
    """
    æŒ‰HTMLæ ‡é¢˜ï¼ˆh1, h2, h3ï¼‰åˆ†å—ï¼Œç¡®ä¿æ¯ä¸ªå—éƒ½ä»¥æ ‡é¢˜å¼€å¤´
    
    Args:
        html_content: åŸå§‹HTMLå†…å®¹
        max_chunk_size: æœ€å¤§å—å¤§å°
        log: æ—¥å¿—å‡½æ•°
        
    Returns:
        list: åˆ†å—åçš„HTMLåˆ—è¡¨
    """
    if not html_content or len(html_content) <= max_chunk_size:
        return [html_content]
    
    # æŸ¥æ‰¾æ‰€æœ‰çš„h1, h2, h3æ ‡é¢˜æ ‡ç­¾ä½ç½®
    header_pattern = r'(<h[123][^>]*>)'
    header_matches = list(re.finditer(header_pattern, html_content, re.IGNORECASE))
    
    if not header_matches:
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡é¢˜ï¼Œå›é€€åˆ°å•å—
        if log:
            log("    âš ï¸ æœªæ‰¾åˆ°æ ‡é¢˜æ ‡ç­¾ï¼Œå›é€€åˆ°å•å—å¤„ç†")
        return [html_content]
    
    chunks = []
    start_pos = 0
    current_chunk = ""
    
    for i, match in enumerate(header_matches):
        header_pos = match.start()
        
        # å¦‚æœè¿™æ˜¯ç¬¬ä¸€ä¸ªæ ‡é¢˜ï¼Œä¸”å‰é¢æœ‰å†…å®¹ï¼Œå…ˆå¤„ç†å‰é¢çš„å†…å®¹
        if i == 0 and header_pos > 0:
            prefix_content = html_content[start_pos:header_pos]
            if prefix_content.strip():
                # å¦‚æœå‰ç¼€å†…å®¹å¾ˆé•¿ï¼Œå•ç‹¬æˆå—
                if len(prefix_content) > max_chunk_size:
                    chunks.append(prefix_content)
                else:
                    current_chunk = prefix_content
            start_pos = header_pos
        
        # è®¡ç®—åˆ°ä¸‹ä¸€ä¸ªæ ‡é¢˜çš„å†…å®¹
        if i < len(header_matches) - 1:
            next_header_pos = header_matches[i + 1].start()
            section_content = html_content[start_pos:next_header_pos]
        else:
            # æœ€åä¸€ä¸ªæ ‡é¢˜ï¼Œå–åˆ°æ–‡ä»¶ç»“å°¾
            section_content = html_content[start_pos:]
        
        # æ£€æŸ¥å½“å‰å—åŠ ä¸Šè¿™ä¸ªsectionæ˜¯å¦è¶…å‡ºå¤§å°é™åˆ¶
        if current_chunk and len(current_chunk + section_content) > max_chunk_size:
            # å½“å‰å—å·²ç»æ»¡äº†ï¼Œå…ˆä¿å­˜ï¼Œç„¶åå¼€å§‹æ–°å—
            chunks.append(current_chunk)
            current_chunk = section_content
        else:
            # æ·»åŠ åˆ°å½“å‰å—
            current_chunk += section_content
        
        # å¦‚æœå½“å‰sectionæœ¬èº«å°±è¶…è¿‡å¤§å°é™åˆ¶ï¼Œå¼ºåˆ¶åˆ†å—
        if len(current_chunk) > max_chunk_size:
            chunks.append(current_chunk)
            current_chunk = ""
        
        # æ›´æ–°ä¸‹ä¸€æ¬¡çš„èµ·å§‹ä½ç½®
        if i < len(header_matches) - 1:
            start_pos = header_matches[i + 1].start()
    
    # æ·»åŠ æœ€åä¸€ä¸ªå—ï¼ˆå¦‚æœæœ‰å†…å®¹ï¼‰
    if current_chunk.strip():
        chunks.append(current_chunk)
    
    # ç¡®ä¿æ‰€æœ‰å—éƒ½æœ‰å†…å®¹
    chunks = [chunk for chunk in chunks if chunk.strip()]
    
    if log:
        log(f"    ğŸ“Š æŒ‰æ ‡é¢˜åˆ†å—ç»Ÿè®¡:")
        for i, chunk in enumerate(chunks, 1):
            chunk_headers = re.findall(r'<h[123][^>]*>', chunk, re.IGNORECASE)
            log(f"      å— {i}: {len(chunk)} å­—ç¬¦, {len(chunk_headers)} ä¸ªæ ‡é¢˜")
    
    return chunks

def extract_activity_from_html(activity_html: str, log=None) -> Dict[str, Any]:
    """ä»HTMLå­—ç¬¦ä¸²ä¸­æå–ç»“æ„åŒ–çš„activityæ•°æ®"""
    if not activity_html:
        return {
            'total_activities': 0,
            'activity_summary': {'æ€è€ƒ': 0, 'æœç´¢': 0, 'è¯»å–ç½‘ç«™': 0},
            'activities': []
        }
    
    from process.extract_activity_structured import extract_activity_structured
    result = extract_activity_structured(activity_html=activity_html, log=log)
    return result

def extract_reference_from_html(reference_html: str, log=None) -> Dict[str, Any]:
    """ä»HTMLå­—ç¬¦ä¸²ä¸­æå–ç»“æ„åŒ–çš„referenceæ•°æ®"""
    if not reference_html:
        return {
            'detailed_references': [],
            'domain_summary': []
        }
    
    from process.extract_reference_structured import extract_reference_structured
    result = extract_reference_structured(reference_html=reference_html, log=log)
    return result

def convert_html_to_markdown_string(html_content: str, log) -> str:
    """å°†HTMLå­—ç¬¦ä¸²è½¬æ¢ä¸ºMarkdownå­—ç¬¦ä¸²"""
    if not html_content or not html_content.strip():
        return ""
    
    try:
        from process.html2markdown import process_html_content, create_html_to_markdown_prompt
        from utils import build_llm
        from langchain.schema import HumanMessage
        
        log("    ğŸ”„ é¢„å¤„ç†HTMLå†…å®¹...")
        # é¢„å¤„ç†HTMLå†…å®¹ï¼ˆURLæ ‡å‡†åŒ–å’Œæ ‡ç­¾æ¸…ç†ï¼‰
        processed_html = process_html_content(html_content)
        
        # å¦‚æœå†…å®¹å¤ªçŸ­ï¼Œç›´æ¥è¿”å›
        if len(processed_html) < 100:
            return processed_html
        
        log(f"    ğŸ“ HTMLå†…å®¹é•¿åº¦: {len(processed_html)} å­—ç¬¦")
        
        # åˆ†å—å¤„ç†ï¼ˆå¦‚æœå†…å®¹å¤ªé•¿ï¼‰
        max_chunk_size = 10240
        
        if len(processed_html) <= max_chunk_size:
            chunks = [processed_html]
        else:
            log("    âœ‚ï¸ æŒ‰æ ‡é¢˜åˆ†å—å¤„ç†é•¿å†…å®¹...")
            chunks = split_html_by_headers(processed_html, max_chunk_size, log)
            
            # æ ¡éªŒï¼šç¡®ä¿åˆ‡å—åˆå¹¶åç­‰äºåŸå§‹å†…å®¹
            merged_content = ''.join(chunks)
            if merged_content != processed_html:
                log("    âš ï¸ è­¦å‘Šï¼šåˆ‡å—åˆå¹¶åä¸åŸå§‹å†…å®¹ä¸ä¸€è‡´ï¼")
                log(f"      åŸå§‹é•¿åº¦: {len(processed_html)}")
                log(f"      åˆå¹¶é•¿åº¦: {len(merged_content)}")
                # å¦‚æœæ ¡éªŒå¤±è´¥ï¼Œå›é€€åˆ°å•å—å¤„ç†
                chunks = [processed_html]
            else:
                log("    âœ… åˆ‡å—æ ¡éªŒé€šè¿‡")
        
        log(f"    ğŸ“¦ åˆ†ä¸º {len(chunks)} ä¸ªå—è¿›è¡Œå¤„ç†")
        
        # åˆå§‹åŒ–LLM
        log("    ğŸ¤– åˆå§‹åŒ–LLM...")
        llm = build_llm(temperature=0.1)
        
        # å¹¶è¡Œè½¬æ¢æ‰€æœ‰å—
        log("    ğŸ”„ å¼€å§‹å¹¶è¡Œå¤„ç†æ‰€æœ‰å—...")
        
        def process_chunk(chunk_data):
            i, chunk = chunk_data
            prompt = create_html_to_markdown_prompt(chunk)
            try:
                response = llm.invoke([HumanMessage(content=prompt)])
                markdown_chunk = response.content.strip()
                
                # æ¸…ç†markdownä»£ç å—æ ‡è®°
                markdown_chunk = clean_markdown_block(markdown_chunk)
                
                log(f"    âœ… ç¬¬ {i} å—è½¬æ¢å®Œæˆ")
                return i, markdown_chunk
            except Exception as e:
                log(f"    âš ï¸ ç¬¬ {i} å—è½¬æ¢å¤±è´¥: {e}")
                # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œä¿ç•™åŸå§‹å†…å®¹
                return i, chunk
        
        # ä½¿ç”¨å¹¶è¡Œå¤„ç†
        chunk_data_list = [(i+1, chunk) for i, chunk in enumerate(chunks)]
        markdown_chunks = [None] * len(chunks)  # é¢„åˆ†é…åˆ—è¡¨ä¿æŒé¡ºåº
        
        with ThreadPoolExecutor(max_workers=min(len(chunks), 5)) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_index = {executor.submit(process_chunk, chunk_data): chunk_data[0] for chunk_data in chunk_data_list}
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_index):
                try:
                    i, result = future.result()
                    markdown_chunks[i-1] = result  # iæ˜¯ä»1å¼€å§‹çš„ï¼Œæ‰€ä»¥è¦å‡1
                except Exception as e:
                    i = future_to_index[future]
                    log(f"    âŒ ç¬¬ {i} å—å¤„ç†å¼‚å¸¸: {e}")
                    markdown_chunks[i-1] = chunks[i-1]  # ä½¿ç”¨åŸå§‹chunk
        
        # åˆå¹¶æ‰€æœ‰å—ï¼Œå†æ¬¡æ¸…ç†ç©ºæ ¼
        cleaned_chunks = [chunk.strip() for chunk in markdown_chunks if chunk and chunk.strip()]
        markdown_content = '\n\n'.join(cleaned_chunks)
        
        # åå¤„ç†ä¿®å¤å’Œæ£€æŸ¥
        log("    ğŸ”§ ä¿®å¤å¸¸è§çš„Markdownæ ¼å¼é—®é¢˜...")
        from process.html2markdown import fix_common_markdown_issues, check_markdown_links, check_markdown_tables
        markdown_content = fix_common_markdown_issues(markdown_content)
        
        # æ£€æŸ¥é“¾æ¥æ­£ç¡®æ€§
        log("    ğŸ” æ£€æŸ¥Markdownè¶…é“¾æ¥...")
        links_ok, link_errors = check_markdown_links(markdown_content)
        if not links_ok:
            log(f"    âš ï¸ å‘ç° {len(link_errors)} ä¸ªé“¾æ¥é—®é¢˜")
            for error in link_errors[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ªé”™è¯¯
                log(f"      - {error}")
            if len(link_errors) > 3:
                log(f"      ... è¿˜æœ‰ {len(link_errors) - 3} ä¸ªé“¾æ¥é—®é¢˜")
        else:
            log("    âœ… æ‰€æœ‰é“¾æ¥æ ¼å¼æ­£ç¡®")
        
        # æ£€æŸ¥è¡¨æ ¼æ­£ç¡®æ€§
        log("    ğŸ” æ£€æŸ¥Markdownè¡¨æ ¼...")
        tables_ok, table_errors = check_markdown_tables(markdown_content)
        if not tables_ok:
            log(f"    âš ï¸ å‘ç° {len(table_errors)} ä¸ªè¡¨æ ¼é—®é¢˜")
            for error in table_errors[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ªé”™è¯¯
                log(f"      - {error}")
            if len(table_errors) > 3:
                log(f"      ... è¿˜æœ‰ {len(table_errors) - 3} ä¸ªè¡¨æ ¼é—®é¢˜")
        else:
            log("    âœ… æ‰€æœ‰è¡¨æ ¼æ ¼å¼æ­£ç¡®")
        
        log(f"    ğŸ“„ æœ€ç»ˆMarkdowné•¿åº¦: {len(markdown_content)} å­—ç¬¦")
        
        return markdown_content
        
    except Exception as e:
        log(f"    âŒ HTMLè½¬æ¢å‡ºé”™: {e}")
        # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œè¿›è¡Œç®€å•çš„HTMLåˆ°æ–‡æœ¬è½¬æ¢
        try:
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(html_content, 'html.parser')
            # ç®€å•çš„æ–‡æœ¬æå–ï¼Œä¿ç•™åŸºæœ¬ç»“æ„
            text = soup.get_text()
            # æ¸…ç†å¤šä½™çš„ç©ºè¡Œ
            lines = [line.strip() for line in text.split('\n') if line.strip()]
            markdown_content = '\n\n'.join(lines)
            
            # å¯¹ç®€å•è½¬æ¢çš„å†…å®¹ä¹Ÿè¿›è¡ŒåŸºæœ¬ä¿®å¤
            log("    ğŸ”§ å¯¹ç®€å•è½¬æ¢ç»“æœè¿›è¡ŒåŸºæœ¬ä¿®å¤...")
            from process.html2markdown import fix_common_markdown_issues
            markdown_content = fix_common_markdown_issues(markdown_content)
            
            return markdown_content
        except:
            return "HTMLè½¬æ¢å¤±è´¥ï¼Œæ— æ³•æå–å†…å®¹"

def generate_output_filename(input_file_path: str, output_dir: str = "result") -> str:
    """æ ¹æ®è¾“å…¥æ–‡ä»¶è·¯å¾„ç”Ÿæˆè¾“å‡ºæ–‡ä»¶è·¯å¾„"""
    # æå–æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
    base_name = os.path.splitext(os.path.basename(input_file_path))[0]
    
    # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶åï¼šparsed_åŸæ–‡ä»¶å.json
    output_filename = f"parsed_{base_name}.json"
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(output_dir, exist_ok=True)
    
    return os.path.join(output_dir, output_filename)

def process_single_file(file_path: str, output_dir: str = "result", include_markdown: bool = True) -> Dict[str, Any]:
    """å¤„ç†å•ä¸ªJSONæ–‡ä»¶çš„å®Œæ•´æµç¨‹ï¼Œå¹¶ä¿å­˜åˆ°ç‹¬ç«‹æ–‡ä»¶"""
    def log(message: str):
        """å†…éƒ¨æ—¥å¿—å‡½æ•°"""
        file_logger.log(file_path, message)
    
    log(f"å¼€å§‹å¤„ç†æ–‡ä»¶: {os.path.basename(file_path)}")
    log("-" * 50)
    
    try:
        # ç¬¬1æ­¥ï¼šå¤„ç†messagesï¼Œæå–promptå’Œresponse
        log("ğŸ” ç¬¬1æ­¥: æå–promptå’Œresponse...")
        from process.process_json import process_openai_json
        basic_data = process_openai_json(file_path)
        if not basic_data:
            log(f"âŒ æ— æ³•å¤„ç†æ–‡ä»¶: {file_path}")
            return None
        
        log(f"  âœ“ Prompté•¿åº¦: {len(basic_data.get('prompt', ''))} å­—ç¬¦")
        log(f"  âœ“ Responseé•¿åº¦: {len(basic_data.get('answer', ''))} å­—ç¬¦")
        log(f"  âœ“ Activityé•¿åº¦: {len(basic_data.get('activity', ''))} å­—ç¬¦")
        log(f"  âœ“ Referenceé•¿åº¦: {len(basic_data.get('reference', ''))} å­—ç¬¦")
        
        # ç¬¬2æ­¥ï¼šå°†responseè½¬æ¢ä¸ºmarkdownï¼ˆå¯é€‰ï¼‰
        markdown_report = ""
        if include_markdown and basic_data.get('answer'):
            log("ğŸ“ ç¬¬2æ­¥: è½¬æ¢responseä¸ºmarkdown...")
            try:
                markdown_report = convert_html_to_markdown_string(basic_data['answer'], log)
                log("  âœ… Markdownè½¬æ¢å®Œæˆ")
            except Exception as e:
                log(f"  âš ï¸ Markdownè½¬æ¢å¤±è´¥: {e}")
                # è¿›è¡Œç®€å•çš„HTMLåˆ°æ–‡æœ¬è½¬æ¢ä½œä¸ºé™çº§æ–¹æ¡ˆ
                try:
                    from bs4 import BeautifulSoup
                    soup = BeautifulSoup(basic_data['answer'], 'html.parser')
                    text = soup.get_text()
                    lines = [line.strip() for line in text.split('\n') if line.strip()]
                    markdown_report = '\n\n'.join(lines)
                    
                    # å¯¹é™çº§å¤„ç†çš„å†…å®¹ä¹Ÿè¿›è¡Œä¿®å¤
                    log("  ğŸ”§ å¯¹é™çº§å¤„ç†ç»“æœè¿›è¡Œä¿®å¤...")
                    from process.html2markdown import fix_common_markdown_issues
                    markdown_report = fix_common_markdown_issues(markdown_report)
                except:
                    markdown_report = "HTMLè½¬æ¢å¤±è´¥ï¼Œæ— æ³•æå–å†…å®¹"
        else:
            log("ğŸ“ ç¬¬2æ­¥: ç®€å•HTMLè½¬æ¢")
            # è¿›è¡Œç®€å•çš„HTMLåˆ°æ–‡æœ¬è½¬æ¢
            try:
                from bs4 import BeautifulSoup
                soup = BeautifulSoup(basic_data['answer'], 'html.parser')
                text = soup.get_text()
                lines = [line.strip() for line in text.split('\n') if line.strip()]
                markdown_report = '\n\n'.join(lines)
                
                # å¯¹ç®€å•è½¬æ¢çš„å†…å®¹ä¹Ÿè¿›è¡Œä¿®å¤å’Œæ£€æŸ¥
                log("  ğŸ”§ ä¿®å¤å¸¸è§æ ¼å¼é—®é¢˜...")
                from process.html2markdown import fix_common_markdown_issues, check_markdown_links, check_markdown_tables
                markdown_report = fix_common_markdown_issues(markdown_report)
                
                # æ£€æŸ¥é“¾æ¥ï¼ˆè™½ç„¶ç®€å•è½¬æ¢é€šå¸¸æ²¡æœ‰é“¾æ¥ï¼Œä½†ä¿æŒä¸€è‡´æ€§ï¼‰
                links_ok, link_errors = check_markdown_links(markdown_report)
                if not links_ok:
                    log(f"  âš ï¸ å‘ç° {len(link_errors)} ä¸ªé“¾æ¥é—®é¢˜")
                
                # æ£€æŸ¥è¡¨æ ¼
                tables_ok, table_errors = check_markdown_tables(markdown_report)
                if not tables_ok:
                    log(f"  âš ï¸ å‘ç° {len(table_errors)} ä¸ªè¡¨æ ¼é—®é¢˜")
                
                log(f"  âœ“ ç®€å•è½¬æ¢å®Œæˆï¼Œé•¿åº¦: {len(markdown_report)} å­—ç¬¦")
            except:
                markdown_report = "HTMLè½¬æ¢å¤±è´¥ï¼Œæ— æ³•æå–å†…å®¹"
                log("  âš ï¸ ç®€å•è½¬æ¢å¤±è´¥")
        
        # ç¬¬3æ­¥ï¼šå¤„ç†activity
        log("âš¡ ç¬¬3æ­¥: å¤„ç†activityæ•°æ®...")
        activity_data = extract_activity_from_html(basic_data.get('activity', ''), log)
        log(f"  âœ“ æå–åˆ° {activity_data['total_activities']} ä¸ªæ´»åŠ¨")
        log(f"    æ€è€ƒ: {activity_data['activity_summary'].get('æ€è€ƒ', 0)} ä¸ª")
        log(f"    æœç´¢: {activity_data['activity_summary'].get('æœç´¢', 0)} ä¸ª") 
        log(f"    è¯»å–ç½‘ç«™: {activity_data['activity_summary'].get('è¯»å–ç½‘ç«™', 0)} ä¸ª")
        
        # ç¬¬4æ­¥ï¼šå¤„ç†reference
        log("ğŸ”— ç¬¬4æ­¥: å¤„ç†referenceæ•°æ®...")
        reference_data = extract_reference_from_html(basic_data.get('reference', ''), log)
        log(f"  âœ“ æå–åˆ° {len(reference_data['detailed_references'])} ä¸ªè¯¦ç»†å¼•ç”¨")
        log(f"  âœ“ æå–åˆ° {len(reference_data['domain_summary'])} ä¸ªåŸŸåæ±‡æ€»")
        
        # æ•´åˆæ‰€æœ‰ç»“æœ
        result = {
            'file_info': {
                'source_file': os.path.basename(file_path),
                'source_path': file_path,
                'processed_at': datetime.now().isoformat()
            },
            'prompt': basic_data.get('prompt', ''),
            'response': markdown_report,
            'activity': activity_data,
            'reference': reference_data
        }
        
        # ç¬¬5æ­¥ï¼šä¿å­˜åˆ°ç‹¬ç«‹æ–‡ä»¶
        output_file = generate_output_filename(file_path, output_dir)
        log(f"ğŸ’¾ ä¿å­˜åˆ°: {output_file}")
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        log(f"âœ… æ–‡ä»¶ {os.path.basename(file_path)} å¤„ç†å®Œæˆ")
        
        # è¿”å›å¤„ç†ç»“æœæ‘˜è¦ï¼ˆç”¨äºæ±‡æ€»ç»Ÿè®¡ï¼‰
        return {
            'input_file': file_path,
            'output_file': output_file,
            'stats': {
                'prompt_length': len(result['prompt']),
                'response_length': len(result['response']),
                'total_activities': result['activity']['total_activities'],
                'activity_breakdown': result['activity']['activity_summary'],
                'detailed_references': len(result['reference']['detailed_references']),
                'domain_summary': len(result['reference']['domain_summary'])
            }
        }
        
    except Exception as e:
        log(f"âŒ å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
        import traceback
        log(traceback.format_exc())
        return None

def create_index_file(processed_results: list, output_dir: str = "result") -> str:
    """åˆ›å»ºç´¢å¼•æ–‡ä»¶ï¼Œè®°å½•æ‰€æœ‰å¤„ç†è¿‡çš„æ–‡ä»¶ä¿¡æ¯"""
    index_data = {
        'summary': {
            'total_files_processed': len(processed_results),
            'processing_completed_at': datetime.now().isoformat(),
            'total_activities': sum(r['stats']['total_activities'] for r in processed_results),
            'total_detailed_references': sum(r['stats']['detailed_references'] for r in processed_results),
            'total_domain_summary': sum(r['stats']['domain_summary'] for r in processed_results)
        },
        'files': []
    }
    
    for result in processed_results:
        file_info = {
            'input_file': os.path.basename(result['input_file']),
            'output_file': os.path.basename(result['output_file']),
            'stats': result['stats']
        }
        index_data['files'].append(file_info)
    
    # ä¿å­˜ç´¢å¼•æ–‡ä»¶
    index_file = os.path.join(output_dir, "processing_index.json")
    with open(index_file, 'w', encoding='utf-8') as f:
        json.dump(index_data, f, ensure_ascii=False, indent=2)
    
    return index_file

def process_all_files(input_dir: str = "openai", output_dir: str = "result", include_markdown: bool = False, max_workers: int = 4) -> None:
    """æ‰¹é‡å¹¶è¡Œå¤„ç†ç›®å½•ä¸‹çš„æ‰€æœ‰JSONæ–‡ä»¶ï¼Œæ¯ä¸ªæ–‡ä»¶ç”Ÿæˆç‹¬ç«‹è¾“å‡º"""
    print("ğŸš€ å¼€å§‹ç»Ÿä¸€å¤„ç†æµç¨‹")
    print("=" * 60)
    
    # æŸ¥æ‰¾æ‰€æœ‰JSONæ–‡ä»¶
    json_files = []
    if os.path.exists(input_dir):
        for file_name in os.listdir(input_dir):
            if file_name.endswith('.json'):
                json_files.append(os.path.join(input_dir, file_name))
    
    if not json_files:
        print(f"âŒ åœ¨ç›®å½• {input_dir} ä¸­æœªæ‰¾åˆ°JSONæ–‡ä»¶")
        return
    
    json_files.sort()  # æŒ‰æ–‡ä»¶åæ’åº
    print(f"ğŸ“Š å‘ç° {len(json_files)} ä¸ªJSONæ–‡ä»¶:")
    for file_path in json_files:
        print(f"  - {os.path.basename(file_path)}")
    
    print(f"\nğŸ”§ é…ç½®é€‰é¡¹:")
    print(f"  - è¾“å…¥ç›®å½•: {input_dir}")
    print(f"  - è¾“å‡ºç›®å½•: {output_dir}")
    print(f"  - åŒ…å«Markdownè½¬æ¢: {'æ˜¯' if include_markdown else 'å¦'}")
    print(f"  - å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•°: {max_workers}")
    print(f"  - æ—¥å¿—è¾“å‡ºç›®å½•: logs/")
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å’Œæ—¥å¿—ç›®å½•å­˜åœ¨
    os.makedirs(output_dir, exist_ok=True)
    os.makedirs("logs", exist_ok=True)
    
    # åˆå§‹åŒ–æ—¥å¿—è®°å½•å™¨
    global file_logger
    file_logger = FileLogger()
    
    print(f"\nğŸ”„ å¼€å§‹å¹¶è¡Œå¤„ç† {len(json_files)} ä¸ªæ–‡ä»¶...")
    
    # ä½¿ç”¨å¹¶è¡Œå¤„ç†æ‰€æœ‰æ–‡ä»¶
    processed_results = []
    failed_files = []
    
    def process_file_wrapper(file_path):
        """åŒ…è£…å‡½æ•°ï¼Œç”¨äºå¤„ç†å•ä¸ªæ–‡ä»¶å¹¶è¿”å›ç»“æœ"""
        try:
            result = process_single_file(file_path, output_dir, include_markdown)
            return file_path, result, None
        except Exception as e:
            return file_path, None, str(e)
    
    # ä½¿ç”¨ThreadPoolExecutorå¹¶è¡Œå¤„ç†ï¼Œé…åˆtqdmæ˜¾ç¤ºè¿›åº¦
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        futures = {executor.submit(process_file_wrapper, file_path): file_path for file_path in json_files}
        
        # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦æ¡
        with tqdm(total=len(json_files), desc="ğŸ“ å¤„ç†æ–‡ä»¶", unit="files") as pbar:
            for future in as_completed(futures):
                file_path = futures[future]
                try:
                    original_path, result, error = future.result()
                    
                    if result:
                        processed_results.append(result)
                        pbar.set_postfix_str(f"âœ… {os.path.basename(original_path)}")
                    else:
                        failed_files.append((original_path, error or "æœªçŸ¥é”™è¯¯"))
                        pbar.set_postfix_str(f"âŒ {os.path.basename(original_path)}")
                    
                except Exception as e:
                    failed_files.append((file_path, str(e)))
                    pbar.set_postfix_str(f"âŒ {os.path.basename(file_path)}")
                
                pbar.update(1)
    
    # å…³é—­æ‰€æœ‰æ—¥å¿—æ–‡ä»¶
    file_logger.close_all()
    
    success_count = len(processed_results)
    
    if not processed_results:
        print("\nâŒ æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•æ–‡ä»¶")
        if failed_files:
            print("å¤±è´¥çš„æ–‡ä»¶:")
            for file_path, error in failed_files:
                print(f"  - {os.path.basename(file_path)}: {error}")
        return
    
    # åˆ›å»ºç´¢å¼•æ–‡ä»¶
    print(f"\nğŸ“‹ åˆ›å»ºå¤„ç†ç´¢å¼•æ–‡ä»¶...")
    index_file = create_index_file(processed_results, output_dir)
    print(f"  âœ“ ç´¢å¼•æ–‡ä»¶å·²ä¿å­˜: {index_file}")
    
    # æ‰“å°æ±‡æ€»ä¿¡æ¯
    print("\n" + "=" * 60)
    print("ğŸ‰ å¤„ç†å®Œæˆ! æ±‡æ€»ä¿¡æ¯")
    print("=" * 60)
    print(f"ğŸ“ æˆåŠŸå¤„ç†æ–‡ä»¶: {success_count}/{len(json_files)}")
    
    if failed_files:
        print(f"âŒ å¤±è´¥æ–‡ä»¶: {len(failed_files)}")
        print("å¤±è´¥çš„æ–‡ä»¶:")
        for file_path, error in failed_files[:5]:  # æœ€å¤šæ˜¾ç¤º5ä¸ªå¤±è´¥æ–‡ä»¶
            print(f"  - {os.path.basename(file_path)}: {error}")
        if len(failed_files) > 5:
            print(f"  ... è¿˜æœ‰ {len(failed_files) - 5} ä¸ªå¤±è´¥æ–‡ä»¶")
    
    total_activities = sum(r['stats']['total_activities'] for r in processed_results)
    total_detailed_refs = sum(r['stats']['detailed_references'] for r in processed_results)
    total_domain_summary = sum(r['stats']['domain_summary'] for r in processed_results)
    
    print(f"âš¡ æ€»æ´»åŠ¨æ•°é‡: {total_activities}")
    print(f"ğŸ”— æ€»è¯¦ç»†å¼•ç”¨æ•°é‡: {total_detailed_refs}")
    print(f"ğŸ·ï¸ æ€»åŸŸåæ±‡æ€»æ•°é‡: {total_domain_summary}")
    
    # æ‰“å°å„æ–‡ä»¶çš„è¯¦ç»†ç»Ÿè®¡å’Œè¾“å‡ºæ–‡ä»¶è·¯å¾„
    print("\nğŸ“Š å„æ–‡ä»¶è¯¦ç»†ç»Ÿè®¡å’Œè¾“å‡ºæ–‡ä»¶:")
    print("-" * 60)
    for result in processed_results:
        input_name = os.path.basename(result['input_file'])
        output_name = os.path.basename(result['output_file'])
        stats = result['stats']
        
        print(f"ğŸ“„ {input_name} -> {output_name}")
        print(f"  ğŸ“ Prompt: {stats['prompt_length']} å­—ç¬¦")
        print(f"  ğŸ“„ Response: {stats['response_length']} å­—ç¬¦")
        print(f"  âš¡ æ´»åŠ¨: {stats['total_activities']} ä¸ª "
              f"(æ€è€ƒ: {stats['activity_breakdown'].get('æ€è€ƒ', 0)}, "
              f"æœç´¢: {stats['activity_breakdown'].get('æœç´¢', 0)}, "
              f"è¯»å–ç½‘ç«™: {stats['activity_breakdown'].get('è¯»å–ç½‘ç«™', 0)})")
        print(f"  ğŸ”— å¼•ç”¨: {stats['detailed_references']} ä¸ªè¯¦ç»†å¼•ç”¨, {stats['domain_summary']} ä¸ªåŸŸåæ±‡æ€»")
        print()
    
    print(f"ğŸ“ æ‰€æœ‰ç»“æœæ–‡ä»¶éƒ½ä¿å­˜åœ¨ç›®å½•: {output_dir}/")
    print(f"ğŸ“‹ å¤„ç†ç´¢å¼•æ–‡ä»¶: {index_file}")
    print(f"ğŸ“ è¯¦ç»†æ—¥å¿—æ–‡ä»¶ä¿å­˜åœ¨ç›®å½•: logs/")

def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    include_markdown = "--markdown" in sys.argv or "-m" in sys.argv
    
    # æ£€æŸ¥è¾“å…¥ç›®å½•å‚æ•°
    input_dir = "openai"  # é»˜è®¤å€¼
    for arg in sys.argv:
        if arg.startswith("--input="):
            input_dir = arg.split("=", 1)[1]
        elif arg.startswith("--input-dir="):
            input_dir = arg.split("=", 1)[1]
    
    # æ£€æŸ¥è¾“å‡ºç›®å½•å‚æ•°
    output_dir = "result"  # é»˜è®¤å€¼
    for arg in sys.argv:
        if arg.startswith("--output="):
            output_dir = arg.split("=", 1)[1]
        elif arg.startswith("--output-dir="):
            output_dir = arg.split("=", 1)[1]
    
    # æ£€æŸ¥å¹¶è¡Œåº¦å‚æ•°
    max_workers = 4  # é»˜è®¤å€¼
    for arg in sys.argv:
        if arg.startswith("--workers="):
            try:
                max_workers = int(arg.split("=")[1])
                max_workers = max(1, min(max_workers, 16))  # é™åˆ¶åœ¨1-16ä¹‹é—´
            except ValueError:
                print("âš ï¸ æ— æ•ˆçš„workerså‚æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼4")
        elif arg.startswith("-j"):
            try:
                max_workers = int(arg[2:])
                max_workers = max(1, min(max_workers, 16))
            except ValueError:
                print("âš ï¸ æ— æ•ˆçš„å¹¶è¡Œåº¦å‚æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼4")
    
    # æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
    if "--help" in sys.argv or "-h" in sys.argv:
        print("ğŸ”§ ç»Ÿä¸€å¤„ç†å™¨ v2 - æ‰¹é‡å¤„ç†JSONæ–‡ä»¶")
        print("=" * 50)
        print("ç”¨æ³•:")
        print("  python unified_processor_v2.py [é€‰é¡¹]")
        print("\né€‰é¡¹:")
        print("  --input=<ç›®å½•>           æŒ‡å®šè¾“å…¥ç›®å½• (é»˜è®¤: openai)")
        print("  --input-dir=<ç›®å½•>       åŒä¸Š")
        print("  --output=<ç›®å½•>          æŒ‡å®šè¾“å‡ºç›®å½• (é»˜è®¤: result)")
        print("  --output-dir=<ç›®å½•>      åŒä¸Š")
        print("  --markdown, -m           å¯ç”¨Markdownè½¬æ¢ (éœ€è¦LLM)")
        print("  --workers=<æ•°é‡>         å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•° (é»˜è®¤: 4)")
        print("  -j<æ•°é‡>                 åŒä¸Š")
        print("  --help, -h               æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯")
        print("\nç¤ºä¾‹:")
        print("  python unified_processor_v2.py")
        print("  python unified_processor_v2.py --input=data --output=results")
        print("  python unified_processor_v2.py --markdown --workers=8")
        print("  python unified_processor_v2.py --input-dir=raw_data --output-dir=processed_data -m -j8")
        return
    
    if include_markdown:
        print("ğŸ”§ å¯ç”¨Markdownè½¬æ¢æ¨¡å¼ï¼ˆéœ€è¦LLMè°ƒç”¨ï¼‰")
    else:
        print("âš¡ å¿«é€Ÿæ¨¡å¼ï¼ˆè·³è¿‡Markdownè½¬æ¢ï¼‰")
    
    print(f"ğŸ“ è¾“å…¥ç›®å½•: {input_dir}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print(f"ğŸ”§ å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•°: {max_workers}")
    print("ğŸ“ è¯¦ç»†å¤„ç†æ—¥å¿—å°†ä¿å­˜åˆ° logs/ ç›®å½•")
    
    try:
        process_all_files(input_dir=input_dir, output_dir=output_dir, include_markdown=include_markdown, max_workers=max_workers)
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­å¤„ç†")
        file_logger.close_all()
    except Exception as e:
        print(f"\n\nâŒ å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        file_logger.close_all()

if __name__ == "__main__":
    main() 